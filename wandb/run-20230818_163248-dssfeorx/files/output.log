[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-08-18 16:32:49,008	INFO tensorboardx.py:48 -- pip install "ray[tune]" to see TensorBoard files.
2023-08-18 16:32:49,008	WARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.
[36m(pid=47064)[39m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.
[36m(pid=47064)[39m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,788	WARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,792	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,792	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,796	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.AttentionWrapper` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,796	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,796	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,816	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.GTrXLNet` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,830	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,830	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,830	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,832	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47063)[39m 2023-08-18 16:32:51,916	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/catalog.py:790: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  prep = cls(observation_space, options)
2023-08-18 16:32:51,961	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,962	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,966	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.AttentionWrapper` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,966	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,966	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/torch/attention_net.py:281: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  super().__init__(obs_space, action_space, None, model_config, name)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/torch/attention_net.py:281: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  super().__init__(obs_space, action_space, None, model_config, name)
2023-08-18 16:32:51,972	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.GTrXLNet` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,977	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,977	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,977	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:51,977	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:440: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  prep = get_preprocessor(space)(space)
2023-08-18 16:32:51,997	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/connectors/agent/obs_preproc.py:40: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  self._preprocessor = get_preprocessor(obs_space)(
2023-08-18 16:32:52,016	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.multi_gpu_learner_thread.MultiGPULearnerThread` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:52,016	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.minibatch_buffer.MinibatchBuffer` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:52,016	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.learner_thread.LearnerThread` has been deprecated. This will raise an error in the future!
2023-08-18 16:32:52,018	WARNING util.py:68 -- Install gputil for GPU system monitoring.
2023-08-18 16:32:52,114	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.replay_ops.SimpleReplayBuffer` has been deprecated. This will raise an error in the future!
train step: 1
agent_timesteps_total: 8100
connector_metrics:
  ObsPreprocessorConnector_ms: 0.032290443778038025
  StateBufferConnector_ms: 0.005894526839256287
  ViewRequirementAgentConnector_ms: 0.1886945217847824
counters:
  num_agent_steps_sampled: 8100
  num_agent_steps_trained: 2500
  num_env_steps_sampled: 8100
  num_env_steps_trained: 2500
  num_samples_added_to_queue: 8000
  num_training_step_calls_since_last_synch_worker_weights: 405
  num_weight_broadcasts: 162
custom_metrics: {}
date: 2023-08-18_16-33-02
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 3.0
episode_reward_mean: 1.234375
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 64
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 3.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5760376453399658
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -14.583683013916016
        total_loss: -6.265893459320068
        var_gnorm: 63.343021392822266
        vf_explained_var: -0.5258620977401733
        vf_loss: 18.21161651611328
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 5.0
  learner_queue:
    size_count: 10
    size_mean: 0.0
    size_quantiles: [0.0, 0.0, 0.0, 0.0, 0.0]
    size_std: 0.0
  num_agent_steps_sampled: 8100
  num_agent_steps_trained: 2500
  num_env_steps_sampled: 8100
  num_env_steps_trained: 2500
  num_samples_added_to_queue: 8000
  num_training_step_calls_since_last_synch_worker_weights: 405
  num_weight_broadcasts: 162
  timing_breakdown:
    learner_dequeue_time_ms: 498.719
    learner_grad_time_ms: 949.715
    learner_load_time_ms: 102.256
    learner_load_wait_time_ms: 8.134
iterations_since_restore: 1
node_ip: 127.0.0.1
num_agent_steps_sampled: 8100
num_agent_steps_trained: 2500
num_env_steps_sampled: 8100
num_env_steps_sampled_this_iter: 8100
num_env_steps_sampled_throughput_per_sec: 809.9942644048565
num_env_steps_trained: 2500
num_env_steps_trained_this_iter: 2500
num_env_steps_trained_throughput_per_sec: 249.99822975458534
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2500
perf:
  cpu_util_percent: 51.54666666666666
  ram_util_percent: 84.18666666666667
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10029696981696097
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03581482426168487
  mean_inference_ms: 1.8405949055674675
  mean_raw_obs_processing_ms: 0.42351496647536346
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.032290443778038025
    StateBufferConnector_ms: 0.005894526839256287
    ViewRequirementAgentConnector_ms: 0.1886945217847824
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 3.0
  episode_reward_mean: 1.234375
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128]
    episode_reward: [1.0, 3.0, 0.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0,
      0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0,
      3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0,
      2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0,
      1.0, 2.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10029696981696097
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03581482426168487
    mean_inference_ms: 1.8405949055674675
    mean_raw_obs_processing_ms: 0.42351496647536346
time_since_restore: 10.385966777801514
time_this_iter_s: 10.385966777801514
time_total_s: 10.385966777801514
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.074
timestamp: 1692343982
timesteps_total: 8100
training_iteration: 1
trial_id: default
train step: 2
agent_timesteps_total: 15900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03352046012878418
  StateBufferConnector_ms: 0.006034374237060547
  ViewRequirementAgentConnector_ms: 0.19664931297302246
counters:
  num_agent_steps_sampled: 15900
  num_agent_steps_trained: 7500
  num_env_steps_sampled: 15900
  num_env_steps_trained: 7500
  num_samples_added_to_queue: 15500
  num_training_step_calls_since_last_synch_worker_weights: 1103
  num_weight_broadcasts: 313
custom_metrics: {}
date: 2023-08-18_16-33-12
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.25
episode_reward_min: 0.0
episodes_this_iter: 60
episodes_total: 124
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 9.2
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.574691891670227
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 14.743997573852539
        total_loss: 22.021705627441406
        var_gnorm: 63.34282684326172
        vf_explained_var: -0.2137455940246582
        vf_loss: 16.130109786987305
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 15.0
  learner_queue:
    size_count: 19
    size_mean: 0.0
    size_quantiles: [0.0, 0.0, 0.0, 0.0, 0.0]
    size_std: 0.0
  num_agent_steps_sampled: 15900
  num_agent_steps_trained: 7500
  num_env_steps_sampled: 15900
  num_env_steps_trained: 7500
  num_samples_added_to_queue: 15500
  num_training_step_calls_since_last_synch_worker_weights: 1103
  num_weight_broadcasts: 313
  timing_breakdown:
    learner_dequeue_time_ms: 5366.172
    learner_grad_time_ms: 1034.993
    learner_load_time_ms: 52.294
    learner_load_wait_time_ms: 4.567
iterations_since_restore: 2
node_ip: 127.0.0.1
num_agent_steps_sampled: 15900
num_agent_steps_trained: 7500
num_env_steps_sampled: 15900
num_env_steps_sampled_this_iter: 7800
num_env_steps_sampled_throughput_per_sec: 779.9985308674827
num_env_steps_trained: 7500
num_env_steps_trained_this_iter: 5000
num_env_steps_trained_throughput_per_sec: 499.99905824838635
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5000
perf:
  cpu_util_percent: 54.25714285714285
  ram_util_percent: 84.17142857142856
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10144039837397145
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.036478858017212394
  mean_inference_ms: 1.8657821609351588
  mean_raw_obs_processing_ms: 0.42824516687426367
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03352046012878418
    StateBufferConnector_ms: 0.006034374237060547
    ViewRequirementAgentConnector_ms: 0.19664931297302246
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.25
  episode_reward_min: 0.0
  episodes_this_iter: 60
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 1.0, 1.0, 0.0, 2.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0,
      1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0,
      3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0, 1.0, 2.0, 0.0, 0.0, 3.0, 0.0, 1.0, 2.0,
      2.0, 2.0, 3.0, 4.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 3.0, 3.0, 1.0, 0.0, 0.0, 1.0,
      0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0,
      0.0, 0.0, 1.0, 2.0, 4.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 2.0, 2.0,
      2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10144039837397145
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.036478858017212394
    mean_inference_ms: 1.8657821609351588
    mean_raw_obs_processing_ms: 0.42824516687426367
time_since_restore: 20.530154943466187
time_this_iter_s: 10.144188165664673
time_total_s: 20.530154943466187
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.073
timestamp: 1692343992
timesteps_total: 15900
training_iteration: 2
trial_id: default
train step: 3
agent_timesteps_total: 23600
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03305697441101074
  StateBufferConnector_ms: 0.006064414978027344
  ViewRequirementAgentConnector_ms: 0.19791340827941895
counters:
  num_agent_steps_sampled: 23600
  num_agent_steps_trained: 12500
  num_env_steps_sampled: 23600
  num_env_steps_trained: 12500
  num_samples_added_to_queue: 23500
  num_training_step_calls_since_last_synch_worker_weights: 1078
  num_weight_broadcasts: 464
custom_metrics: {}
date: 2023-08-18_16-33-22
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.18
episode_reward_min: 0.0
episodes_this_iter: 60
episodes_total: 184
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 11.3
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5736030340194702
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -28.93906021118164
        total_loss: -20.397062301635742
        var_gnorm: 63.342620849609375
        vf_explained_var: -0.7466543912887573
        vf_loss: 18.65760040283203
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 25.0
  learner_queue:
    size_count: 29
    size_mean: 0.8620689655172413
    size_quantiles: [0.0, 0.0, 0.0, 4.0, 5.0]
    size_std: 1.6342062911824533
  num_agent_steps_sampled: 23600
  num_agent_steps_trained: 12500
  num_env_steps_sampled: 23600
  num_env_steps_trained: 12500
  num_samples_added_to_queue: 23500
  num_training_step_calls_since_last_synch_worker_weights: 1078
  num_weight_broadcasts: 464
  timing_breakdown:
    learner_dequeue_time_ms: 3810.836
    learner_grad_time_ms: 996.09
    learner_load_time_ms: 52.294
    learner_load_wait_time_ms: 25.644
iterations_since_restore: 3
node_ip: 127.0.0.1
num_agent_steps_sampled: 23600
num_agent_steps_trained: 12500
num_env_steps_sampled: 23600
num_env_steps_sampled_this_iter: 7700
num_env_steps_sampled_throughput_per_sec: 769.9954471857333
num_env_steps_trained: 12500
num_env_steps_trained_this_iter: 5000
num_env_steps_trained_throughput_per_sec: 499.9970436270995
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5000
perf:
  cpu_util_percent: 53.873333333333335
  ram_util_percent: 83.55999999999999
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10284037437305066
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03708633344349102
  mean_inference_ms: 1.8988825390493318
  mean_raw_obs_processing_ms: 0.43495063588755056
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03305697441101074
    StateBufferConnector_ms: 0.006064414978027344
    ViewRequirementAgentConnector_ms: 0.19791340827941895
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.18
  episode_reward_min: 0.0
  episodes_this_iter: 60
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0,
      1.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 4.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,
      1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 2.0, 1.0, 2.0,
      2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 3.0, 1.0, 1.0, 0.0, 2.0, 1.0,
      0.0, 0.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0,
      2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 2.0, 4.0, 0.0, 2.0, 4.0, 1.0, 4.0, 1.0,
      2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10284037437305066
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03708633344349102
    mean_inference_ms: 1.8988825390493318
    mean_raw_obs_processing_ms: 0.43495063588755056
time_since_restore: 30.742171049118042
time_this_iter_s: 10.212016105651855
time_total_s: 30.742171049118042
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344002
timesteps_total: 23600
training_iteration: 3
trial_id: default
train step: 4
agent_timesteps_total: 29000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03923153877258301
  StateBufferConnector_ms: 0.007174968719482422
  ViewRequirementAgentConnector_ms: 0.23242902755737305
counters:
  num_agent_steps_sampled: 29000
  num_agent_steps_trained: 16500
  num_env_steps_sampled: 29000
  num_env_steps_trained: 16500
  num_samples_added_to_queue: 29000
  num_training_step_calls_since_last_synch_worker_weights: 786
  num_weight_broadcasts: 569
custom_metrics: {}
date: 2023-08-18_16-33-33
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.32
episode_reward_min: 0.0
episodes_this_iter: 44
episodes_total: 228
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 13.8
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.574467658996582
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -12.98924446105957
        total_loss: -7.45143985748291
        var_gnorm: 63.34247970581055
        vf_explained_var: -0.36616694927215576
        vf_loss: 12.650076866149902
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 33.0
  learner_queue:
    size_count: 40
    size_mean: 1.875
    size_quantiles: [0.0, 0.0, 0.0, 5.100000000000001, 8.0]
    size_std: 2.4102645083060903
  num_agent_steps_sampled: 29000
  num_agent_steps_trained: 16500
  num_env_steps_sampled: 29000
  num_env_steps_trained: 16500
  num_samples_added_to_queue: 29000
  num_training_step_calls_since_last_synch_worker_weights: 786
  num_weight_broadcasts: 569
  timing_breakdown:
    learner_dequeue_time_ms: 2858.129
    learner_grad_time_ms: 997.606
    learner_load_time_ms: 59.403
    learner_load_wait_time_ms: 13.518
iterations_since_restore: 4
node_ip: 127.0.0.1
num_agent_steps_sampled: 29000
num_agent_steps_trained: 16500
num_env_steps_sampled: 29000
num_env_steps_sampled_this_iter: 5400
num_env_steps_sampled_throughput_per_sec: 539.9968199917188
num_env_steps_trained: 16500
num_env_steps_trained_this_iter: 4000
num_env_steps_trained_throughput_per_sec: 399.99764443831026
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 4000
perf:
  cpu_util_percent: 57.45
  ram_util_percent: 84.25714285714285
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10724694447628316
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03906553578997321
  mean_inference_ms: 1.9850563649058166
  mean_raw_obs_processing_ms: 0.4528929809354388
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03923153877258301
    StateBufferConnector_ms: 0.007174968719482422
    ViewRequirementAgentConnector_ms: 0.23242902755737305
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.32
  episode_reward_min: 0.0
  episodes_this_iter: 44
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 3.0, 1.0,
      1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0,
      1.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 1.0, 2.0, 4.0, 0.0, 2.0,
      4.0, 1.0, 4.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 4.0, 0.0, 2.0, 2.0, 0.0, 2.0,
      1.0, 1.0, 1.0, 1.0, 0.0, 2.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0,
      1.0, 2.0, 3.0, 0.0, 3.0, 1.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 2.0,
      4.0, 1.0, 3.0, 1.0, 0.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10724694447628316
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03906553578997321
    mean_inference_ms: 1.9850563649058166
    mean_raw_obs_processing_ms: 0.4528929809354388
time_since_restore: 41.04013514518738
time_this_iter_s: 10.297964096069336
time_total_s: 41.04013514518738
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344013
timesteps_total: 29000
training_iteration: 4
trial_id: default
train step: 5
agent_timesteps_total: 35950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.04159188270568848
  StateBufferConnector_ms: 0.007445096969604492
  ViewRequirementAgentConnector_ms: 0.2451343536376953
counters:
  num_agent_steps_sampled: 35950
  num_agent_steps_trained: 22500
  num_env_steps_sampled: 35950
  num_env_steps_trained: 22500
  num_samples_added_to_queue: 35500
  num_training_step_calls_since_last_synch_worker_weights: 1197
  num_weight_broadcasts: 703
custom_metrics: {}
date: 2023-08-18_16-33-43
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.36
episode_reward_min: 0.0
episodes_this_iter: 54
episodes_total: 282
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 20.4
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5777324438095093
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 36.398616790771484
        total_loss: 42.07029342651367
        var_gnorm: 63.342262268066406
        vf_explained_var: -0.560139536857605
        vf_loss: 12.921088218688965
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 45.0
  learner_queue:
    size_count: 52
    size_mean: 2.98
    size_quantiles: [0.0, 0.0, 3.0, 7.100000000000001, 10.0]
    size_std: 2.9765080211549906
  num_agent_steps_sampled: 35950
  num_agent_steps_trained: 22500
  num_env_steps_sampled: 35950
  num_env_steps_trained: 22500
  num_samples_added_to_queue: 35500
  num_training_step_calls_since_last_synch_worker_weights: 1197
  num_weight_broadcasts: 703
  timing_breakdown:
    learner_dequeue_time_ms: 2286.505
    learner_grad_time_ms: 625.944
    learner_load_time_ms: 45.566
    learner_load_wait_time_ms: 30.0
iterations_since_restore: 5
node_ip: 127.0.0.1
num_agent_steps_sampled: 35950
num_agent_steps_trained: 22500
num_env_steps_sampled: 35950
num_env_steps_sampled_this_iter: 6950
num_env_steps_sampled_throughput_per_sec: 694.9937365619559
num_env_steps_trained: 22500
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9945927153576
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 54.693333333333335
  ram_util_percent: 83.06666666666665
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1125567605013509
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04156116208522276
  mean_inference_ms: 2.0875296164049817
  mean_raw_obs_processing_ms: 0.47638396421673884
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.04159188270568848
    StateBufferConnector_ms: 0.007445096969604492
    ViewRequirementAgentConnector_ms: 0.2451343536376953
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.36
  episode_reward_min: 0.0
  episodes_this_iter: 54
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 4.0, 0.0, 2.0, 2.0, 0.0, 2.0, 1.0, 1.0, 1.0, 1.0, 0.0, 2.0,
      2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 3.0, 0.0, 3.0, 1.0,
      3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 0.0, 1.0,
      1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 3.0, 0.0, 3.0,
      4.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0,
      2.0, 4.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0,
      0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1125567605013509
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04156116208522276
    mean_inference_ms: 2.0875296164049817
    mean_raw_obs_processing_ms: 0.47638396421673884
time_since_restore: 51.36633324623108
time_this_iter_s: 10.326198101043701
time_total_s: 51.36633324623108
timers:
  sample_time_ms: 0.031
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.09
timestamp: 1692344023
timesteps_total: 35950
training_iteration: 5
trial_id: default
train step: 6
agent_timesteps_total: 41950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.04094409942626953
  StateBufferConnector_ms: 0.007533550262451172
  ViewRequirementAgentConnector_ms: 0.23921680450439453
counters:
  num_agent_steps_sampled: 41950
  num_agent_steps_trained: 28000
  num_env_steps_sampled: 41950
  num_env_steps_trained: 28000
  num_samples_added_to_queue: 41500
  num_training_step_calls_since_last_synch_worker_weights: 1154
  num_weight_broadcasts: 819
custom_metrics: {}
date: 2023-08-18_16-33-53
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.29
episode_reward_min: 0.0
episodes_this_iter: 46
episodes_total: 328
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 23.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5771543979644775
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -11.182892799377441
        total_loss: -4.863698482513428
        var_gnorm: 63.342140197753906
        vf_explained_var: -0.4565533399581909
        vf_loss: 14.215542793273926
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 56.0
  learner_queue:
    size_count: 61
    size_mean: 4.32
    size_quantiles: [0.0, 0.0, 4.5, 8.100000000000001, 10.0]
    size_std: 3.1012255641923243
  num_agent_steps_sampled: 41950
  num_agent_steps_trained: 28000
  num_env_steps_sampled: 41950
  num_env_steps_trained: 28000
  num_samples_added_to_queue: 41500
  num_training_step_calls_since_last_synch_worker_weights: 1154
  num_weight_broadcasts: 819
  timing_breakdown:
    learner_dequeue_time_ms: 2286.505
    learner_grad_time_ms: 994.978
    learner_load_time_ms: 45.566
    learner_load_wait_time_ms: 35.485
iterations_since_restore: 6
node_ip: 127.0.0.1
num_agent_steps_sampled: 41950
num_agent_steps_trained: 28000
num_env_steps_sampled: 41950
num_env_steps_sampled_this_iter: 6000
num_env_steps_sampled_throughput_per_sec: 599.9956655815441
num_env_steps_trained: 28000
num_env_steps_trained_this_iter: 5500
num_env_steps_trained_throughput_per_sec: 549.9960267830821
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5500
perf:
  cpu_util_percent: 69.13571428571427
  ram_util_percent: 83.82142857142858
pid: 47026
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11479246516070919
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04244438354074836
  mean_inference_ms: 2.127912795320319
  mean_raw_obs_processing_ms: 0.48654642310645885
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.04094409942626953
    StateBufferConnector_ms: 0.007533550262451172
    ViewRequirementAgentConnector_ms: 0.23921680450439453
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.29
  episode_reward_min: 0.0
  episodes_this_iter: 46
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 3.0,
      0.0, 3.0, 4.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0,
      0.0, 2.0, 2.0, 4.0, 2.0, 2.0, 3.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0,
      1.0, 3.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 4.0, 0.0, 0.0, 1.0, 1.0, 1.0,
      0.0, 0.0, 5.0, 3.0, 1.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 1.0, 3.0, 1.0, 1.0, 0.0,
      0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 3.0, 2.0, 2.0, 3.0, 1.0,
      2.0, 1.0, 0.0, 0.0, 1.0, 2.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11479246516070919
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04244438354074836
    mean_inference_ms: 2.127912795320319
    mean_raw_obs_processing_ms: 0.48654642310645885
time_since_restore: 61.62595534324646
time_this_iter_s: 10.25962209701538
time_total_s: 61.62595534324646
timers:
  sample_time_ms: 0.035
  synch_weights_time_ms: 0.01
  training_iteration_time_ms: 0.105
timestamp: 1692344033
timesteps_total: 41950
training_iteration: 6
trial_id: default
train step: 7
Traceback (most recent call last):
  File "/Users/sangbin/Impala/launch.py", line 304, in <module>
    result = algo.train()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 372, in train
    result = self.step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 2837, in _run_one_training_iteration
    results = self.training_step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 750, in training_step
    with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/util/timer.py", line 29, in __exit__
    time_delta = time.time() - self._start_time
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/sangbin/Impala/launch.py", line 304, in <module>
    result = algo.train()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 372, in train
    result = self.step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 2837, in _run_one_training_iteration
    results = self.training_step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 750, in training_step
    with self._timers[SYNCH_WORKER_WEIGHTS_TIMER]:
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/util/timer.py", line 29, in __exit__
    time_delta = time.time() - self._start_time
KeyboardInterrupt