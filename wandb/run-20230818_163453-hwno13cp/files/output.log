[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2023-08-18 16:34:54,144	INFO tensorboardx.py:48 -- pip install "ray[tune]" to see TensorBoard files.
2023-08-18 16:34:54,144	WARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.
[36m(pid=47216)[39m lz4 not available, disabling sample compression. This will significantly impact RLlib performance. To install lz4, run `pip install lz4`.
[36m(pid=47216)[39m DeprecationWarning: `DirectStepOptimizer` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,807	WARNING env.py:162 -- Your env doesn't have a .spec.max_episode_steps attribute. Your horizon will default to infinity, and your environment will not be reset.
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,812	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,812	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,815	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.AttentionWrapper` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,815	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,815	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,836	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.GTrXLNet` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,849	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,849	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,849	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,850	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/catalog.py:790: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  prep = cls(observation_space, options)
2023-08-18 16:34:56,946	WARNING deprecation.py:50 -- DeprecationWarning: `LearningRateSchedule` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,946	WARNING deprecation.py:50 -- DeprecationWarning: `EntropyCoeffSchedule` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,949	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.AttentionWrapper` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,949	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.fcnet.FullyConnectedNetwork` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,949	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_modelv2.TorchModelV2` has been deprecated. Use `ray.rllib.core.rl_module.rl_module.RLModule` instead. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/torch/attention_net.py:281: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  super().__init__(obs_space, action_space, None, model_config, name)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/torch/attention_net.py:281: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  super().__init__(obs_space, action_space, None, model_config, name)
2023-08-18 16:34:56,955	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.attention_net.GTrXLNet` has been deprecated. This will raise an error in the future!
[36m(RolloutWorker pid=47215)[39m 2023-08-18 16:34:56,902	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
2023-08-18 16:34:56,959	WARNING deprecation.py:50 -- DeprecationWarning: `StochasticSampling` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,960	WARNING deprecation.py:50 -- DeprecationWarning: `Exploration` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,960	WARNING deprecation.py:50 -- DeprecationWarning: `Random` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:56,960	WARNING deprecation.py:50 -- DeprecationWarning: `ValueNetworkMixin` has been deprecated. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/modelv2.py:440: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  prep = get_preprocessor(space)(space)
2023-08-18 16:34:56,980	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.models.torch.torch_action_dist.TorchDistributionWrapper` has been deprecated. Use `ray.rllib.models.torch.torch_distributions.TorchCategorical` instead. This will raise an error in the future!
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/models/preprocessors.py:307: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  preprocessor = preprocessor_class(space, self._options)
/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/connectors/agent/obs_preproc.py:40: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.
  self._preprocessor = get_preprocessor(obs_space)(
2023-08-18 16:34:57,002	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.multi_gpu_learner_thread.MultiGPULearnerThread` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:57,002	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.minibatch_buffer.MinibatchBuffer` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:57,002	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.learner_thread.LearnerThread` has been deprecated. This will raise an error in the future!
2023-08-18 16:34:57,004	WARNING util.py:68 -- Install gputil for GPU system monitoring.
2023-08-18 16:34:57,118	WARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.replay_ops.SimpleReplayBuffer` has been deprecated. This will raise an error in the future!
train step: 1
agent_timesteps_total: 7850
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03242915676486108
  StateBufferConnector_ms: 0.005905089839812248
  ViewRequirementAgentConnector_ms: 0.19099635462607106
counters:
  num_agent_steps_sampled: 7850
  num_agent_steps_trained: 2500
  num_env_steps_sampled: 7850
  num_env_steps_trained: 2500
  num_samples_added_to_queue: 7500
  num_training_step_calls_since_last_synch_worker_weights: 305
  num_weight_broadcasts: 157
custom_metrics: {}
date: 2023-08-18_16-35-07
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.0806451612903225
episode_reward_min: 0.0
episodes_this_iter: 62
episodes_total: 62
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 3.2
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.572094440460205
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -17.84070587158203
        total_loss: -7.150055408477783
        var_gnorm: 63.31544494628906
        vf_explained_var: -0.6004341840744019
        vf_loss: 22.95339584350586
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 5.0
  learner_queue:
    size_count: 11
    size_mean: 0.0
    size_quantiles: [0.0, 0.0, 0.0, 0.0, 0.0]
    size_std: 0.0
  num_agent_steps_sampled: 7850
  num_agent_steps_trained: 2500
  num_env_steps_sampled: 7850
  num_env_steps_trained: 2500
  num_samples_added_to_queue: 7500
  num_training_step_calls_since_last_synch_worker_weights: 305
  num_weight_broadcasts: 157
  timing_breakdown:
    learner_dequeue_time_ms: 514.253
    learner_grad_time_ms: 780.086
    learner_load_time_ms: 28.858
    learner_load_wait_time_ms: 3.683
iterations_since_restore: 1
node_ip: 127.0.0.1
num_agent_steps_sampled: 7850
num_agent_steps_trained: 2500
num_env_steps_sampled: 7850
num_env_steps_sampled_this_iter: 7850
num_env_steps_sampled_throughput_per_sec: 784.9985963131255
num_env_steps_trained: 2500
num_env_steps_trained_this_iter: 2500
num_env_steps_trained_throughput_per_sec: 249.99955296596355
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 2500
perf:
  cpu_util_percent: 48.72666666666667
  ram_util_percent: 83.78666666666668
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10173671238146899
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.036980737408445534
  mean_inference_ms: 1.8882992392841103
  mean_raw_obs_processing_ms: 0.42896980143991637
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03242915676486108
    StateBufferConnector_ms: 0.005905089839812248
    ViewRequirementAgentConnector_ms: 0.19099635462607106
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.0806451612903225
  episode_reward_min: 0.0
  episodes_this_iter: 62
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128]
    episode_reward: [3.0, 0.0, 0.0, 0.0, 0.0, 2.0, 1.0, 3.0, 2.0, 1.0, 0.0, 1.0, 0.0,
      2.0, 3.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 2.0,
      0.0, 0.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0,
      1.0, 1.0, 4.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 3.0, 1.0,
      3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10173671238146899
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.036980737408445534
    mean_inference_ms: 1.8882992392841103
    mean_raw_obs_processing_ms: 0.42896980143991637
time_since_restore: 10.34986686706543
time_this_iter_s: 10.34986686706543
time_total_s: 10.34986686706543
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.074
timestamp: 1692344107
timesteps_total: 7850
training_iteration: 1
trial_id: default
train step: 2
agent_timesteps_total: 16200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.032122135162353516
  StateBufferConnector_ms: 0.005647420883178711
  ViewRequirementAgentConnector_ms: 0.1890256404876709
counters:
  num_agent_steps_sampled: 16200
  num_agent_steps_trained: 8500
  num_env_steps_sampled: 16200
  num_env_steps_trained: 8500
  num_samples_added_to_queue: 16000
  num_training_step_calls_since_last_synch_worker_weights: 934
  num_weight_broadcasts: 321
custom_metrics: {}
date: 2023-08-18_16-35-17
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.23
episode_reward_min: 0.0
episodes_this_iter: 65
episodes_total: 127
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 5.6
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5692322254180908
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 12.559500694274902
        total_loss: 24.550485610961914
        var_gnorm: 63.31511306762695
        vf_explained_var: -1.0
        vf_loss: 25.55120086669922
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 17.0
  learner_queue:
    size_count: 21
    size_mean: 0.0
    size_quantiles: [0.0, 0.0, 0.0, 0.0, 0.0]
    size_std: 0.0
  num_agent_steps_sampled: 16200
  num_agent_steps_trained: 8500
  num_env_steps_sampled: 16200
  num_env_steps_trained: 8500
  num_samples_added_to_queue: 16000
  num_training_step_calls_since_last_synch_worker_weights: 934
  num_weight_broadcasts: 321
  timing_breakdown:
    learner_dequeue_time_ms: 5504.369
    learner_grad_time_ms: 1000.888
    learner_load_time_ms: 16.188
    learner_load_wait_time_ms: 28.747
iterations_since_restore: 2
node_ip: 127.0.0.1
num_agent_steps_sampled: 16200
num_agent_steps_trained: 8500
num_env_steps_sampled: 16200
num_env_steps_sampled_this_iter: 8350
num_env_steps_sampled_throughput_per_sec: 834.9988254325223
num_env_steps_trained: 8500
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9991559994172
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 45.89999999999999
  ram_util_percent: 82.07142857142857
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.09990800379040027
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03607860532581482
  mean_inference_ms: 1.8631005001684655
  mean_raw_obs_processing_ms: 0.4239762337213513
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.032122135162353516
    StateBufferConnector_ms: 0.005647420883178711
    ViewRequirementAgentConnector_ms: 0.1890256404876709
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.23
  episode_reward_min: 0.0
  episodes_this_iter: 65
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 2.0, 0.0, 0.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 0.0, 3.0, 1.0,
      0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 4.0, 1.0, 1.0, 0.0, 1.0, 2.0, 0.0, 0.0, 0.0,
      1.0, 1.0, 1.0, 3.0, 1.0, 3.0, 0.0, 2.0, 0.0, 0.0, 4.0, 0.0, 0.0, 0.0, 0.0, 4.0,
      0.0, 1.0, 3.0, 3.0, 3.0, 1.0, 2.0, 0.0, 2.0, 1.0, 1.0, 3.0, 0.0, 1.0, 1.0, 3.0,
      0.0, 0.0, 3.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 3.0, 1.0, 1.0, 2.0, 3.0, 2.0, 1.0,
      1.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0, 1.0, 1.0, 0.0, 2.0, 3.0, 0.0,
      1.0, 1.0, 3.0, 0.0, 0.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.09990800379040027
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03607860532581482
    mean_inference_ms: 1.8631005001684655
    mean_raw_obs_processing_ms: 0.4239762337213513
time_since_restore: 20.507791996002197
time_this_iter_s: 10.157925128936768
time_total_s: 20.507791996002197
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.075
timestamp: 1692344117
timesteps_total: 16200
training_iteration: 2
trial_id: default
train step: 3
agent_timesteps_total: 23650
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03350496292114258
  StateBufferConnector_ms: 0.005801677703857422
  ViewRequirementAgentConnector_ms: 0.196458101272583
counters:
  num_agent_steps_sampled: 23650
  num_agent_steps_trained: 13500
  num_env_steps_sampled: 23650
  num_env_steps_trained: 13500
  num_samples_added_to_queue: 23500
  num_training_step_calls_since_last_synch_worker_weights: 564
  num_weight_broadcasts: 467
custom_metrics: {}
date: 2023-08-18_16-35-27
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 58
episodes_total: 185
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 11.6
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5714175701141357
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -22.76642608642578
        total_loss: -10.780956268310547
        var_gnorm: 63.3148307800293
        vf_explained_var: -0.5529980659484863
        vf_loss: 25.542356491088867
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 27.0
  learner_queue:
    size_count: 32
    size_mean: 0.3125
    size_quantiles: [0.0, 0.0, 0.0, 1.0, 3.0]
    size_std: 0.8076779989575054
  num_agent_steps_sampled: 23650
  num_agent_steps_trained: 13500
  num_env_steps_sampled: 23650
  num_env_steps_trained: 13500
  num_samples_added_to_queue: 23500
  num_training_step_calls_since_last_synch_worker_weights: 564
  num_weight_broadcasts: 467
  timing_breakdown:
    learner_dequeue_time_ms: 4527.817
    learner_grad_time_ms: 934.229
    learner_load_time_ms: 11.53
    learner_load_wait_time_ms: 22.373
iterations_since_restore: 3
node_ip: 127.0.0.1
num_agent_steps_sampled: 23650
num_agent_steps_trained: 13500
num_env_steps_sampled: 23650
num_env_steps_sampled_this_iter: 7450
num_env_steps_sampled_throughput_per_sec: 744.9942095729751
num_env_steps_trained: 13500
num_env_steps_trained_this_iter: 5000
num_env_steps_trained_throughput_per_sec: 499.9961138073659
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5000
perf:
  cpu_util_percent: 57.773333333333326
  ram_util_percent: 82.40666666666668
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1007381302639386
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03639547134529233
  mean_inference_ms: 1.8819278676665947
  mean_raw_obs_processing_ms: 0.42694728759072903
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03350496292114258
    StateBufferConnector_ms: 0.005801677703857422
    ViewRequirementAgentConnector_ms: 0.196458101272583
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.34
  episode_reward_min: 0.0
  episodes_this_iter: 58
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 3.0, 0.0, 0.0, 3.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 3.0,
      1.0, 1.0, 2.0, 3.0, 2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 2.0, 1.0,
      1.0, 1.0, 0.0, 2.0, 3.0, 0.0, 1.0, 1.0, 3.0, 0.0, 0.0, 1.0, 2.0, 3.0, 0.0, 2.0,
      4.0, 1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,
      1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 6.0, 1.0, 1.0, 4.0, 0.0, 0.0, 3.0, 0.0, 1.0, 1.0,
      2.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 4.0, 2.0, 2.0, 0.0, 1.0, 0.0, 1.0,
      2.0, 1.0, 1.0, 0.0, 3.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1007381302639386
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03639547134529233
    mean_inference_ms: 1.8819278676665947
    mean_raw_obs_processing_ms: 0.42694728759072903
time_since_restore: 30.7054660320282
time_this_iter_s: 10.197674036026001
time_total_s: 30.7054660320282
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.08
timestamp: 1692344127
timesteps_total: 23650
training_iteration: 3
trial_id: default
train step: 4
agent_timesteps_total: 29550
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03938007354736328
  StateBufferConnector_ms: 0.007183074951171875
  ViewRequirementAgentConnector_ms: 0.2277371883392334
counters:
  num_agent_steps_sampled: 29550
  num_agent_steps_trained: 18000
  num_env_steps_sampled: 29550
  num_env_steps_trained: 18000
  num_samples_added_to_queue: 29500
  num_training_step_calls_since_last_synch_worker_weights: 604
  num_weight_broadcasts: 582
custom_metrics: {}
date: 2023-08-18_16-35-37
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.22
episode_reward_min: 0.0
episodes_this_iter: 46
episodes_total: 231
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 13.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5742086172103882
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 11.00965690612793
        total_loss: 21.020671844482422
        var_gnorm: 63.314613342285156
        vf_explained_var: -0.5369408130645752
        vf_loss: 21.596237182617188
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 36.0
  learner_queue:
    size_count: 42
    size_mean: 0.9523809523809523
    size_quantiles: [0.0, 0.0, 0.0, 3.0, 6.0]
    size_std: 1.6322987143812009
  num_agent_steps_sampled: 29550
  num_agent_steps_trained: 18000
  num_env_steps_sampled: 29550
  num_env_steps_trained: 18000
  num_samples_added_to_queue: 29500
  num_training_step_calls_since_last_synch_worker_weights: 604
  num_weight_broadcasts: 582
  timing_breakdown:
    learner_dequeue_time_ms: 3661.057
    learner_grad_time_ms: 996.193
    learner_load_time_ms: 11.53
    learner_load_wait_time_ms: 30.77
iterations_since_restore: 4
node_ip: 127.0.0.1
num_agent_steps_sampled: 29550
num_agent_steps_trained: 18000
num_env_steps_sampled: 29550
num_env_steps_sampled_this_iter: 5900
num_env_steps_sampled_throughput_per_sec: 589.9971726076199
num_env_steps_trained: 18000
num_env_steps_trained_this_iter: 4500
num_env_steps_trained_throughput_per_sec: 449.9978435142864
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 4500
perf:
  cpu_util_percent: 64.42857142857143
  ram_util_percent: 83.2
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1056622010899406
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.038290289996875435
  mean_inference_ms: 1.966009558914359
  mean_raw_obs_processing_ms: 0.44529453475176894
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03938007354736328
    StateBufferConnector_ms: 0.007183074951171875
    ViewRequirementAgentConnector_ms: 0.2277371883392334
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.22
  episode_reward_min: 0.0
  episodes_this_iter: 46
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0,
      0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 6.0, 1.0, 1.0, 4.0, 0.0, 0.0, 3.0, 0.0,
      1.0, 1.0, 2.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 4.0, 2.0, 2.0, 0.0, 1.0,
      0.0, 1.0, 2.0, 1.0, 1.0, 0.0, 3.0, 2.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0,
      2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 2.0,
      0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 2.0, 1.0, 1.0, 3.0, 0.0, 1.0, 2.0, 1.0, 1.0,
      2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1056622010899406
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.038290289996875435
    mean_inference_ms: 1.966009558914359
    mean_raw_obs_processing_ms: 0.44529453475176894
time_since_restore: 40.94630694389343
time_this_iter_s: 10.240840911865234
time_total_s: 40.94630694389343
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.087
timestamp: 1692344137
timesteps_total: 29550
training_iteration: 4
trial_id: default
train step: 5
agent_timesteps_total: 37000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03943324089050293
  StateBufferConnector_ms: 0.00725245475769043
  ViewRequirementAgentConnector_ms: 0.229400634765625
counters:
  num_agent_steps_sampled: 37000
  num_agent_steps_trained: 23500
  num_env_steps_sampled: 37000
  num_env_steps_trained: 23500
  num_samples_added_to_queue: 37000
  num_training_step_calls_since_last_synch_worker_weights: 396
  num_weight_broadcasts: 728
custom_metrics: {}
date: 2023-08-18_16-35-48
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 3.0
episode_reward_mean: 1.21
episode_reward_min: 0.0
episodes_this_iter: 59
episodes_total: 290
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 19.8
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5778058767318726
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -46.75768280029297
        total_loss: -37.966514587402344
        var_gnorm: 63.314388275146484
        vf_explained_var: -0.5944006443023682
        vf_loss: 19.160144805908203
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 47.0
  learner_queue:
    size_count: 53
    size_mean: 2.26
    size_quantiles: [0.0, 0.0, 0.5, 7.0, 10.0]
    size_std: 2.9516774891576487
  num_agent_steps_sampled: 37000
  num_agent_steps_trained: 23500
  num_env_steps_sampled: 37000
  num_env_steps_trained: 23500
  num_samples_added_to_queue: 37000
  num_training_step_calls_since_last_synch_worker_weights: 396
  num_weight_broadcasts: 728
  timing_breakdown:
    learner_dequeue_time_ms: 2928.847
    learner_grad_time_ms: 856.635
    learner_load_time_ms: 9.242
    learner_load_wait_time_ms: 22.423
iterations_since_restore: 5
node_ip: 127.0.0.1
num_agent_steps_sampled: 37000
num_agent_steps_trained: 23500
num_env_steps_sampled: 37000
num_env_steps_sampled_this_iter: 7450
num_env_steps_sampled_throughput_per_sec: 744.9937477636526
num_env_steps_trained: 23500
num_env_steps_trained_this_iter: 5500
num_env_steps_trained_throughput_per_sec: 549.9953842550455
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5500
perf:
  cpu_util_percent: 60.046666666666674
  ram_util_percent: 82.74000000000001
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10965896710849708
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03976018815481932
  mean_inference_ms: 2.037276458036001
  mean_raw_obs_processing_ms: 0.4629778898244919
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03943324089050293
    StateBufferConnector_ms: 0.00725245475769043
    ViewRequirementAgentConnector_ms: 0.229400634765625
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 3.0
  episode_reward_mean: 1.21
  episode_reward_min: 0.0
  episodes_this_iter: 59
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 0.0,
      1.0, 2.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 2.0, 1.0, 1.0, 3.0,
      0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 3.0, 1.0,
      1.0, 3.0, 3.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0,
      2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0,
      2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0, 1.0, 3.0, 1.0,
      1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10965896710849708
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03976018815481932
    mean_inference_ms: 2.037276458036001
    mean_raw_obs_processing_ms: 0.4629778898244919
time_since_restore: 51.254459857940674
time_this_iter_s: 10.308152914047241
time_total_s: 51.254459857940674
timers:
  sample_time_ms: 0.027
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.076
timestamp: 1692344148
timesteps_total: 37000
training_iteration: 5
trial_id: default
train step: 6
agent_timesteps_total: 44900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.033385276794433594
  StateBufferConnector_ms: 0.0060422420501708984
  ViewRequirementAgentConnector_ms: 0.2006375789642334
counters:
  num_agent_steps_sampled: 44900
  num_agent_steps_trained: 29500
  num_env_steps_sampled: 44900
  num_env_steps_trained: 29500
  num_samples_added_to_queue: 44500
  num_training_step_calls_since_last_synch_worker_weights: 487
  num_weight_broadcasts: 883
custom_metrics: {}
date: 2023-08-18_16-35-58
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.34
episode_reward_min: 0.0
episodes_this_iter: 61
episodes_total: 351
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 22.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.573594331741333
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -21.469675064086914
        total_loss: -14.352056503295898
        var_gnorm: 63.314125061035156
        vf_explained_var: -0.5993447303771973
        vf_loss: 15.808832168579102
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 59.0
  learner_queue:
    size_count: 64
    size_mean: 4.5
    size_quantiles: [0.0, 0.0, 3.5, 10.100000000000001, 13.0]
    size_std: 4.172529209005013
  num_agent_steps_sampled: 44900
  num_agent_steps_trained: 29500
  num_env_steps_sampled: 44900
  num_env_steps_trained: 29500
  num_samples_added_to_queue: 44500
  num_training_step_calls_since_last_synch_worker_weights: 487
  num_weight_broadcasts: 883
  timing_breakdown:
    learner_dequeue_time_ms: 2440.706
    learner_grad_time_ms: 805.476
    learner_load_time_ms: 7.85
    learner_load_wait_time_ms: 32.246
iterations_since_restore: 6
node_ip: 127.0.0.1
num_agent_steps_sampled: 44900
num_agent_steps_trained: 29500
num_env_steps_sampled: 44900
num_env_steps_sampled_this_iter: 7900
num_env_steps_sampled_throughput_per_sec: 789.992899243283
num_env_steps_trained: 29500
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9946070202149
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 57.135714285714286
  ram_util_percent: 82.87857142857142
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10871950046444823
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03941321468061087
  mean_inference_ms: 2.025150531511065
  mean_raw_obs_processing_ms: 0.4605744776427942
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.033385276794433594
    StateBufferConnector_ms: 0.0060422420501708984
    ViewRequirementAgentConnector_ms: 0.2006375789642334
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.34
  episode_reward_min: 0.0
  episodes_this_iter: 61
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0,
      0.0, 2.0, 0.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0,
      1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0,
      1.0, 0.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 5.0, 1.0, 3.0, 2.0,
      1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 2.0, 3.0, 0.0, 2.0, 3.0, 1.0, 2.0, 0.0, 2.0, 2.0,
      2.0, 2.0, 0.0, 1.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0,
      1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10871950046444823
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03941321468061087
    mean_inference_ms: 2.025150531511065
    mean_raw_obs_processing_ms: 0.4605744776427942
time_since_restore: 61.45344591140747
time_this_iter_s: 10.198986053466797
time_total_s: 61.45344591140747
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.086
timestamp: 1692344158
timesteps_total: 44900
training_iteration: 6
trial_id: default
train step: 7
agent_timesteps_total: 53050
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03164052963256836
  StateBufferConnector_ms: 0.005821943283081055
  ViewRequirementAgentConnector_ms: 0.1898326873779297
counters:
  num_agent_steps_sampled: 53050
  num_agent_steps_trained: 36500
  num_env_steps_sampled: 53050
  num_env_steps_trained: 36500
  num_samples_added_to_queue: 53000
  num_training_step_calls_since_last_synch_worker_weights: 202
  num_weight_broadcasts: 1043
custom_metrics: {}
date: 2023-08-18_16-36-08
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 4.0
episode_reward_mean: 1.13
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 415
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 26.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5705362558364868
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -45.387943267822266
        total_loss: -34.84821319580078
        var_gnorm: 63.31389236450195
        vf_explained_var: -0.19227612018585205
        vf_loss: 22.649993896484375
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 73.0
  learner_queue:
    size_count: 79
    size_mean: 8.48
    size_quantiles: [0.0, 1.0, 9.0, 16.0, 16.0]
    size_std: 4.944653678469303
  num_agent_steps_sampled: 53050
  num_agent_steps_trained: 36500
  num_env_steps_sampled: 53050
  num_env_steps_trained: 36500
  num_samples_added_to_queue: 53000
  num_training_step_calls_since_last_synch_worker_weights: 202
  num_weight_broadcasts: 1043
  timing_breakdown:
    learner_dequeue_time_ms: 2440.706
    learner_grad_time_ms: 296.036
    learner_load_time_ms: 7.85
    learner_load_wait_time_ms: 2.55
iterations_since_restore: 7
node_ip: 127.0.0.1
num_agent_steps_sampled: 53050
num_agent_steps_trained: 36500
num_env_steps_sampled: 53050
num_env_steps_sampled_this_iter: 8150
num_env_steps_sampled_throughput_per_sec: 814.9928105511689
num_env_steps_trained: 36500
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9938250132739
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 45.779999999999994
  ram_util_percent: 83.12666666666665
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10739297835704752
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03889273050198874
  mean_inference_ms: 2.001763163952775
  mean_raw_obs_processing_ms: 0.4551571526333177
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03164052963256836
    StateBufferConnector_ms: 0.005821943283081055
    ViewRequirementAgentConnector_ms: 0.1898326873779297
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 4.0
  episode_reward_mean: 1.13
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 1.0, 2.0, 3.0, 0.0, 2.0, 3.0, 1.0, 2.0, 0.0, 2.0, 2.0,
      2.0, 2.0, 0.0, 1.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 0.0, 1.0, 2.0,
      1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0,
      0.0, 0.0, 1.0, 0.0, 2.0, 3.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0,
      2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 3.0,
      2.0, 1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0,
      1.0, 4.0, 1.0, 1.0, 1.0, 0.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10739297835704752
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03889273050198874
    mean_inference_ms: 2.001763163952775
    mean_raw_obs_processing_ms: 0.4551571526333177
time_since_restore: 71.68072080612183
time_this_iter_s: 10.227274894714355
time_total_s: 71.68072080612183
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.087
timestamp: 1692344168
timesteps_total: 53050
training_iteration: 7
trial_id: default
train step: 8
agent_timesteps_total: 61500
connector_metrics:
  ObsPreprocessorConnector_ms: 0.031160354614257812
  StateBufferConnector_ms: 0.005707979202270508
  ViewRequirementAgentConnector_ms: 0.1844162940979004
counters:
  num_agent_steps_sampled: 61500
  num_agent_steps_trained: 45000
  num_env_steps_sampled: 61500
  num_env_steps_trained: 45000
  num_samples_added_to_queue: 61500
  num_training_step_calls_since_last_synch_worker_weights: 104
  num_weight_broadcasts: 1209
custom_metrics: {}
date: 2023-08-18_16-36-19
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 481
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.299999999999997
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5699235200881958
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 0.7684304714202881
        total_loss: 6.8781843185424805
        var_gnorm: 63.31367111206055
        vf_explained_var: -0.1376039981842041
        vf_loss: 13.78943157196045
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 90.0
  learner_queue:
    size_count: 95
    size_mean: 12.38
    size_quantiles: [5.0, 7.9, 13.0, 16.0, 16.0]
    size_std: 3.309924470437354
  num_agent_steps_sampled: 61500
  num_agent_steps_trained: 45000
  num_env_steps_sampled: 61500
  num_env_steps_trained: 45000
  num_samples_added_to_queue: 61500
  num_training_step_calls_since_last_synch_worker_weights: 104
  num_weight_broadcasts: 1209
  timing_breakdown:
    learner_dequeue_time_ms: 2092.035
    learner_grad_time_ms: 321.371
    learner_load_time_ms: 7.597
    learner_load_wait_time_ms: 2.768
iterations_since_restore: 8
node_ip: 127.0.0.1
num_agent_steps_sampled: 61500
num_agent_steps_trained: 45000
num_env_steps_sampled: 61500
num_env_steps_sampled_this_iter: 8450
num_env_steps_sampled_throughput_per_sec: 844.9948224146518
num_env_steps_trained: 45000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9947917780522
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 48.635714285714286
  ram_util_percent: 83.00714285714285
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10594397743379791
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03828787542101022
  mean_inference_ms: 1.975803383430802
  mean_raw_obs_processing_ms: 0.4493809042181472
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.031160354614257812
    StateBufferConnector_ms: 0.005707979202270508
    ViewRequirementAgentConnector_ms: 0.1844162940979004
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.35
  episode_reward_min: 0.0
  episodes_this_iter: 66
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 3.0, 2.0, 0.0, 0.0, 0.0, 2.0, 1.0, 0.0, 1.0, 3.0, 2.0, 1.0,
      1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 1.0, 4.0,
      1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 6.0, 0.0, 1.0, 4.0, 1.0, 1.0, 1.0, 1.0,
      1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 2.0, 1.0,
      2.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0,
      2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 2.0, 5.0, 2.0, 5.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0,
      0.0, 1.0, 5.0, 1.0, 1.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10594397743379791
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03828787542101022
    mean_inference_ms: 1.975803383430802
    mean_raw_obs_processing_ms: 0.4493809042181472
time_since_restore: 81.90726566314697
time_this_iter_s: 10.226544857025146
time_total_s: 81.90726566314697
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.076
timestamp: 1692344179
timesteps_total: 61500
training_iteration: 8
trial_id: default
train step: 9
agent_timesteps_total: 69850
connector_metrics:
  ObsPreprocessorConnector_ms: 0.031096696853637695
  StateBufferConnector_ms: 0.005598783493041992
  ViewRequirementAgentConnector_ms: 0.18691778182983398
counters:
  num_agent_steps_sampled: 69850
  num_agent_steps_trained: 53000
  num_env_steps_sampled: 69850
  num_env_steps_trained: 53000
  num_samples_added_to_queue: 69500
  num_training_step_calls_since_last_synch_worker_weights: 171
  num_weight_broadcasts: 1373
custom_metrics: {}
date: 2023-08-18_16-36-29
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.49
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 547
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.3
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5712018013000488
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -2.72078800201416
        total_loss: 0.27967262268066406
        var_gnorm: 63.31345748901367
        vf_explained_var: -0.16351068019866943
        vf_loss: 7.572123050689697
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 106.0
  learner_queue:
    size_count: 112
    size_mean: 14.3
    size_quantiles: [9.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 2.071231517720798
  num_agent_steps_sampled: 69850
  num_agent_steps_trained: 53000
  num_env_steps_sampled: 69850
  num_env_steps_trained: 53000
  num_samples_added_to_queue: 69500
  num_training_step_calls_since_last_synch_worker_weights: 171
  num_weight_broadcasts: 1373
  timing_breakdown:
    learner_dequeue_time_ms: 1627.14
    learner_grad_time_ms: 302.96
    learner_load_time_ms: 6.411
    learner_load_wait_time_ms: 2.625
iterations_since_restore: 9
node_ip: 127.0.0.1
num_agent_steps_sampled: 69850
num_agent_steps_trained: 53000
num_env_steps_sampled: 69850
num_env_steps_sampled_this_iter: 8350
num_env_steps_sampled_throughput_per_sec: 834.9960184287146
num_env_steps_trained: 53000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9961853209242
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 52.73333333333334
  ram_util_percent: 82.54
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10481244094989092
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03782388654601346
  mean_inference_ms: 1.956247317736317
  mean_raw_obs_processing_ms: 0.44489252645333693
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.031096696853637695
    StateBufferConnector_ms: 0.005598783493041992
    ViewRequirementAgentConnector_ms: 0.18691778182983398
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.49
  episode_reward_min: 0.0
  episodes_this_iter: 66
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 3.0, 3.0, 2.0, 2.0, 1.0,
      1.0, 2.0, 0.0, 1.0, 2.0, 5.0, 2.0, 5.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0,
      5.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0,
      1.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 1.0, 1.0, 0.0, 3.0,
      3.0, 4.0, 2.0, 4.0, 1.0, 1.0, 0.0, 2.0, 3.0, 2.0, 1.0, 1.0, 3.0, 0.0, 0.0, 1.0,
      0.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0,
      2.0, 4.0, 1.0, 2.0, 2.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10481244094989092
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03782388654601346
    mean_inference_ms: 1.956247317736317
    mean_raw_obs_processing_ms: 0.44489252645333693
time_since_restore: 92.18903160095215
time_this_iter_s: 10.281765937805176
time_total_s: 92.18903160095215
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.087
timestamp: 1692344189
timesteps_total: 69850
training_iteration: 9
trial_id: default
train step: 10
agent_timesteps_total: 78200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.030905723571777344
  StateBufferConnector_ms: 0.005614042282104492
  ViewRequirementAgentConnector_ms: 0.18726825714111328
counters:
  num_agent_steps_sampled: 78200
  num_agent_steps_trained: 61500
  num_env_steps_sampled: 78200
  num_env_steps_trained: 61500
  num_samples_added_to_queue: 78000
  num_training_step_calls_since_last_synch_worker_weights: 176
  num_weight_broadcasts: 1537
custom_metrics: {}
date: 2023-08-18_16-36-39
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.42
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 611
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.3
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5651320219039917
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 21.281930923461914
        total_loss: 25.8825626373291
        var_gnorm: 63.31325149536133
        vf_explained_var: 0.18004095554351807
        vf_loss: 10.766395568847656
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 123.0
  learner_queue:
    size_count: 129
    size_mean: 14.72
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6737980762326141
  num_agent_steps_sampled: 78200
  num_agent_steps_trained: 61500
  num_env_steps_sampled: 78200
  num_env_steps_trained: 61500
  num_samples_added_to_queue: 78000
  num_training_step_calls_since_last_synch_worker_weights: 176
  num_weight_broadcasts: 1537
  timing_breakdown:
    learner_dequeue_time_ms: 1464.426
    learner_grad_time_ms: 291.785
    learner_load_time_ms: 5.952
    learner_load_wait_time_ms: 2.58
iterations_since_restore: 10
node_ip: 127.0.0.1
num_agent_steps_sampled: 78200
num_agent_steps_trained: 61500
num_env_steps_sampled: 78200
num_env_steps_sampled_this_iter: 8350
num_env_steps_sampled_throughput_per_sec: 834.9973124352174
num_env_steps_trained: 61500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9972641556105
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 46.00000000000001
  ram_util_percent: 81.57857142857144
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10397521287896783
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03747428949419523
  mean_inference_ms: 1.941631826483935
  mean_raw_obs_processing_ms: 0.4416896695740756
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.030905723571777344
    StateBufferConnector_ms: 0.005614042282104492
    ViewRequirementAgentConnector_ms: 0.18726825714111328
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.42
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 1.0, 1.0, 0.0, 2.0, 3.0, 2.0, 1.0, 1.0, 3.0, 0.0, 0.0, 1.0,
      0.0, 1.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0,
      2.0, 4.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 2.0, 2.0, 1.0,
      2.0, 2.0, 2.0, 1.0, 1.0, 0.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 5.0, 0.0, 2.0, 4.0,
      2.0, 2.0, 1.0, 0.0, 2.0, 1.0, 4.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
      0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 3.0,
      1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10397521287896783
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03747428949419523
    mean_inference_ms: 1.941631826483935
    mean_raw_obs_processing_ms: 0.4416896695740756
time_since_restore: 102.42551255226135
time_this_iter_s: 10.236480951309204
time_total_s: 102.42551255226135
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.075
timestamp: 1692344199
timesteps_total: 78200
training_iteration: 10
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000010'.
train step: 11
agent_timesteps_total: 86150
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03152060508728027
  StateBufferConnector_ms: 0.005689859390258789
  ViewRequirementAgentConnector_ms: 0.18828606605529785
counters:
  num_agent_steps_sampled: 86150
  num_agent_steps_trained: 69500
  num_env_steps_sampled: 86150
  num_env_steps_trained: 69500
  num_samples_added_to_queue: 86000
  num_training_step_calls_since_last_synch_worker_weights: 667
  num_weight_broadcasts: 1693
custom_metrics: {}
date: 2023-08-18_16-36-49
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 1.35
episode_reward_min: 0.0
episodes_this_iter: 63
episodes_total: 674
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.3
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.557652235031128
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 43.71287536621094
        total_loss: 53.258182525634766
        var_gnorm: 63.31309127807617
        vf_explained_var: 0.11852937936782837
        vf_loss: 20.648273468017578
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 139.0
  learner_queue:
    size_count: 144
    size_mean: 14.62
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.7422973339817749
  num_agent_steps_sampled: 86150
  num_agent_steps_trained: 69500
  num_env_steps_sampled: 86150
  num_env_steps_trained: 69500
  num_samples_added_to_queue: 86000
  num_training_step_calls_since_last_synch_worker_weights: 667
  num_weight_broadcasts: 1693
  timing_breakdown:
    learner_dequeue_time_ms: 1413.001
    learner_grad_time_ms: 358.944
    learner_load_time_ms: 5.581
    learner_load_wait_time_ms: 2.819
iterations_since_restore: 11
node_ip: 127.0.0.1
num_agent_steps_sampled: 86150
num_agent_steps_trained: 69500
num_env_steps_sampled: 86150
num_env_steps_sampled_this_iter: 7950
num_env_steps_sampled_throughput_per_sec: 794.9960385758385
num_env_steps_trained: 69500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9960136612211
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 52.45714285714286
  ram_util_percent: 79.72142857142858
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10363300803235437
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03732796831656634
  mean_inference_ms: 1.9346454014997077
  mean_raw_obs_processing_ms: 0.44013015490299734
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03152060508728027
    StateBufferConnector_ms: 0.005689859390258789
    ViewRequirementAgentConnector_ms: 0.18828606605529785
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 1.35
  episode_reward_min: 0.0
  episodes_this_iter: 63
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 2.0, 1.0, 4.0, 1.0, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0,
      0.0, 0.0, 2.0, 1.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0,
      3.0, 1.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 0.0, 2.0,
      0.0, 3.0, 1.0, 3.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 2.0, 4.0, 3.0, 1.0,
      1.0, 3.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 0.0, 1.0, 1.0,
      3.0, 2.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 0.0, 0.0, 1.0, 2.0,
      1.0, 0.0, 7.0, 2.0, 1.0, 0.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10363300803235437
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03732796831656634
    mean_inference_ms: 1.9346454014997077
    mean_raw_obs_processing_ms: 0.44013015490299734
time_since_restore: 112.63372945785522
time_this_iter_s: 10.208216905593872
time_total_s: 112.63372945785522
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344209
timesteps_total: 86150
training_iteration: 11
trial_id: default
train step: 12
agent_timesteps_total: 93500
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03322720527648926
  StateBufferConnector_ms: 0.005995273590087891
  ViewRequirementAgentConnector_ms: 0.198563814163208
counters:
  num_agent_steps_sampled: 93500
  num_agent_steps_trained: 77000
  num_env_steps_sampled: 93500
  num_env_steps_trained: 77000
  num_samples_added_to_queue: 93500
  num_training_step_calls_since_last_synch_worker_weights: 425
  num_weight_broadcasts: 1837
custom_metrics: {}
date: 2023-08-18_16-37-00
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 1.52
episode_reward_min: 0.0
episodes_this_iter: 57
episodes_total: 731
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.3
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5614982843399048
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -0.0566481351852417
        total_loss: 3.838832378387451
        var_gnorm: 63.313018798828125
        vf_explained_var: 0.1148766279220581
        vf_loss: 9.352459907531738
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 154.0
  learner_queue:
    size_count: 159
    size_mean: 14.6
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.7320508075688772
  num_agent_steps_sampled: 93500
  num_agent_steps_trained: 77000
  num_env_steps_sampled: 93500
  num_env_steps_trained: 77000
  num_samples_added_to_queue: 93500
  num_training_step_calls_since_last_synch_worker_weights: 425
  num_weight_broadcasts: 1837
  timing_breakdown:
    learner_dequeue_time_ms: 1413.001
    learner_grad_time_ms: 359.485
    learner_load_time_ms: 5.581
    learner_load_wait_time_ms: 2.778
iterations_since_restore: 12
node_ip: 127.0.0.1
num_agent_steps_sampled: 93500
num_agent_steps_trained: 77000
num_env_steps_sampled: 93500
num_env_steps_sampled_this_iter: 7350
num_env_steps_sampled_throughput_per_sec: 734.9980198198261
num_env_steps_trained: 77000
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.9979794079859
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 56.786666666666655
  ram_util_percent: 78.53333333333333
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1039525280264861
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.037474607427106336
  mean_inference_ms: 1.9390061279788204
  mean_raw_obs_processing_ms: 0.44116353910341066
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03322720527648926
    StateBufferConnector_ms: 0.005995273590087891
    ViewRequirementAgentConnector_ms: 0.198563814163208
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 1.52
  episode_reward_min: 0.0
  episodes_this_iter: 57
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 4.0, 3.0, 1.0, 1.0, 3.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0,
      1.0, 3.0, 1.0, 1.0, 0.0, 1.0, 1.0, 3.0, 2.0, 0.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0,
      1.0, 3.0, 2.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 7.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0,
      4.0, 1.0, 3.0, 2.0, 1.0, 2.0, 0.0, 4.0, 1.0, 2.0, 1.0, 1.0, 4.0, 4.0, 1.0, 2.0,
      1.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 0.0, 0.0,
      4.0, 0.0, 2.0, 4.0, 1.0, 0.0, 2.0, 1.0, 2.0, 3.0, 0.0, 2.0, 1.0, 4.0, 1.0, 0.0,
      2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1039525280264861
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.037474607427106336
    mean_inference_ms: 1.9390061279788204
    mean_raw_obs_processing_ms: 0.44116353910341066
time_since_restore: 122.8517107963562
time_this_iter_s: 10.217981338500977
time_total_s: 122.8517107963562
timers:
  sample_time_ms: 0.027
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.08
timestamp: 1692344220
timesteps_total: 93500
training_iteration: 12
trial_id: default
train step: 13
agent_timesteps_total: 101450
connector_metrics:
  ObsPreprocessorConnector_ms: 0.033133506774902344
  StateBufferConnector_ms: 0.0059397220611572266
  ViewRequirementAgentConnector_ms: 0.1976931095123291
counters:
  num_agent_steps_sampled: 101450
  num_agent_steps_trained: 84500
  num_env_steps_sampled: 101450
  num_env_steps_trained: 84500
  num_samples_added_to_queue: 101000
  num_training_step_calls_since_last_synch_worker_weights: 241
  num_weight_broadcasts: 1992
custom_metrics: {}
date: 2023-08-18_16-37-10
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.44
episode_reward_min: 0.0
episodes_this_iter: 62
episodes_total: 793
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.553594708442688
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 30.147525787353516
        total_loss: 38.06187057495117
        var_gnorm: 63.313053131103516
        vf_explained_var: 0.05153369903564453
        vf_loss: 17.38228416442871
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 169.0
  learner_queue:
    size_count: 175
    size_mean: 14.64
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6584329953302306
  num_agent_steps_sampled: 101450
  num_agent_steps_trained: 84500
  num_env_steps_sampled: 101450
  num_env_steps_trained: 84500
  num_samples_added_to_queue: 101000
  num_training_step_calls_since_last_synch_worker_weights: 241
  num_weight_broadcasts: 1992
  timing_breakdown:
    learner_dequeue_time_ms: 363.553
    learner_grad_time_ms: 336.077
    learner_load_time_ms: 3.061
    learner_load_wait_time_ms: 2.613
iterations_since_restore: 13
node_ip: 127.0.0.1
num_agent_steps_sampled: 101450
num_agent_steps_trained: 84500
num_env_steps_sampled: 101450
num_env_steps_sampled_this_iter: 7950
num_env_steps_sampled_throughput_per_sec: 794.997858172465
num_env_steps_trained: 84500
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.9979794079859
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 50.707142857142856
  ram_util_percent: 77.67857142857143
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10412819987524619
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03761000779793876
  mean_inference_ms: 1.9423235119088542
  mean_raw_obs_processing_ms: 0.4417786042880762
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.033133506774902344
    StateBufferConnector_ms: 0.0059397220611572266
    ViewRequirementAgentConnector_ms: 0.1976931095123291
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.44
  episode_reward_min: 0.0
  episodes_this_iter: 62
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0,
      0.0, 0.0, 4.0, 0.0, 2.0, 4.0, 1.0, 0.0, 2.0, 1.0, 2.0, 3.0, 0.0, 2.0, 1.0, 4.0,
      1.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 2.0, 1.0, 3.0, 0.0, 0.0, 5.0, 2.0, 1.0,
      3.0, 3.0, 1.0, 2.0, 2.0, 0.0, 3.0, 3.0, 0.0, 3.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0,
      3.0, 1.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0, 1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 2.0, 1.0,
      1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 4.0, 1.0, 1.0, 0.0, 1.0, 2.0,
      2.0, 0.0, 0.0, 0.0, 5.0, 0.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10412819987524619
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03761000779793876
    mean_inference_ms: 1.9423235119088542
    mean_raw_obs_processing_ms: 0.4417786042880762
time_since_restore: 133.13203382492065
time_this_iter_s: 10.280323028564453
time_total_s: 133.13203382492065
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.077
timestamp: 1692344230
timesteps_total: 101450
training_iteration: 13
trial_id: default
train step: 14
agent_timesteps_total: 108650
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03503990173339844
  StateBufferConnector_ms: 0.0062901973724365234
  ViewRequirementAgentConnector_ms: 0.20740246772766113
counters:
  num_agent_steps_sampled: 108650
  num_agent_steps_trained: 92000
  num_env_steps_sampled: 108650
  num_env_steps_trained: 92000
  num_samples_added_to_queue: 108500
  num_training_step_calls_since_last_synch_worker_weights: 84
  num_weight_broadcasts: 2133
custom_metrics: {}
date: 2023-08-18_16-37-20
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.52
episode_reward_min: 0.0
episodes_this_iter: 56
episodes_total: 849
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5586038827896118
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 1.8333427906036377
        total_loss: 6.942894458770752
        var_gnorm: 63.31306457519531
        vf_explained_var: 0.056522905826568604
        vf_loss: 11.777708053588867
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 184.0
  learner_queue:
    size_count: 190
    size_mean: 14.64
    size_quantiles: [11.0, 12.0, 15.5, 16.0, 16.0]
    size_std: 1.6094719630984569
  num_agent_steps_sampled: 108650
  num_agent_steps_trained: 92000
  num_env_steps_sampled: 108650
  num_env_steps_trained: 92000
  num_samples_added_to_queue: 108500
  num_training_step_calls_since_last_synch_worker_weights: 84
  num_weight_broadcasts: 2133
  timing_breakdown:
    learner_dequeue_time_ms: 106.082
    learner_grad_time_ms: 395.287
    learner_load_time_ms: 3.062
    learner_load_wait_time_ms: 2.87
iterations_since_restore: 14
node_ip: 127.0.0.1
num_agent_steps_sampled: 108650
num_agent_steps_trained: 92000
num_env_steps_sampled: 108650
num_env_steps_sampled_this_iter: 7200
num_env_steps_sampled_throughput_per_sec: 719.9943180532628
num_env_steps_trained: 92000
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.9940813054822
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 65.76666666666667
  ram_util_percent: 77.69333333333334
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10440912825588006
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.037746564136324545
  mean_inference_ms: 1.9477171342514843
  mean_raw_obs_processing_ms: 0.4428010108355644
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03503990173339844
    StateBufferConnector_ms: 0.0062901973724365234
    ViewRequirementAgentConnector_ms: 0.20740246772766113
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.52
  episode_reward_min: 0.0
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 1.0, 1.0, 1.0, 3.0, 1.0, 3.0, 2.0, 0.0, 2.0, 2.0, 0.0,
      1.0, 1.0, 1.0, 1.0, 3.0, 2.0, 2.0, 1.0, 1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 0.0,
      2.0, 0.0, 4.0, 1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 0.0, 0.0, 0.0, 5.0, 0.0, 1.0, 1.0,
      4.0, 1.0, 3.0, 2.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0,
      1.0, 0.0, 3.0, 0.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 3.0, 0.0, 3.0, 1.0, 2.0,
      2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 1.0, 0.0, 1.0, 2.0, 1.0,
      1.0, 2.0, 1.0, 1.0, 0.0, 6.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10440912825588006
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.037746564136324545
    mean_inference_ms: 1.9477171342514843
    mean_raw_obs_processing_ms: 0.4428010108355644
time_since_restore: 143.373450756073
time_this_iter_s: 10.241416931152344
time_total_s: 143.373450756073
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.085
timestamp: 1692344240
timesteps_total: 108650
training_iteration: 14
trial_id: default
train step: 15
agent_timesteps_total: 116700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03481626510620117
  StateBufferConnector_ms: 0.006142139434814453
  ViewRequirementAgentConnector_ms: 0.2044057846069336
counters:
  num_agent_steps_sampled: 116700
  num_agent_steps_trained: 100000
  num_env_steps_sampled: 116700
  num_env_steps_trained: 100000
  num_samples_added_to_queue: 116500
  num_training_step_calls_since_last_synch_worker_weights: 1148
  num_weight_broadcasts: 2291
custom_metrics: {}
date: 2023-08-18_16-37-30
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.7
episode_reward_min: 0.0
episodes_this_iter: 63
episodes_total: 912
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5306620597839355
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -16.74027442932129
        total_loss: -12.197002410888672
        var_gnorm: 63.31309127807617
        vf_explained_var: 0.09519648551940918
        vf_loss: 10.617206573486328
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 200.0
  learner_queue:
    size_count: 204
    size_mean: 14.58
    size_quantiles: [11.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.6624078921853087
  num_agent_steps_sampled: 116700
  num_agent_steps_trained: 100000
  num_env_steps_sampled: 116700
  num_env_steps_trained: 100000
  num_samples_added_to_queue: 116500
  num_training_step_calls_since_last_synch_worker_weights: 1148
  num_weight_broadcasts: 2291
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 502.672
    learner_load_time_ms: 3.24
    learner_load_wait_time_ms: 2.71
iterations_since_restore: 15
node_ip: 127.0.0.1
num_agent_steps_sampled: 116700
num_agent_steps_trained: 100000
num_env_steps_sampled: 116700
num_env_steps_sampled_this_iter: 8050
num_env_steps_sampled_throughput_per_sec: 804.998963595771
num_env_steps_trained: 100000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9989700330643
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 48.021428571428565
  ram_util_percent: 78.35714285714285
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10449948005367758
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03780233450817733
  mean_inference_ms: 1.9497295267009451
  mean_raw_obs_processing_ms: 0.4431351328354748
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03481626510620117
    StateBufferConnector_ms: 0.006142139434814453
    ViewRequirementAgentConnector_ms: 0.2044057846069336
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.7
  episode_reward_min: 0.0
  episodes_this_iter: 63
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 0.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 3.0, 0.0, 3.0, 1.0,
      2.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0, 4.0, 1.0, 0.0, 1.0, 2.0,
      1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 6.0, 4.0, 1.0, 0.0, 0.0, 3.0, 1.0, 3.0, 2.0, 3.0,
      2.0, 2.0, 1.0, 1.0, 1.0, 3.0, 6.0, 0.0, 0.0, 2.0, 2.0, 2.0, 1.0, 5.0, 3.0, 1.0,
      0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 2.0, 0.0, 0.0, 2.0, 3.0, 1.0, 3.0, 3.0, 3.0, 2.0,
      0.0, 0.0, 3.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0,
      1.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10449948005367758
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03780233450817733
    mean_inference_ms: 1.9497295267009451
    mean_raw_obs_processing_ms: 0.4431351328354748
time_since_restore: 153.56348872184753
time_this_iter_s: 10.190037965774536
time_total_s: 153.56348872184753
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.073
timestamp: 1692344250
timesteps_total: 116700
training_iteration: 15
trial_id: default
train step: 16
agent_timesteps_total: 120300
connector_metrics:
  ObsPreprocessorConnector_ms: 0.04636645317077637
  StateBufferConnector_ms: 0.007972478866577148
  ViewRequirementAgentConnector_ms: 0.26128196716308594
counters:
  num_agent_steps_sampled: 120300
  num_agent_steps_trained: 103500
  num_env_steps_sampled: 120300
  num_env_steps_trained: 103500
  num_samples_added_to_queue: 120000
  num_training_step_calls_since_last_synch_worker_weights: 657
  num_weight_broadcasts: 2360
custom_metrics: {}
date: 2023-08-18_16-37-41
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.73
episode_reward_min: 0.0
episodes_this_iter: 28
episodes_total: 940
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.557051420211792
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 21.662147521972656
        total_loss: 30.77692413330078
        var_gnorm: 63.31312561035156
        vf_explained_var: 0.050952911376953125
        vf_loss: 19.78660774230957
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 207.0
  learner_queue:
    size_count: 214
    size_mean: 14.38
    size_quantiles: [10.0, 11.9, 15.0, 16.0, 16.0]
    size_std: 1.8208789086592223
  num_agent_steps_sampled: 120300
  num_agent_steps_trained: 103500
  num_env_steps_sampled: 120300
  num_env_steps_trained: 103500
  num_samples_added_to_queue: 120000
  num_training_step_calls_since_last_synch_worker_weights: 657
  num_weight_broadcasts: 2360
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 1033.813
    learner_load_time_ms: 3.462
    learner_load_wait_time_ms: 33.711
iterations_since_restore: 16
node_ip: 127.0.0.1
num_agent_steps_sampled: 120300
num_agent_steps_trained: 103500
num_env_steps_sampled: 120300
num_env_steps_sampled_this_iter: 3600
num_env_steps_sampled_throughput_per_sec: 359.9974508466029
num_env_steps_trained: 103500
num_env_steps_trained_this_iter: 3500
num_env_steps_trained_throughput_per_sec: 349.99752165641945
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 3500
perf:
  cpu_util_percent: 79.42
  ram_util_percent: 79.64666666666666
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10559973408920988
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03820672509732399
  mean_inference_ms: 1.9678516380290256
  mean_raw_obs_processing_ms: 0.4469787902278873
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.04636645317077637
    StateBufferConnector_ms: 0.007972478866577148
    ViewRequirementAgentConnector_ms: 0.26128196716308594
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.73
  episode_reward_min: 0.0
  episodes_this_iter: 28
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 6.0, 4.0, 1.0, 0.0, 0.0, 3.0,
      1.0, 3.0, 2.0, 3.0, 2.0, 2.0, 1.0, 1.0, 1.0, 3.0, 6.0, 0.0, 0.0, 2.0, 2.0, 2.0,
      1.0, 5.0, 3.0, 1.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0, 2.0, 0.0, 0.0, 2.0, 3.0, 1.0,
      3.0, 3.0, 3.0, 2.0, 0.0, 0.0, 3.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 2.0, 3.0, 1.0,
      0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0, 1.0, 2.0, 4.0, 3.0, 1.0,
      1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 0.0,
      3.0, 3.0, 1.0, 2.0, 3.0, 1.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10559973408920988
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03820672509732399
    mean_inference_ms: 1.9678516380290256
    mean_raw_obs_processing_ms: 0.4469787902278873
time_since_restore: 164.23820400238037
time_this_iter_s: 10.674715280532837
time_total_s: 164.23820400238037
timers:
  sample_time_ms: 0.037
  synch_weights_time_ms: 0.011
  training_iteration_time_ms: 0.11
timestamp: 1692344261
timesteps_total: 120300
training_iteration: 16
trial_id: default
train step: 17
agent_timesteps_total: 123600
connector_metrics:
  ObsPreprocessorConnector_ms: 0.05792546272277832
  StateBufferConnector_ms: 0.009855985641479492
  ViewRequirementAgentConnector_ms: 0.3315873146057129
counters:
  num_agent_steps_sampled: 123600
  num_agent_steps_trained: 108000
  num_env_steps_sampled: 123600
  num_env_steps_trained: 108000
  num_samples_added_to_queue: 123500
  num_training_step_calls_since_last_synch_worker_weights: 2093
  num_weight_broadcasts: 2423
custom_metrics: {}
date: 2023-08-18_16-37-51
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.69
episode_reward_min: 0.0
episodes_this_iter: 26
episodes_total: 966
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5373733043670654
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -14.114730834960938
        total_loss: -10.690267562866211
        var_gnorm: 63.313167572021484
        vf_explained_var: 0.18392300605773926
        vf_loss: 8.386300086975098
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 216.0
  learner_queue:
    size_count: 221
    size_mean: 13.84
    size_quantiles: [10.0, 11.0, 14.0, 16.0, 16.0]
    size_std: 1.9324595726689862
  num_agent_steps_sampled: 123600
  num_agent_steps_trained: 108000
  num_env_steps_sampled: 123600
  num_env_steps_trained: 108000
  num_samples_added_to_queue: 123500
  num_training_step_calls_since_last_synch_worker_weights: 2093
  num_weight_broadcasts: 2423
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 1002.046
    learner_load_time_ms: 3.462
    learner_load_wait_time_ms: 51.3
iterations_since_restore: 17
node_ip: 127.0.0.1
num_agent_steps_sampled: 123600
num_agent_steps_trained: 108000
num_env_steps_sampled: 123600
num_env_steps_sampled_this_iter: 3300
num_env_steps_sampled_throughput_per_sec: 329.99764754065
num_env_steps_trained: 108000
num_env_steps_trained_this_iter: 4500
num_env_steps_trained_throughput_per_sec: 449.9967921008864
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 4500
perf:
  cpu_util_percent: 78.61333333333333
  ram_util_percent: 79.33333333333333
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10760978722829967
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03906000191456076
  mean_inference_ms: 2.006072099681223
  mean_raw_obs_processing_ms: 0.4544982352661113
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.05792546272277832
    StateBufferConnector_ms: 0.009855985641479492
    ViewRequirementAgentConnector_ms: 0.3315873146057129
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.69
  episode_reward_min: 0.0
  episodes_this_iter: 26
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 2.0, 1.0, 5.0, 3.0, 1.0, 0.0, 3.0, 2.0, 1.0, 5.0, 1.0,
      2.0, 0.0, 0.0, 2.0, 3.0, 1.0, 3.0, 3.0, 3.0, 2.0, 0.0, 0.0, 3.0, 1.0, 3.0, 4.0,
      2.0, 1.0, 3.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 3.0,
      2.0, 1.0, 2.0, 4.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 0.0, 2.0, 2.0, 2.0,
      1.0, 2.0, 2.0, 2.0, 0.0, 0.0, 3.0, 3.0, 1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 5.0, 1.0,
      2.0, 2.0, 1.0, 2.0, 5.0, 2.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0,
      1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10760978722829967
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03906000191456076
    mean_inference_ms: 2.006072099681223
    mean_raw_obs_processing_ms: 0.4544982352661113
time_since_restore: 174.47121286392212
time_this_iter_s: 10.233008861541748
time_total_s: 174.47121286392212
timers:
  sample_time_ms: 0.027
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.078
timestamp: 1692344271
timesteps_total: 123600
training_iteration: 17
trial_id: default
train step: 18
agent_timesteps_total: 128150
connector_metrics:
  ObsPreprocessorConnector_ms: 0.07130169868469238
  StateBufferConnector_ms: 0.012668132781982422
  ViewRequirementAgentConnector_ms: 0.40949082374572754
counters:
  num_agent_steps_sampled: 128150
  num_agent_steps_trained: 112000
  num_env_steps_sampled: 128150
  num_env_steps_trained: 112000
  num_samples_added_to_queue: 128000
  num_training_step_calls_since_last_synch_worker_weights: 337
  num_weight_broadcasts: 2511
custom_metrics: {}
date: 2023-08-18_16-38-02
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.7
episode_reward_min: 0.0
episodes_this_iter: 36
episodes_total: 1002
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.5
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5403592586517334
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -8.905715942382812
        total_loss: -3.1427674293518066
        var_gnorm: 63.31322479248047
        vf_explained_var: 0.1273181438446045
        vf_loss: 13.066256523132324
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 224.0
  learner_queue:
    size_count: 231
    size_mean: 13.52
    size_quantiles: [9.0, 11.0, 14.0, 16.0, 16.0]
    size_std: 2.0517309765171454
  num_agent_steps_sampled: 128150
  num_agent_steps_trained: 112000
  num_env_steps_sampled: 128150
  num_env_steps_trained: 112000
  num_samples_added_to_queue: 128000
  num_training_step_calls_since_last_synch_worker_weights: 337
  num_weight_broadcasts: 2511
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 1016.152
    learner_load_time_ms: 10.717
    learner_load_wait_time_ms: 25.192
iterations_since_restore: 18
node_ip: 127.0.0.1
num_agent_steps_sampled: 128150
num_agent_steps_trained: 112000
num_env_steps_sampled: 128150
num_env_steps_sampled_this_iter: 4550
num_env_steps_sampled_throughput_per_sec: 454.9925475149121
num_env_steps_trained: 112000
num_env_steps_trained_this_iter: 4000
num_env_steps_trained_throughput_per_sec: 399.99344836475785
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 4000
perf:
  cpu_util_percent: 66.05714285714285
  ram_util_percent: 79.44285714285715
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11138158759175201
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.040566339631350166
  mean_inference_ms: 2.074305449554392
  mean_raw_obs_processing_ms: 0.46848102041499023
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.07130169868469238
    StateBufferConnector_ms: 0.012668132781982422
    ViewRequirementAgentConnector_ms: 0.40949082374572754
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.7
  episode_reward_min: 0.0
  episodes_this_iter: 36
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 3.0, 2.0, 1.0, 2.0, 4.0,
      3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 3.0, 0.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0,
      0.0, 0.0, 3.0, 3.0, 1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 5.0, 1.0, 2.0, 2.0, 1.0, 2.0,
      5.0, 2.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 0.0, 2.0, 2.0,
      0.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 2.0, 1.0, 4.0, 3.0, 3.0, 2.0, 3.0, 1.0,
      3.0, 2.0, 0.0, 1.0, 6.0, 4.0, 3.0, 1.0, 3.0, 0.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0,
      2.0, 3.0, 1.0, 0.0, 2.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11138158759175201
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.040566339631350166
    mean_inference_ms: 2.074305449554392
    mean_raw_obs_processing_ms: 0.46848102041499023
time_since_restore: 184.8566837310791
time_this_iter_s: 10.385470867156982
time_total_s: 184.8566837310791
timers:
  sample_time_ms: 0.041
  synch_weights_time_ms: 0.012
  training_iteration_time_ms: 0.117
timestamp: 1692344282
timesteps_total: 128150
training_iteration: 18
trial_id: default
train step: 19
agent_timesteps_total: 132950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.06510353088378906
  StateBufferConnector_ms: 0.012253761291503906
  ViewRequirementAgentConnector_ms: 0.3883793354034424
counters:
  num_agent_steps_sampled: 132950
  num_agent_steps_trained: 117000
  num_env_steps_sampled: 132950
  num_env_steps_trained: 117000
  num_samples_added_to_queue: 132500
  num_training_step_calls_since_last_synch_worker_weights: 828
  num_weight_broadcasts: 2604
custom_metrics: {}
date: 2023-08-18_16-38-12
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 1.83
episode_reward_min: 0.0
episodes_this_iter: 38
episodes_total: 1040
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.5
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5107065439224243
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 0.8455989360809326
        total_loss: 7.5011725425720215
        var_gnorm: 63.31325912475586
        vf_explained_var: 0.16300606727600098
        vf_loss: 14.821853637695312
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 234.0
  learner_queue:
    size_count: 240
    size_mean: 12.98
    size_quantiles: [9.0, 10.0, 13.0, 16.0, 16.0]
    size_std: 2.14
  num_agent_steps_sampled: 132950
  num_agent_steps_trained: 117000
  num_env_steps_sampled: 132950
  num_env_steps_trained: 117000
  num_samples_added_to_queue: 132500
  num_training_step_calls_since_last_synch_worker_weights: 828
  num_weight_broadcasts: 2604
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 992.917
    learner_load_time_ms: 10.444
    learner_load_wait_time_ms: 41.507
iterations_since_restore: 19
node_ip: 127.0.0.1
num_agent_steps_sampled: 132950
num_agent_steps_trained: 117000
num_env_steps_sampled: 132950
num_env_steps_sampled_this_iter: 4800
num_env_steps_sampled_throughput_per_sec: 479.99955368083494
num_env_steps_trained: 117000
num_env_steps_trained_this_iter: 5000
num_env_steps_trained_throughput_per_sec: 499.99953508420305
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5000
perf:
  cpu_util_percent: 60.38666666666666
  ram_util_percent: 79.89333333333335
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11499998080790805
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.041991818151843475
  mean_inference_ms: 2.1395422997656857
  mean_raw_obs_processing_ms: 0.48219081651801515
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.06510353088378906
    StateBufferConnector_ms: 0.012253761291503906
    ViewRequirementAgentConnector_ms: 0.3883793354034424
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 1.83
  episode_reward_min: 0.0
  episodes_this_iter: 38
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 5.0, 1.0, 2.0, 2.0, 1.0, 2.0, 5.0, 2.0, 3.0, 1.0, 0.0, 1.0,
      2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 1.0, 3.0, 2.0,
      3.0, 2.0, 2.0, 1.0, 4.0, 3.0, 3.0, 2.0, 3.0, 1.0, 3.0, 2.0, 0.0, 1.0, 6.0, 4.0,
      3.0, 1.0, 3.0, 0.0, 3.0, 0.0, 1.0, 1.0, 1.0, 2.0, 2.0, 3.0, 1.0, 0.0, 2.0, 1.0,
      1.0, 2.0, 2.0, 2.0, 2.0, 1.0, 0.0, 0.0, 1.0, 3.0, 1.0, 0.0, 2.0, 2.0, 4.0, 3.0,
      0.0, 3.0, 0.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 0.0, 1.0, 4.0, 1.0,
      6.0, 4.0, 0.0, 0.0, 4.0, 7.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11499998080790805
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.041991818151843475
    mean_inference_ms: 2.1395422997656857
    mean_raw_obs_processing_ms: 0.48219081651801515
time_since_restore: 195.15931057929993
time_this_iter_s: 10.302626848220825
time_total_s: 195.15931057929993
timers:
  sample_time_ms: 0.031
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.091
timestamp: 1692344292
timesteps_total: 132950
training_iteration: 19
trial_id: default
train step: 20
agent_timesteps_total: 139750
connector_metrics:
  ObsPreprocessorConnector_ms: 0.04852890968322754
  StateBufferConnector_ms: 0.0085906982421875
  ViewRequirementAgentConnector_ms: 0.27309322357177734
counters:
  num_agent_steps_sampled: 139750
  num_agent_steps_trained: 123000
  num_env_steps_sampled: 139750
  num_env_steps_trained: 123000
  num_samples_added_to_queue: 139500
  num_training_step_calls_since_last_synch_worker_weights: 643
  num_weight_broadcasts: 2737
custom_metrics: {}
date: 2023-08-18_16-38-22
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 1.6
episode_reward_min: 0.0
episodes_this_iter: 52
episodes_total: 1092
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.5
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5376431941986084
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 5.688081741333008
        total_loss: 12.012218475341797
        var_gnorm: 63.31327438354492
        vf_explained_var: 0.10864925384521484
        vf_loss: 14.185917854309082
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 246.0
  learner_queue:
    size_count: 252
    size_mean: 12.64
    size_quantiles: [9.0, 10.0, 13.0, 15.100000000000001, 16.0]
    size_std: 1.9975985582694038
  num_agent_steps_sampled: 139750
  num_agent_steps_trained: 123000
  num_env_steps_sampled: 139750
  num_env_steps_trained: 123000
  num_samples_added_to_queue: 139500
  num_training_step_calls_since_last_synch_worker_weights: 643
  num_weight_broadcasts: 2737
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 662.042
    learner_load_time_ms: 10.444
    learner_load_wait_time_ms: 7.285
iterations_since_restore: 20
node_ip: 127.0.0.1
num_agent_steps_sampled: 139750
num_agent_steps_trained: 123000
num_env_steps_sampled: 139750
num_env_steps_sampled_this_iter: 6800
num_env_steps_sampled_throughput_per_sec: 679.9935961372739
num_env_steps_trained: 123000
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9943495328888
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 56.26666666666666
  ram_util_percent: 80.62000000000002
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11691063849480424
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.042787733917129714
  mean_inference_ms: 2.174715825845615
  mean_raw_obs_processing_ms: 0.4896911606172702
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.04852890968322754
    StateBufferConnector_ms: 0.0085906982421875
    ViewRequirementAgentConnector_ms: 0.27309322357177734
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 1.6
  episode_reward_min: 0.0
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 2.0, 2.0, 3.0, 1.0, 0.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0,
      2.0, 1.0, 0.0, 0.0, 1.0, 3.0, 1.0, 0.0, 2.0, 2.0, 4.0, 3.0, 0.0, 3.0, 0.0, 2.0,
      0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 3.0, 0.0, 1.0, 4.0, 1.0, 6.0, 4.0, 0.0, 0.0,
      4.0, 7.0, 4.0, 1.0, 0.0, 1.0, 1.0, 3.0, 2.0, 1.0, 1.0, 0.0, 2.0, 1.0, 1.0, 1.0,
      0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 3.0, 3.0, 3.0, 1.0, 2.0,
      1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 4.0, 2.0, 1.0, 3.0,
      2.0, 2.0, 3.0, 1.0, 1.0, 2.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11691063849480424
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.042787733917129714
    mean_inference_ms: 2.174715825845615
    mean_raw_obs_processing_ms: 0.4896911606172702
time_since_restore: 205.52860832214355
time_this_iter_s: 10.369297742843628
time_total_s: 205.52860832214355
timers:
  sample_time_ms: 0.032
  synch_weights_time_ms: 0.01
  training_iteration_time_ms: 0.096
timestamp: 1692344302
timesteps_total: 139750
training_iteration: 20
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000020'.
train step: 21
agent_timesteps_total: 147200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.038628578186035156
  StateBufferConnector_ms: 0.006417036056518555
  ViewRequirementAgentConnector_ms: 0.211899995803833
counters:
  num_agent_steps_sampled: 147200
  num_agent_steps_trained: 130500
  num_env_steps_sampled: 147200
  num_env_steps_trained: 130500
  num_samples_added_to_queue: 147000
  num_training_step_calls_since_last_synch_worker_weights: 4
  num_weight_broadcasts: 2883
custom_metrics: {}
date: 2023-08-18_16-38-33
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.69
episode_reward_min: 0.0
episodes_this_iter: 58
episodes_total: 1150
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 28.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5286537408828735
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -1.8255178928375244
        total_loss: 5.090271949768066
        var_gnorm: 63.31330490112305
        vf_explained_var: 0.11131298542022705
        vf_loss: 15.360233306884766
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 261.0
  learner_queue:
    size_count: 268
    size_mean: 12.88
    size_quantiles: [9.0, 10.0, 13.0, 16.0, 16.0]
    size_std: 2.2148589119851403
  num_agent_steps_sampled: 147200
  num_agent_steps_trained: 130500
  num_env_steps_sampled: 147200
  num_env_steps_trained: 130500
  num_samples_added_to_queue: 147000
  num_training_step_calls_since_last_synch_worker_weights: 4
  num_weight_broadcasts: 2883
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 265.195
    learner_load_time_ms: 10.667
    learner_load_wait_time_ms: 2.926
iterations_since_restore: 21
node_ip: 127.0.0.1
num_agent_steps_sampled: 147200
num_agent_steps_trained: 130500
num_env_steps_sampled: 147200
num_env_steps_sampled_this_iter: 7450
num_env_steps_sampled_throughput_per_sec: 744.9976198749289
num_env_steps_trained: 130500
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.9976039009351
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 53.92857142857144
  ram_util_percent: 81.00714285714285
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11704675143179014
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04292650562632366
  mean_inference_ms: 2.1766199715708625
  mean_raw_obs_processing_ms: 0.4900220806312801
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.038628578186035156
    StateBufferConnector_ms: 0.006417036056518555
    ViewRequirementAgentConnector_ms: 0.211899995803833
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.69
  episode_reward_min: 0.0
  episodes_this_iter: 58
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 1.0,
      2.0, 3.0, 3.0, 3.0, 1.0, 2.0, 1.0, 3.0, 3.0, 3.0, 2.0, 1.0, 0.0, 1.0, 1.0, 1.0,
      0.0, 1.0, 4.0, 2.0, 1.0, 3.0, 2.0, 2.0, 3.0, 1.0, 1.0, 2.0, 3.0, 1.0, 0.0, 2.0,
      0.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 3.0, 0.0, 4.0, 1.0, 1.0, 1.0, 0.0, 3.0, 3.0,
      1.0, 4.0, 3.0, 1.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 2.0, 1.0, 2.0, 2.0, 3.0, 4.0,
      2.0, 0.0, 1.0, 2.0, 1.0, 3.0, 0.0, 2.0, 3.0, 1.0, 1.0, 0.0, 2.0, 3.0, 0.0, 1.0,
      1.0, 6.0, 1.0, 1.0, 2.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11704675143179014
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04292650562632366
    mean_inference_ms: 2.1766199715708625
    mean_raw_obs_processing_ms: 0.4900220806312801
time_since_restore: 215.83319211006165
time_this_iter_s: 10.30458378791809
time_total_s: 215.83319211006165
timers:
  sample_time_ms: 0.133
  synch_weights_time_ms: 0.381
  training_iteration_time_ms: 0.625
timestamp: 1692344313
timesteps_total: 147200
training_iteration: 21
trial_id: default
train step: 22
agent_timesteps_total: 154250
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03453636169433594
  StateBufferConnector_ms: 0.00623631477355957
  ViewRequirementAgentConnector_ms: 0.20623350143432617
counters:
  num_agent_steps_sampled: 154250
  num_agent_steps_trained: 137500
  num_env_steps_sampled: 154250
  num_env_steps_trained: 137500
  num_samples_added_to_queue: 154000
  num_training_step_calls_since_last_synch_worker_weights: 773
  num_weight_broadcasts: 3021
custom_metrics: {}
date: 2023-08-18_16-38-43
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.75
episode_reward_min: 0.0
episodes_this_iter: 56
episodes_total: 1206
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4938701391220093
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -3.7453701496124268
        total_loss: 1.951378345489502
        var_gnorm: 63.313350677490234
        vf_explained_var: 0.11467069387435913
        vf_loss: 12.887367248535156
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 275.0
  learner_queue:
    size_count: 280
    size_mean: 13.44
    size_quantiles: [9.0, 10.0, 13.5, 16.0, 16.0]
    size_std: 2.255304857441672
  num_agent_steps_sampled: 154250
  num_agent_steps_trained: 137500
  num_env_steps_sampled: 154250
  num_env_steps_trained: 137500
  num_samples_added_to_queue: 154000
  num_training_step_calls_since_last_synch_worker_weights: 773
  num_weight_broadcasts: 3021
  timing_breakdown:
    learner_dequeue_time_ms: 0.007
    learner_grad_time_ms: 701.661
    learner_load_time_ms: 11.543
    learner_load_wait_time_ms: 4.95
iterations_since_restore: 22
node_ip: 127.0.0.1
num_agent_steps_sampled: 154250
num_agent_steps_trained: 137500
num_env_steps_sampled: 154250
num_env_steps_sampled_this_iter: 7050
num_env_steps_sampled_throughput_per_sec: 704.9939321801778
num_env_steps_trained: 137500
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.993975214361
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 57.70666666666667
  ram_util_percent: 80.92666666666668
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11683873958250426
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.042880384312149876
  mean_inference_ms: 2.1738151629958717
  mean_raw_obs_processing_ms: 0.4893140738977861
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03453636169433594
    StateBufferConnector_ms: 0.00623631477355957
    ViewRequirementAgentConnector_ms: 0.20623350143432617
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.75
  episode_reward_min: 0.0
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 0.0, 3.0, 3.0, 1.0, 4.0, 3.0, 1.0, 2.0, 2.0, 3.0, 3.0,
      3.0, 3.0, 2.0, 1.0, 2.0, 2.0, 3.0, 4.0, 2.0, 0.0, 1.0, 2.0, 1.0, 3.0, 0.0, 2.0,
      3.0, 1.0, 1.0, 0.0, 2.0, 3.0, 0.0, 1.0, 1.0, 6.0, 1.0, 1.0, 2.0, 2.0, 2.0, 0.0,
      1.0, 0.0, 4.0, 1.0, 0.0, 2.0, 3.0, 1.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0,
      0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 2.0, 1.0,
      2.0, 0.0, 3.0, 2.0, 0.0, 0.0, 4.0, 3.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0, 1.0,
      3.0, 1.0, 3.0, 0.0, 1.0, 3.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11683873958250426
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.042880384312149876
    mean_inference_ms: 2.1738151629958717
    mean_raw_obs_processing_ms: 0.4893140738977861
time_since_restore: 226.04043626785278
time_this_iter_s: 10.207244157791138
time_total_s: 226.04043626785278
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.087
timestamp: 1692344323
timesteps_total: 154250
training_iteration: 22
trial_id: default
train step: 23
agent_timesteps_total: 160250
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03952646255493164
  StateBufferConnector_ms: 0.00726008415222168
  ViewRequirementAgentConnector_ms: 0.233964204788208
counters:
  num_agent_steps_sampled: 160250
  num_agent_steps_trained: 143500
  num_env_steps_sampled: 160250
  num_env_steps_trained: 143500
  num_samples_added_to_queue: 160000
  num_training_step_calls_since_last_synch_worker_weights: 176
  num_weight_broadcasts: 3138
custom_metrics: {}
date: 2023-08-18_16-38-53
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.71
episode_reward_min: 0.0
episodes_this_iter: 47
episodes_total: 1253
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.099999999999994
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5080182552337646
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -12.364017486572266
        total_loss: -6.320644378662109
        var_gnorm: 63.31342315673828
        vf_explained_var: 0.12874507904052734
        vf_loss: 13.594764709472656
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 287.0
  learner_queue:
    size_count: 294
    size_mean: 14.24
    size_quantiles: [10.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 1.9032603605392509
  num_agent_steps_sampled: 160250
  num_agent_steps_trained: 143500
  num_env_steps_sampled: 160250
  num_env_steps_trained: 143500
  num_samples_added_to_queue: 160000
  num_training_step_calls_since_last_synch_worker_weights: 176
  num_weight_broadcasts: 3138
  timing_breakdown:
    learner_dequeue_time_ms: 0.007
    learner_grad_time_ms: 315.679
    learner_load_time_ms: 11.58
    learner_load_wait_time_ms: 2.92
iterations_since_restore: 23
node_ip: 127.0.0.1
num_agent_steps_sampled: 160250
num_agent_steps_trained: 143500
num_env_steps_sampled: 160250
num_env_steps_sampled_this_iter: 6000
num_env_steps_sampled_throughput_per_sec: 599.9977827154083
num_env_steps_trained: 143500
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9977827154083
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 56.77857142857142
  ram_util_percent: 81.20714285714287
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1171690247761787
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.043006208886728244
  mean_inference_ms: 2.1780698456588166
  mean_raw_obs_processing_ms: 0.49060816956245196
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03952646255493164
    StateBufferConnector_ms: 0.00726008415222168
    ViewRequirementAgentConnector_ms: 0.233964204788208
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.71
  episode_reward_min: 0.0
  episodes_this_iter: 47
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 1.0, 0.0, 2.0, 3.0, 1.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 3.0,
      0.0, 0.0, 1.0, 1.0, 1.0, 2.0, 0.0, 1.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 2.0,
      1.0, 2.0, 0.0, 3.0, 2.0, 0.0, 0.0, 4.0, 3.0, 2.0, 2.0, 2.0, 2.0, 4.0, 2.0, 2.0,
      1.0, 3.0, 1.0, 3.0, 0.0, 1.0, 3.0, 2.0, 0.0, 2.0, 4.0, 0.0, 1.0, 1.0, 0.0, 4.0,
      1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 3.0, 2.0,
      3.0, 2.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 4.0, 3.0, 3.0, 3.0, 2.0, 1.0, 0.0, 1.0,
      5.0, 2.0, 1.0, 3.0, 3.0, 3.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1171690247761787
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.043006208886728244
    mean_inference_ms: 2.1780698456588166
    mean_raw_obs_processing_ms: 0.49060816956245196
time_since_restore: 236.33067512512207
time_this_iter_s: 10.290238857269287
time_total_s: 236.33067512512207
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.089
timestamp: 1692344333
timesteps_total: 160250
training_iteration: 23
trial_id: default
train step: 24
agent_timesteps_total: 166900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.04188108444213867
  StateBufferConnector_ms: 0.007750988006591797
  ViewRequirementAgentConnector_ms: 0.2460017204284668
counters:
  num_agent_steps_sampled: 166900
  num_agent_steps_trained: 150000
  num_env_steps_sampled: 166900
  num_env_steps_trained: 150000
  num_samples_added_to_queue: 166500
  num_training_step_calls_since_last_synch_worker_weights: 624
  num_weight_broadcasts: 3268
custom_metrics: {}
date: 2023-08-18_16-39-04
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 5.0
episode_reward_mean: 1.93
episode_reward_min: 0.0
episodes_this_iter: 52
episodes_total: 1305
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.5085448026657104
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -6.447290897369385
        total_loss: -3.07346248626709
        var_gnorm: 63.31353759765625
        vf_explained_var: 0.1938025951385498
        vf_loss: 8.25620174407959
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 300.0
  learner_queue:
    size_count: 305
    size_mean: 14.36
    size_quantiles: [10.0, 11.9, 15.0, 16.0, 16.0]
    size_std: 1.862900963551203
  num_agent_steps_sampled: 166900
  num_agent_steps_trained: 150000
  num_env_steps_sampled: 166900
  num_env_steps_trained: 150000
  num_samples_added_to_queue: 166500
  num_training_step_calls_since_last_synch_worker_weights: 624
  num_weight_broadcasts: 3268
  timing_breakdown:
    learner_dequeue_time_ms: 0.008
    learner_grad_time_ms: 857.908
    learner_load_time_ms: 11.591
    learner_load_wait_time_ms: 3.956
iterations_since_restore: 24
node_ip: 127.0.0.1
num_agent_steps_sampled: 166900
num_agent_steps_trained: 150000
num_env_steps_sampled: 166900
num_env_steps_sampled_this_iter: 6650
num_env_steps_sampled_throughput_per_sec: 664.9943557264653
num_env_steps_trained: 150000
num_env_steps_trained_this_iter: 6500
num_env_steps_trained_throughput_per_sec: 649.994483040906
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6500
perf:
  cpu_util_percent: 53.12666666666667
  ram_util_percent: 79.26666666666667
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1176436281191111
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.043284205429839545
  mean_inference_ms: 2.187137413386863
  mean_raw_obs_processing_ms: 0.4926788191552693
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.04188108444213867
    StateBufferConnector_ms: 0.007750988006591797
    ViewRequirementAgentConnector_ms: 0.2460017204284668
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 5.0
  episode_reward_mean: 1.93
  episode_reward_min: 0.0
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 0.0, 2.0, 4.0, 0.0, 1.0, 1.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0,
      2.0, 1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 3.0, 2.0, 3.0, 2.0, 0.0, 3.0,
      1.0, 0.0, 1.0, 0.0, 4.0, 3.0, 3.0, 3.0, 2.0, 1.0, 0.0, 1.0, 5.0, 2.0, 1.0, 3.0,
      3.0, 3.0, 3.0, 3.0, 2.0, 2.0, 2.0, 1.0, 1.0, 3.0, 3.0, 1.0, 2.0, 5.0, 5.0, 2.0,
      0.0, 3.0, 4.0, 2.0, 2.0, 3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 1.0,
      2.0, 1.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 4.0, 2.0, 0.0, 2.0, 0.0, 3.0, 4.0,
      1.0, 1.0, 4.0, 1.0, 2.0, 2.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1176436281191111
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.043284205429839545
    mean_inference_ms: 2.187137413386863
    mean_raw_obs_processing_ms: 0.4926788191552693
time_since_restore: 246.56887197494507
time_this_iter_s: 10.238196849822998
time_total_s: 246.56887197494507
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344344
timesteps_total: 166900
training_iteration: 24
trial_id: default
train step: 25
agent_timesteps_total: 173750
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03857541084289551
  StateBufferConnector_ms: 0.007258176803588867
  ViewRequirementAgentConnector_ms: 0.2297055721282959
counters:
  num_agent_steps_sampled: 173750
  num_agent_steps_trained: 157000
  num_env_steps_sampled: 173750
  num_env_steps_trained: 157000
  num_samples_added_to_queue: 173500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 3402
custom_metrics: {}
date: 2023-08-18_16-39-14
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.01
episode_reward_min: 0.0
episodes_this_iter: 52
episodes_total: 1357
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.48323655128479
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 34.67318344116211
        total_loss: 43.876617431640625
        var_gnorm: 63.3137092590332
        vf_explained_var: 0.10790383815765381
        vf_loss: 19.890104293823242
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 314.0
  learner_queue:
    size_count: 319
    size_mean: 14.48
    size_quantiles: [10.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.734819875376115
  num_agent_steps_sampled: 173750
  num_agent_steps_trained: 157000
  num_env_steps_sampled: 173750
  num_env_steps_trained: 157000
  num_samples_added_to_queue: 173500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 3402
  timing_breakdown:
    learner_dequeue_time_ms: 0.008
    learner_grad_time_ms: 422.691
    learner_load_time_ms: 11.591
    learner_load_wait_time_ms: 3.024
iterations_since_restore: 25
node_ip: 127.0.0.1
num_agent_steps_sampled: 173750
num_agent_steps_trained: 157000
num_env_steps_sampled: 173750
num_env_steps_sampled_this_iter: 6850
num_env_steps_sampled_throughput_per_sec: 684.8000443901146
num_env_steps_trained: 157000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.7956658001171
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 60.89999999999999
  ram_util_percent: 79.35714285714288
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11776669288705893
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.043406311388973016
  mean_inference_ms: 2.1905953400136067
  mean_raw_obs_processing_ms: 0.4932404731841656
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03857541084289551
    StateBufferConnector_ms: 0.007258176803588867
    ViewRequirementAgentConnector_ms: 0.2297055721282959
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.01
  episode_reward_min: 0.0
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 3.0, 3.0, 1.0, 2.0, 5.0, 5.0, 2.0, 0.0, 3.0, 4.0, 2.0,
      2.0, 3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 1.0, 2.0, 1.0, 3.0, 2.0,
      2.0, 3.0, 3.0, 2.0, 2.0, 4.0, 2.0, 0.0, 2.0, 0.0, 3.0, 4.0, 1.0, 1.0, 4.0, 1.0,
      2.0, 2.0, 1.0, 0.0, 1.0, 6.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 4.0,
      1.0, 3.0, 3.0, 2.0, 0.0, 0.0, 2.0, 2.0, 4.0, 1.0, 1.0, 6.0, 1.0, 0.0, 1.0, 1.0,
      1.0, 3.0, 3.0, 0.0, 2.0, 2.0, 1.0, 2.0, 0.0, 1.0, 5.0, 2.0, 4.0, 1.0, 1.0, 3.0,
      5.0, 1.0, 3.0, 1.0, 1.0, 0.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11776669288705893
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.043406311388973016
    mean_inference_ms: 2.1905953400136067
    mean_raw_obs_processing_ms: 0.4932404731841656
time_since_restore: 256.78784584999084
time_this_iter_s: 10.218973875045776
time_total_s: 256.78784584999084
timers:
  sample_time_ms: 0.061
  synch_weights_time_ms: 0.413
  training_iteration_time_ms: 0.605
timestamp: 1692344354
timesteps_total: 173750
training_iteration: 25
trial_id: default
train step: 26
agent_timesteps_total: 181750
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03368234634399414
  StateBufferConnector_ms: 0.0062160491943359375
  ViewRequirementAgentConnector_ms: 0.20109224319458008
counters:
  num_agent_steps_sampled: 181750
  num_agent_steps_trained: 165000
  num_env_steps_sampled: 181750
  num_env_steps_trained: 165000
  num_samples_added_to_queue: 181500
  num_training_step_calls_since_last_synch_worker_weights: 300
  num_weight_broadcasts: 3560
custom_metrics: {}
date: 2023-08-18_16-39-24
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 9.0
episode_reward_mean: 2.06
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1421
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4765881299972534
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -24.399141311645508
        total_loss: -19.304433822631836
        var_gnorm: 63.3138313293457
        vf_explained_var: 0.08196824789047241
        vf_loss: 11.66600227355957
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 330.0
  learner_queue:
    size_count: 336
    size_mean: 14.46
    size_quantiles: [10.0, 11.9, 15.0, 16.0, 16.0]
    size_std: 1.8023318229449317
  num_agent_steps_sampled: 181750
  num_agent_steps_trained: 165000
  num_env_steps_sampled: 181750
  num_env_steps_trained: 165000
  num_samples_added_to_queue: 181500
  num_training_step_calls_since_last_synch_worker_weights: 300
  num_weight_broadcasts: 3560
  timing_breakdown:
    learner_dequeue_time_ms: 0.008
    learner_grad_time_ms: 292.803
    learner_load_time_ms: 11.767
    learner_load_wait_time_ms: 2.67
iterations_since_restore: 26
node_ip: 127.0.0.1
num_agent_steps_sampled: 181750
num_agent_steps_trained: 165000
num_env_steps_sampled: 181750
num_env_steps_sampled_this_iter: 8000
num_env_steps_sampled_throughput_per_sec: 799.993763018596
num_env_steps_trained: 165000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.993763018596
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 50.406666666666666
  ram_util_percent: 78.64666666666668
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11730278393306168
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04326369967318321
  mean_inference_ms: 2.1843486133682357
  mean_raw_obs_processing_ms: 0.4916094602954227
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03368234634399414
    StateBufferConnector_ms: 0.0062160491943359375
    ViewRequirementAgentConnector_ms: 0.20109224319458008
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 9.0
  episode_reward_mean: 2.06
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 0.0, 0.0, 2.0, 2.0, 4.0, 1.0, 1.0, 6.0, 1.0, 0.0, 1.0, 1.0,
      1.0, 3.0, 3.0, 0.0, 2.0, 2.0, 1.0, 2.0, 0.0, 1.0, 5.0, 2.0, 4.0, 1.0, 1.0, 3.0,
      5.0, 1.0, 3.0, 1.0, 1.0, 0.0, 3.0, 1.0, 9.0, 4.0, 1.0, 0.0, 1.0, 2.0, 3.0, 4.0,
      0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 1.0, 3.0, 2.0, 3.0, 0.0, 4.0, 1.0, 1.0, 4.0,
      2.0, 3.0, 1.0, 1.0, 3.0, 2.0, 0.0, 7.0, 1.0, 2.0, 0.0, 3.0, 0.0, 4.0, 3.0, 3.0,
      1.0, 3.0, 4.0, 2.0, 4.0, 3.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 3.0, 3.0, 2.0, 6.0,
      3.0, 0.0, 5.0, 1.0, 2.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11730278393306168
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04326369967318321
    mean_inference_ms: 2.1843486133682357
    mean_raw_obs_processing_ms: 0.4916094602954227
time_since_restore: 267.0098948478699
time_this_iter_s: 10.222048997879028
time_total_s: 267.0098948478699
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.075
timestamp: 1692344364
timesteps_total: 181750
training_iteration: 26
trial_id: default
train step: 27
agent_timesteps_total: 190200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03036355972290039
  StateBufferConnector_ms: 0.005377292633056641
  ViewRequirementAgentConnector_ms: 0.18309283256530762
counters:
  num_agent_steps_sampled: 190200
  num_agent_steps_trained: 173500
  num_env_steps_sampled: 190200
  num_env_steps_trained: 173500
  num_samples_added_to_queue: 190000
  num_training_step_calls_since_last_synch_worker_weights: 665
  num_weight_broadcasts: 3726
custom_metrics: {}
date: 2023-08-18_16-39-34
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.25
episode_reward_min: 0.0
episodes_this_iter: 65
episodes_total: 1486
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4859532117843628
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 8.538675308227539
        total_loss: 17.009784698486328
        var_gnorm: 63.313907623291016
        vf_explained_var: 0.12203657627105713
        vf_loss: 18.428171157836914
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 347.0
  learner_queue:
    size_count: 352
    size_mean: 14.76
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.59449051423958
  num_agent_steps_sampled: 190200
  num_agent_steps_trained: 173500
  num_env_steps_sampled: 190200
  num_env_steps_trained: 173500
  num_samples_added_to_queue: 190000
  num_training_step_calls_since_last_synch_worker_weights: 665
  num_weight_broadcasts: 3726
  timing_breakdown:
    learner_dequeue_time_ms: 0.008
    learner_grad_time_ms: 336.793
    learner_load_time_ms: 11.623
    learner_load_wait_time_ms: 2.736
iterations_since_restore: 27
node_ip: 127.0.0.1
num_agent_steps_sampled: 190200
num_agent_steps_trained: 173500
num_env_steps_sampled: 190200
num_env_steps_sampled_this_iter: 8450
num_env_steps_sampled_throughput_per_sec: 844.9943590540759
num_env_steps_trained: 173500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9943256756976
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 47.7
  ram_util_percent: 78.50714285714285
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11640468631545468
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.042929179317052396
  mean_inference_ms: 2.170052128358111
  mean_raw_obs_processing_ms: 0.48825371051239413
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03036355972290039
    StateBufferConnector_ms: 0.005377292633056641
    ViewRequirementAgentConnector_ms: 0.18309283256530762
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.25
  episode_reward_min: 0.0
  episodes_this_iter: 65
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 0.0, 7.0, 1.0, 2.0, 0.0, 3.0, 0.0, 4.0, 3.0, 3.0, 1.0,
      3.0, 4.0, 2.0, 4.0, 3.0, 1.0, 2.0, 2.0, 2.0, 3.0, 1.0, 3.0, 3.0, 2.0, 6.0, 3.0,
      0.0, 5.0, 1.0, 2.0, 1.0, 1.0, 3.0, 3.0, 2.0, 3.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0,
      0.0, 2.0, 3.0, 2.0, 3.0, 0.0, 2.0, 3.0, 4.0, 2.0, 4.0, 3.0, 1.0, 3.0, 3.0, 1.0,
      1.0, 0.0, 2.0, 1.0, 5.0, 1.0, 0.0, 1.0, 6.0, 1.0, 1.0, 1.0, 2.0, 1.0, 3.0, 2.0,
      4.0, 1.0, 4.0, 2.0, 6.0, 4.0, 3.0, 1.0, 2.0, 3.0, 2.0, 4.0, 5.0, 1.0, 3.0, 2.0,
      2.0, 1.0, 3.0, 1.0, 2.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11640468631545468
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.042929179317052396
    mean_inference_ms: 2.170052128358111
    mean_raw_obs_processing_ms: 0.48825371051239413
time_since_restore: 277.19642758369446
time_this_iter_s: 10.186532735824585
time_total_s: 277.19642758369446
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.074
timestamp: 1692344374
timesteps_total: 190200
training_iteration: 27
trial_id: default
train step: 28
agent_timesteps_total: 198250
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03128409385681152
  StateBufferConnector_ms: 0.005609035491943359
  ViewRequirementAgentConnector_ms: 0.1883537769317627
counters:
  num_agent_steps_sampled: 198250
  num_agent_steps_trained: 181500
  num_env_steps_sampled: 198250
  num_env_steps_trained: 181500
  num_samples_added_to_queue: 198000
  num_training_step_calls_since_last_synch_worker_weights: 412
  num_weight_broadcasts: 3884
custom_metrics: {}
date: 2023-08-18_16-39-45
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.1
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1550
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4917590618133545
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -37.05009460449219
        total_loss: -30.42057228088379
        var_gnorm: 63.314056396484375
        vf_explained_var: 0.06307411193847656
        vf_loss: 14.750805854797363
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 363.0
  learner_queue:
    size_count: 370
    size_mean: 14.86
    size_quantiles: [10.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6853486286225767
  num_agent_steps_sampled: 198250
  num_agent_steps_trained: 181500
  num_env_steps_sampled: 198250
  num_env_steps_trained: 181500
  num_samples_added_to_queue: 198000
  num_training_step_calls_since_last_synch_worker_weights: 412
  num_weight_broadcasts: 3884
  timing_breakdown:
    learner_dequeue_time_ms: 0.007
    learner_grad_time_ms: 248.379
    learner_load_time_ms: 11.429
    learner_load_wait_time_ms: 2.984
iterations_since_restore: 28
node_ip: 127.0.0.1
num_agent_steps_sampled: 198250
num_agent_steps_trained: 181500
num_env_steps_sampled: 198250
num_env_steps_sampled_this_iter: 8050
num_env_steps_sampled_throughput_per_sec: 804.998483779902
num_env_steps_trained: 181500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9984931974182
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 52.233333333333334
  ram_util_percent: 79.56666666666666
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11569529280414102
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04263403541432405
  mean_inference_ms: 2.1575396970170106
  mean_raw_obs_processing_ms: 0.4854967893007536
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03128409385681152
    StateBufferConnector_ms: 0.005609035491943359
    ViewRequirementAgentConnector_ms: 0.1883537769317627
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.1
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 5.0, 1.0, 0.0, 1.0, 6.0, 1.0, 1.0, 1.0, 2.0, 1.0, 3.0, 2.0,
      4.0, 1.0, 4.0, 2.0, 6.0, 4.0, 3.0, 1.0, 2.0, 3.0, 2.0, 4.0, 5.0, 1.0, 3.0, 2.0,
      2.0, 1.0, 3.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 4.0, 2.0, 1.0, 2.0, 2.0, 3.0, 3.0,
      4.0, 2.0, 0.0, 4.0, 0.0, 4.0, 2.0, 1.0, 2.0, 2.0, 4.0, 0.0, 1.0, 3.0, 1.0, 4.0,
      0.0, 1.0, 1.0, 3.0, 2.0, 4.0, 2.0, 1.0, 3.0, 3.0, 2.0, 0.0, 2.0, 3.0, 0.0, 2.0,
      4.0, 5.0, 4.0, 2.0, 2.0, 1.0, 1.0, 3.0, 1.0, 0.0, 1.0, 4.0, 2.0, 3.0, 1.0, 1.0,
      2.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11569529280414102
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04263403541432405
    mean_inference_ms: 2.1575396970170106
    mean_raw_obs_processing_ms: 0.4854967893007536
time_since_restore: 287.55459475517273
time_this_iter_s: 10.358167171478271
time_total_s: 287.55459475517273
timers:
  sample_time_ms: 0.059
  synch_weights_time_ms: 0.027
  training_iteration_time_ms: 0.174
timestamp: 1692344385
timesteps_total: 198250
training_iteration: 28
trial_id: default
train step: 29
agent_timesteps_total: 206500
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0313725471496582
  StateBufferConnector_ms: 0.0056040287017822266
  ViewRequirementAgentConnector_ms: 0.18820786476135254
counters:
  num_agent_steps_sampled: 206500
  num_agent_steps_trained: 190000
  num_env_steps_sampled: 206500
  num_env_steps_trained: 190000
  num_samples_added_to_queue: 206500
  num_training_step_calls_since_last_synch_worker_weights: 721
  num_weight_broadcasts: 4046
custom_metrics: {}
date: 2023-08-18_16-39-55
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.07
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1614
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.45792555809021
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -4.630301475524902
        total_loss: 4.548831939697266
        var_gnorm: 63.31422805786133
        vf_explained_var: 0.0959465503692627
        vf_loss: 19.816192626953125
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 380.0
  learner_queue:
    size_count: 384
    size_mean: 14.7
    size_quantiles: [10.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.7691806012954132
  num_agent_steps_sampled: 206500
  num_agent_steps_trained: 190000
  num_env_steps_sampled: 206500
  num_env_steps_trained: 190000
  num_samples_added_to_queue: 206500
  num_training_step_calls_since_last_synch_worker_weights: 721
  num_weight_broadcasts: 4046
  timing_breakdown:
    learner_dequeue_time_ms: 0.007
    learner_grad_time_ms: 403.592
    learner_load_time_ms: 4.19
    learner_load_wait_time_ms: 2.556
iterations_since_restore: 29
node_ip: 127.0.0.1
num_agent_steps_sampled: 206500
num_agent_steps_trained: 190000
num_env_steps_sampled: 206500
num_env_steps_sampled_this_iter: 8250
num_env_steps_sampled_throughput_per_sec: 824.9933714207132
num_env_steps_trained: 190000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9931705546742
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 49.957142857142856
  ram_util_percent: 79.45714285714287
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11503330733789051
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04235804254617401
  mean_inference_ms: 2.14585814195173
  mean_raw_obs_processing_ms: 0.4828826063655582
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0313725471496582
    StateBufferConnector_ms: 0.0056040287017822266
    ViewRequirementAgentConnector_ms: 0.18820786476135254
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.07
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 4.0, 2.0, 1.0, 3.0, 3.0, 2.0, 0.0, 2.0, 3.0, 0.0, 2.0,
      4.0, 5.0, 4.0, 2.0, 2.0, 1.0, 1.0, 3.0, 1.0, 0.0, 1.0, 4.0, 2.0, 3.0, 1.0, 1.0,
      2.0, 1.0, 0.0, 1.0, 1.0, 2.0, 3.0, 0.0, 1.0, 2.0, 2.0, 3.0, 2.0, 4.0, 3.0, 1.0,
      2.0, 1.0, 3.0, 1.0, 4.0, 0.0, 4.0, 1.0, 3.0, 0.0, 1.0, 2.0, 2.0, 3.0, 5.0, 1.0,
      5.0, 3.0, 4.0, 4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 0.0,
      1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 6.0, 1.0, 2.0, 1.0, 3.0, 1.0,
      3.0, 1.0, 3.0, 2.0, 4.0, 3.0, 5.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11503330733789051
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04235804254617401
    mean_inference_ms: 2.14585814195173
    mean_raw_obs_processing_ms: 0.4828826063655582
time_since_restore: 297.73569679260254
time_this_iter_s: 10.18110203742981
time_total_s: 297.73569679260254
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344395
timesteps_total: 206500
training_iteration: 29
trial_id: default
train step: 30
agent_timesteps_total: 214700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.030943632125854492
  StateBufferConnector_ms: 0.005645751953125
  ViewRequirementAgentConnector_ms: 0.18725323677062988
counters:
  num_agent_steps_sampled: 214700
  num_agent_steps_trained: 198000
  num_env_steps_sampled: 214700
  num_env_steps_trained: 198000
  num_samples_added_to_queue: 214500
  num_training_step_calls_since_last_synch_worker_weights: 343
  num_weight_broadcasts: 4207
custom_metrics: {}
date: 2023-08-18_16-40-05
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.93
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1678
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4417544603347778
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -22.99459457397461
        total_loss: -16.763383865356445
        var_gnorm: 63.31443786621094
        vf_explained_var: 0.1106271743774414
        vf_loss: 13.9041748046875
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 396.0
  learner_queue:
    size_count: 401
    size_mean: 14.84
    size_quantiles: [10.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6656530250925612
  num_agent_steps_sampled: 214700
  num_agent_steps_trained: 198000
  num_env_steps_sampled: 214700
  num_env_steps_trained: 198000
  num_samples_added_to_queue: 214500
  num_training_step_calls_since_last_synch_worker_weights: 343
  num_weight_broadcasts: 4207
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 372.635
    learner_load_time_ms: 4.054
    learner_load_wait_time_ms: 2.405
iterations_since_restore: 30
node_ip: 127.0.0.1
num_agent_steps_sampled: 214700
num_agent_steps_trained: 198000
num_env_steps_sampled: 214700
num_env_steps_sampled_this_iter: 8200
num_env_steps_sampled_throughput_per_sec: 819.9961486043626
num_env_steps_trained: 198000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9962425408415
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 46.835714285714296
  ram_util_percent: 78.90714285714287
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11435350874031001
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04207422109412395
  mean_inference_ms: 2.134411290263202
  mean_raw_obs_processing_ms: 0.4802825327643805
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.030943632125854492
    StateBufferConnector_ms: 0.005645751953125
    ViewRequirementAgentConnector_ms: 0.18725323677062988
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.93
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 0.0,
      1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 6.0, 1.0, 2.0, 1.0, 3.0, 1.0,
      3.0, 1.0, 3.0, 2.0, 4.0, 3.0, 5.0, 1.0, 2.0, 1.0, 2.0, 5.0, 0.0, 3.0, 1.0, 3.0,
      3.0, 1.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 3.0, 1.0, 3.0,
      1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 1.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0,
      1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 5.0, 4.0, 0.0, 3.0, 1.0, 1.0, 2.0, 6.0, 0.0, 2.0,
      4.0, 4.0, 0.0, 2.0, 1.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11435350874031001
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04207422109412395
    mean_inference_ms: 2.134411290263202
    mean_raw_obs_processing_ms: 0.4802825327643805
time_since_restore: 307.94454073905945
time_this_iter_s: 10.20884394645691
time_total_s: 307.94454073905945
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.076
timestamp: 1692344405
timesteps_total: 214700
training_iteration: 30
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000030'.
train step: 31
agent_timesteps_total: 222950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.031008005142211914
  StateBufferConnector_ms: 0.005632162094116211
  ViewRequirementAgentConnector_ms: 0.18858575820922852
counters:
  num_agent_steps_sampled: 222950
  num_agent_steps_trained: 206000
  num_env_steps_sampled: 222950
  num_env_steps_trained: 206000
  num_samples_added_to_queue: 222500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 4369
custom_metrics: {}
date: 2023-08-18_16-40-15
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 1.98
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1742
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4543614387512207
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 50.72764587402344
        total_loss: 59.83032989501953
        var_gnorm: 63.3145751953125
        vf_explained_var: 0.13927239179611206
        vf_loss: 19.659725189208984
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 412.0
  learner_queue:
    size_count: 417
    size_mean: 14.62
    size_quantiles: [10.0, 11.9, 16.0, 16.0, 16.0]
    size_std: 1.809861873182592
  num_agent_steps_sampled: 222950
  num_agent_steps_trained: 206000
  num_env_steps_sampled: 222950
  num_env_steps_trained: 206000
  num_samples_added_to_queue: 222500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 4369
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 364.643
    learner_load_time_ms: 3.713
    learner_load_wait_time_ms: 2.678
iterations_since_restore: 31
node_ip: 127.0.0.1
num_agent_steps_sampled: 222950
num_agent_steps_trained: 206000
num_env_steps_sampled: 222950
num_env_steps_sampled_this_iter: 8250
num_env_steps_sampled_throughput_per_sec: 824.950848766063
num_env_steps_trained: 206000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9523381973945
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 48.77999999999999
  ram_util_percent: 77.82666666666664
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11373232594274965
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.041807974048580575
  mean_inference_ms: 2.1234211738038
  mean_raw_obs_processing_ms: 0.4778247878954556
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.031008005142211914
    StateBufferConnector_ms: 0.005632162094116211
    ViewRequirementAgentConnector_ms: 0.18858575820922852
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 1.98
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 1.0, 1.0, 2.0, 3.0, 3.0, 2.0, 2.0,
      1.0, 1.0, 0.0, 1.0, 2.0, 2.0, 5.0, 4.0, 0.0, 3.0, 1.0, 1.0, 2.0, 6.0, 0.0, 2.0,
      4.0, 4.0, 0.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 0.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0,
      1.0, 2.0, 3.0, 2.0, 1.0, 6.0, 1.0, 3.0, 5.0, 0.0, 3.0, 2.0, 4.0, 3.0, 1.0, 3.0,
      1.0, 5.0, 1.0, 6.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 1.0, 2.0,
      1.0, 4.0, 3.0, 2.0, 2.0, 0.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 5.0, 1.0,
      2.0, 3.0, 1.0, 0.0, 1.0, 3.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11373232594274965
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.041807974048580575
    mean_inference_ms: 2.1234211738038
    mean_raw_obs_processing_ms: 0.4778247878954556
time_since_restore: 318.1173884868622
time_this_iter_s: 10.172847747802734
time_total_s: 318.1173884868622
timers:
  sample_time_ms: 0.053
  synch_weights_time_ms: 0.394
  training_iteration_time_ms: 0.552
timestamp: 1692344415
timesteps_total: 222950
training_iteration: 31
trial_id: default
train step: 32
agent_timesteps_total: 228550
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03162527084350586
  StateBufferConnector_ms: 0.006165266036987305
  ViewRequirementAgentConnector_ms: 0.19447898864746094
counters:
  num_agent_steps_sampled: 228550
  num_agent_steps_trained: 212000
  num_env_steps_sampled: 228550
  num_env_steps_trained: 212000
  num_samples_added_to_queue: 228500
  num_training_step_calls_since_last_synch_worker_weights: 235
  num_weight_broadcasts: 4478
custom_metrics: {}
date: 2023-08-18_16-40-28
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.04
episode_reward_min: 0.0
episodes_this_iter: 44
episodes_total: 1786
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4298431873321533
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -59.505455017089844
        total_loss: -57.0671272277832
        var_gnorm: 63.31467819213867
        vf_explained_var: 0.019608914852142334
        vf_loss: 6.306492805480957
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 424.0
  learner_queue:
    size_count: 430
    size_mean: 14.72
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6129476122924762
  num_agent_steps_sampled: 228550
  num_agent_steps_trained: 212000
  num_env_steps_sampled: 228550
  num_env_steps_trained: 212000
  num_samples_added_to_queue: 228500
  num_training_step_calls_since_last_synch_worker_weights: 235
  num_weight_broadcasts: 4478
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 789.624
    learner_load_time_ms: 3.713
    learner_load_wait_time_ms: 3.479
iterations_since_restore: 32
node_ip: 127.0.0.1
num_agent_steps_sampled: 228550
num_agent_steps_trained: 212000
num_env_steps_sampled: 228550
num_env_steps_sampled_this_iter: 5600
num_env_steps_sampled_throughput_per_sec: 457.05518263446953
num_env_steps_trained: 212000
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 489.7019813940745
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 54.02727272727273
  ram_util_percent: 78.74545454545455
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11352409898630812
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04163943628381218
  mean_inference_ms: 2.135028216879987
  mean_raw_obs_processing_ms: 0.47650778057815224
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03162527084350586
    StateBufferConnector_ms: 0.006165266036987305
    ViewRequirementAgentConnector_ms: 0.19447898864746094
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.04
  episode_reward_min: 0.0
  episodes_this_iter: 44
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 2.0, 3.0, 2.0, 1.0, 6.0, 1.0, 3.0, 5.0, 0.0, 3.0, 2.0,
      4.0, 3.0, 1.0, 3.0, 1.0, 5.0, 1.0, 6.0, 1.0, 1.0, 0.0, 2.0, 2.0, 2.0, 3.0, 3.0,
      1.0, 1.0, 1.0, 2.0, 1.0, 4.0, 3.0, 2.0, 2.0, 0.0, 3.0, 3.0, 1.0, 3.0, 1.0, 1.0,
      2.0, 1.0, 5.0, 1.0, 2.0, 3.0, 1.0, 0.0, 1.0, 3.0, 2.0, 2.0, 1.0, 0.0, 1.0, 1.0,
      0.0, 0.0, 2.0, 1.0, 3.0, 1.0, 1.0, 1.0, 6.0, 3.0, 4.0, 2.0, 1.0, 2.0, 6.0, 2.0,
      3.0, 5.0, 1.0, 2.0, 0.0, 0.0, 2.0, 3.0, 4.0, 2.0, 2.0, 3.0, 3.0, 2.0, 1.0, 1.0,
      4.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11352409898630812
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04163943628381218
    mean_inference_ms: 2.135028216879987
    mean_raw_obs_processing_ms: 0.47650778057815224
time_since_restore: 330.76487731933594
time_this_iter_s: 12.647488832473755
time_total_s: 330.76487731933594
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.013
  training_iteration_time_ms: 494.272
timestamp: 1692344428
timesteps_total: 228550
training_iteration: 32
trial_id: default
train step: 33
agent_timesteps_total: 235700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03371930122375488
  StateBufferConnector_ms: 0.0064809322357177734
  ViewRequirementAgentConnector_ms: 0.20677828788757324
counters:
  num_agent_steps_sampled: 235700
  num_agent_steps_trained: 219000
  num_env_steps_sampled: 235700
  num_env_steps_trained: 219000
  num_samples_added_to_queue: 235500
  num_training_step_calls_since_last_synch_worker_weights: 436
  num_weight_broadcasts: 4618
custom_metrics: {}
date: 2023-08-18_16-40-38
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.1
episode_reward_min: 0.0
episodes_this_iter: 56
episodes_total: 1842
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4411346912384033
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 29.45210838317871
        total_loss: 39.31345748901367
        var_gnorm: 63.31480026245117
        vf_explained_var: 0.12080365419387817
        vf_loss: 21.163829803466797
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 438.0
  learner_queue:
    size_count: 443
    size_mean: 14.54
    size_quantiles: [11.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.7112568480505783
  num_agent_steps_sampled: 235700
  num_agent_steps_trained: 219000
  num_env_steps_sampled: 235700
  num_env_steps_trained: 219000
  num_samples_added_to_queue: 235500
  num_training_step_calls_since_last_synch_worker_weights: 436
  num_weight_broadcasts: 4618
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 470.332
    learner_load_time_ms: 3.27
    learner_load_wait_time_ms: 6.346
iterations_since_restore: 33
node_ip: 127.0.0.1
num_agent_steps_sampled: 235700
num_agent_steps_trained: 219000
num_env_steps_sampled: 235700
num_env_steps_sampled_this_iter: 7150
num_env_steps_sampled_throughput_per_sec: 714.9937779252781
num_env_steps_trained: 219000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9939084583142
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 63.53333333333334
  ram_util_percent: 79.7
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11348442400877898
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04162533175321779
  mean_inference_ms: 2.1566834068332783
  mean_raw_obs_processing_ms: 0.4760701036275248
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03371930122375488
    StateBufferConnector_ms: 0.0064809322357177734
    ViewRequirementAgentConnector_ms: 0.20677828788757324
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.1
  episode_reward_min: 0.0
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 2.0, 1.0, 3.0, 1.0, 1.0, 1.0,
      6.0, 3.0, 4.0, 2.0, 1.0, 2.0, 6.0, 2.0, 3.0, 5.0, 1.0, 2.0, 0.0, 0.0, 2.0, 3.0,
      4.0, 2.0, 2.0, 3.0, 3.0, 2.0, 1.0, 1.0, 4.0, 2.0, 2.0, 0.0, 1.0, 1.0, 2.0, 3.0,
      2.0, 3.0, 4.0, 3.0, 2.0, 1.0, 2.0, 4.0, 0.0, 2.0, 4.0, 5.0, 1.0, 3.0, 3.0, 0.0,
      1.0, 1.0, 1.0, 0.0, 1.0, 3.0, 3.0, 1.0, 1.0, 4.0, 3.0, 4.0, 4.0, 3.0, 5.0, 1.0,
      4.0, 1.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 0.0, 0.0, 1.0, 4.0, 5.0, 3.0, 2.0, 2.0,
      3.0, 0.0, 1.0, 1.0, 5.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11348442400877898
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04162533175321779
    mean_inference_ms: 2.1566834068332783
    mean_raw_obs_processing_ms: 0.4760701036275248
time_since_restore: 341.0087203979492
time_this_iter_s: 10.243843078613281
time_total_s: 341.0087203979492
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.089
timestamp: 1692344438
timesteps_total: 235700
training_iteration: 33
trial_id: default
train step: 34
agent_timesteps_total: 244000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03199005126953125
  StateBufferConnector_ms: 0.005736351013183594
  ViewRequirementAgentConnector_ms: 0.19477057456970215
counters:
  num_agent_steps_sampled: 244000
  num_agent_steps_trained: 227500
  num_env_steps_sampled: 244000
  num_env_steps_trained: 227500
  num_samples_added_to_queue: 244000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 4781
custom_metrics: {}
date: 2023-08-18_16-40-48
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.01
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 1906
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4314242601394653
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 19.80635643005371
        total_loss: 30.863157272338867
        var_gnorm: 63.314918518066406
        vf_explained_var: 0.06248706579208374
        vf_loss: 23.54502296447754
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 455.0
  learner_queue:
    size_count: 457
    size_mean: 14.88
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.544538766104626
  num_agent_steps_sampled: 244000
  num_agent_steps_trained: 227500
  num_env_steps_sampled: 244000
  num_env_steps_trained: 227500
  num_samples_added_to_queue: 244000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 4781
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 496.394
    learner_load_time_ms: 3.234
    learner_load_wait_time_ms: 2.59
iterations_since_restore: 34
node_ip: 127.0.0.1
num_agent_steps_sampled: 244000
num_agent_steps_trained: 227500
num_env_steps_sampled: 244000
num_env_steps_sampled_this_iter: 8300
num_env_steps_sampled_throughput_per_sec: 827.2826897069726
num_env_steps_trained: 227500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 847.217212350514
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 49.15714285714286
  ram_util_percent: 79.27142857142856
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11306662147724067
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04154586980247465
  mean_inference_ms: 2.151968290087562
  mean_raw_obs_processing_ms: 0.4748892716469419
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03199005126953125
    StateBufferConnector_ms: 0.005736351013183594
    ViewRequirementAgentConnector_ms: 0.19477057456970215
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.01
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 1.0, 3.0, 3.0, 1.0, 1.0, 4.0, 3.0, 4.0, 4.0, 3.0, 5.0, 1.0,
      4.0, 1.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 0.0, 0.0, 1.0, 4.0, 5.0, 3.0, 2.0, 2.0,
      3.0, 0.0, 1.0, 1.0, 5.0, 1.0, 1.0, 3.0, 0.0, 1.0, 3.0, 0.0, 3.0, 2.0, 2.0, 1.0,
      0.0, 2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 6.0, 2.0, 2.0, 2.0, 2.0, 0.0, 3.0, 3.0, 0.0,
      3.0, 1.0, 3.0, 0.0, 1.0, 2.0, 2.0, 3.0, 5.0, 5.0, 1.0, 1.0, 3.0, 3.0, 1.0, 4.0,
      2.0, 1.0, 0.0, 2.0, 4.0, 3.0, 0.0, 2.0, 3.0, 2.0, 0.0, 2.0, 3.0, 1.0, 1.0, 1.0,
      2.0, 1.0, 2.0, 2.0, 2.0, 0.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11306662147724067
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04154586980247465
    mean_inference_ms: 2.151968290087562
    mean_raw_obs_processing_ms: 0.4748892716469419
time_since_restore: 351.13873744010925
time_this_iter_s: 10.130017042160034
time_total_s: 351.13873744010925
timers:
  sample_time_ms: 0.052
  synch_weights_time_ms: 0.624
  training_iteration_time_ms: 3.37
timestamp: 1692344448
timesteps_total: 244000
training_iteration: 34
trial_id: default
train step: 35
agent_timesteps_total: 252400
connector_metrics:
  ObsPreprocessorConnector_ms: 0.030141830444335938
  StateBufferConnector_ms: 0.005331516265869141
  ViewRequirementAgentConnector_ms: 0.18176817893981934
counters:
  num_agent_steps_sampled: 252400
  num_agent_steps_trained: 235500
  num_env_steps_sampled: 252400
  num_env_steps_trained: 235500
  num_samples_added_to_queue: 252000
  num_training_step_calls_since_last_synch_worker_weights: 865
  num_weight_broadcasts: 4947
custom_metrics: {}
date: 2023-08-18_16-40-58
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.2
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 1973
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.436885952949524
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 34.3367805480957
        total_loss: 44.18120574951172
        var_gnorm: 63.31501007080078
        vf_explained_var: 0.08182591199874878
        vf_loss: 21.125736236572266
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 471.0
  learner_queue:
    size_count: 476
    size_mean: 14.96
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.535708305636197
  num_agent_steps_sampled: 252400
  num_agent_steps_trained: 235500
  num_env_steps_sampled: 252400
  num_env_steps_trained: 235500
  num_samples_added_to_queue: 252000
  num_training_step_calls_since_last_synch_worker_weights: 865
  num_weight_broadcasts: 4947
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 360.004
    learner_load_time_ms: 3.219
    learner_load_wait_time_ms: 2.468
iterations_since_restore: 35
node_ip: 127.0.0.1
num_agent_steps_sampled: 252400
num_agent_steps_trained: 235500
num_env_steps_sampled: 252400
num_env_steps_sampled_this_iter: 8400
num_env_steps_sampled_throughput_per_sec: 839.9973764501497
num_env_steps_trained: 235500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9975013810949
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 48.32857142857143
  ram_util_percent: 79.24999999999999
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11245944087879277
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.041316658658961954
  mean_inference_ms: 2.1413215053922254
  mean_raw_obs_processing_ms: 0.47269294883014595
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.030141830444335938
    StateBufferConnector_ms: 0.005331516265869141
    ViewRequirementAgentConnector_ms: 0.18176817893981934
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.2
  episode_reward_min: 0.0
  episodes_this_iter: 67
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 3.0, 5.0, 5.0, 1.0, 1.0, 3.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0,
      2.0, 4.0, 3.0, 0.0, 2.0, 3.0, 2.0, 0.0, 2.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 2.0,
      2.0, 2.0, 0.0, 2.0, 1.0, 0.0, 3.0, 2.0, 2.0, 1.0, 0.0, 1.0, 3.0, 1.0, 3.0, 1.0,
      0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 3.0, 2.0, 1.0, 2.0, 4.0, 3.0, 1.0, 4.0, 2.0,
      1.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 3.0, 3.0, 1.0, 3.0, 4.0, 2.0, 2.0, 2.0,
      3.0, 0.0, 4.0, 3.0, 4.0, 2.0, 2.0, 3.0, 3.0, 1.0, 2.0, 4.0, 5.0, 2.0, 3.0, 4.0,
      3.0, 2.0, 1.0, 2.0, 6.0, 4.0, 6.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11245944087879277
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.041316658658961954
    mean_inference_ms: 2.1413215053922254
    mean_raw_obs_processing_ms: 0.47269294883014595
time_since_restore: 361.31382155418396
time_this_iter_s: 10.175084114074707
time_total_s: 361.31382155418396
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.073
timestamp: 1692344458
timesteps_total: 252400
training_iteration: 35
trial_id: default
train step: 36
agent_timesteps_total: 260950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.02961587905883789
  StateBufferConnector_ms: 0.005225419998168945
  ViewRequirementAgentConnector_ms: 0.17942357063293457
counters:
  num_agent_steps_sampled: 260950
  num_agent_steps_trained: 244000
  num_env_steps_sampled: 260950
  num_env_steps_trained: 244000
  num_samples_added_to_queue: 260500
  num_training_step_calls_since_last_synch_worker_weights: 981
  num_weight_broadcasts: 5115
custom_metrics: {}
date: 2023-08-18_16-41-09
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.51
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 2039
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3941198587417603
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -10.950286865234375
        total_loss: -5.377786636352539
        var_gnorm: 63.31515884399414
        vf_explained_var: 0.15299886465072632
        vf_loss: 12.5391206741333
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 488.0
  learner_queue:
    size_count: 492
    size_mean: 15.38
    size_quantiles: [12.0, 13.9, 16.0, 16.0, 16.0]
    size_std: 1.1643023662262308
  num_agent_steps_sampled: 260950
  num_agent_steps_trained: 244000
  num_env_steps_sampled: 260950
  num_env_steps_trained: 244000
  num_samples_added_to_queue: 260500
  num_training_step_calls_since_last_synch_worker_weights: 981
  num_weight_broadcasts: 5115
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 422.542
    learner_load_time_ms: 2.764
    learner_load_wait_time_ms: 2.46
iterations_since_restore: 36
node_ip: 127.0.0.1
num_agent_steps_sampled: 260950
num_agent_steps_trained: 244000
num_env_steps_sampled: 260950
num_env_steps_sampled_this_iter: 8550
num_env_steps_sampled_throughput_per_sec: 854.9960657539203
num_env_steps_trained: 244000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9960887612074
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 47.05714285714286
  ram_util_percent: 79.26428571428572
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11186732780467068
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.041064389245844116
  mean_inference_ms: 2.1296272253923196
  mean_raw_obs_processing_ms: 0.4704461235535382
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.02961587905883789
    StateBufferConnector_ms: 0.005225419998168945
    ViewRequirementAgentConnector_ms: 0.17942357063293457
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.51
  episode_reward_min: 0.0
  episodes_this_iter: 66
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 2.0, 3.0, 3.0, 1.0, 3.0, 4.0, 2.0, 2.0, 2.0, 3.0, 0.0,
      4.0, 3.0, 4.0, 2.0, 2.0, 3.0, 3.0, 1.0, 2.0, 4.0, 5.0, 2.0, 3.0, 4.0, 3.0, 2.0,
      1.0, 2.0, 6.0, 4.0, 6.0, 5.0, 4.0, 2.0, 3.0, 3.0, 3.0, 2.0, 5.0, 3.0, 2.0, 1.0,
      4.0, 1.0, 2.0, 3.0, 3.0, 2.0, 3.0, 1.0, 2.0, 4.0, 2.0, 1.0, 2.0, 2.0, 2.0, 4.0,
      1.0, 3.0, 4.0, 2.0, 3.0, 3.0, 1.0, 4.0, 1.0, 2.0, 2.0, 3.0, 0.0, 1.0, 1.0, 3.0,
      3.0, 1.0, 2.0, 1.0, 4.0, 1.0, 3.0, 4.0, 3.0, 3.0, 1.0, 3.0, 3.0, 4.0, 2.0, 2.0,
      2.0, 1.0, 1.0, 1.0, 0.0, 4.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11186732780467068
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.041064389245844116
    mean_inference_ms: 2.1296272253923196
    mean_raw_obs_processing_ms: 0.4704461235535382
time_since_restore: 371.4667315483093
time_this_iter_s: 10.152909994125366
time_total_s: 371.4667315483093
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.074
timestamp: 1692344469
timesteps_total: 260950
training_iteration: 36
trial_id: default
train step: 37
agent_timesteps_total: 269550
connector_metrics:
  ObsPreprocessorConnector_ms: 0.02970600128173828
  StateBufferConnector_ms: 0.005271434783935547
  ViewRequirementAgentConnector_ms: 0.17888665199279785
counters:
  num_agent_steps_sampled: 269550
  num_agent_steps_trained: 253000
  num_env_steps_sampled: 269550
  num_env_steps_trained: 253000
  num_samples_added_to_queue: 269500
  num_training_step_calls_since_last_synch_worker_weights: 80
  num_weight_broadcasts: 5284
custom_metrics: {}
date: 2023-08-18_16-41-19
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.18
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 2106
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.40305495262146
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 83.63253784179688
        total_loss: 101.30541229248047
        var_gnorm: 63.31535720825195
        vf_explained_var: 0.15558916330337524
        vf_loss: 36.748809814453125
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 506.0
  learner_queue:
    size_count: 512
    size_mean: 15.28
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.2812493902437574
  num_agent_steps_sampled: 269550
  num_agent_steps_trained: 253000
  num_env_steps_sampled: 269550
  num_env_steps_trained: 253000
  num_samples_added_to_queue: 269500
  num_training_step_calls_since_last_synch_worker_weights: 80
  num_weight_broadcasts: 5284
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 268.196
    learner_load_time_ms: 2.75
    learner_load_wait_time_ms: 2.572
iterations_since_restore: 37
node_ip: 127.0.0.1
num_agent_steps_sampled: 269550
num_agent_steps_trained: 253000
num_env_steps_sampled: 269550
num_env_steps_sampled_this_iter: 8600
num_env_steps_sampled_throughput_per_sec: 859.9948330235977
num_env_steps_trained: 253000
num_env_steps_trained_this_iter: 9000
num_env_steps_trained_throughput_per_sec: 899.9945926991138
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 9000
perf:
  cpu_util_percent: 45.42
  ram_util_percent: 79.22666666666669
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11127637244240832
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04080986799004077
  mean_inference_ms: 2.117746385223177
  mean_raw_obs_processing_ms: 0.4682097923536498
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.02970600128173828
    StateBufferConnector_ms: 0.005271434783935547
    ViewRequirementAgentConnector_ms: 0.17888665199279785
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.18
  episode_reward_min: 0.0
  episodes_this_iter: 67
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 4.0, 1.0, 2.0, 2.0, 3.0, 0.0, 1.0, 1.0, 3.0, 3.0, 1.0, 2.0,
      1.0, 4.0, 1.0, 3.0, 4.0, 3.0, 3.0, 1.0, 3.0, 3.0, 4.0, 2.0, 2.0, 2.0, 1.0, 1.0,
      1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 6.0, 1.0, 1.0, 3.0, 1.0, 2.0, 1.0, 2.0, 3.0, 3.0,
      2.0, 4.0, 3.0, 3.0, 4.0, 2.0, 0.0, 4.0, 3.0, 1.0, 3.0, 4.0, 1.0, 3.0, 2.0, 1.0,
      1.0, 2.0, 6.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 3.0,
      1.0, 3.0, 1.0, 2.0, 4.0, 3.0, 6.0, 2.0, 3.0, 3.0, 0.0, 2.0, 1.0, 4.0, 2.0, 0.0,
      0.0, 3.0, 2.0, 2.0, 3.0, 2.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11127637244240832
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04080986799004077
    mean_inference_ms: 2.117746385223177
    mean_raw_obs_processing_ms: 0.4682097923536498
time_since_restore: 381.6978986263275
time_this_iter_s: 10.231167078018188
time_total_s: 381.6978986263275
timers:
  sample_time_ms: 0.034
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.093
timestamp: 1692344479
timesteps_total: 269550
training_iteration: 37
trial_id: default
train step: 38
agent_timesteps_total: 278100
connector_metrics:
  ObsPreprocessorConnector_ms: 0.029564619064331055
  StateBufferConnector_ms: 0.005339384078979492
  ViewRequirementAgentConnector_ms: 0.17953896522521973
counters:
  num_agent_steps_sampled: 278100
  num_agent_steps_trained: 261500
  num_env_steps_sampled: 278100
  num_env_steps_trained: 261500
  num_samples_added_to_queue: 278000
  num_training_step_calls_since_last_synch_worker_weights: 932
  num_weight_broadcasts: 5452
custom_metrics: {}
date: 2023-08-18_16-41-29
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.13
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 2173
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.19999999999999
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4045934677124023
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -76.9707260131836
        total_loss: -72.11976623535156
        var_gnorm: 63.31559753417969
        vf_explained_var: 0.02429676055908203
        vf_loss: 11.106512069702148
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 523.0
  learner_queue:
    size_count: 528
    size_mean: 15.12
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.4372195378577344
  num_agent_steps_sampled: 278100
  num_agent_steps_trained: 261500
  num_env_steps_sampled: 278100
  num_env_steps_trained: 261500
  num_samples_added_to_queue: 278000
  num_training_step_calls_since_last_synch_worker_weights: 932
  num_weight_broadcasts: 5452
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 326.678
    learner_load_time_ms: 2.75
    learner_load_wait_time_ms: 2.687
iterations_since_restore: 38
node_ip: 127.0.0.1
num_agent_steps_sampled: 278100
num_agent_steps_trained: 261500
num_env_steps_sampled: 278100
num_env_steps_sampled_this_iter: 8550
num_env_steps_sampled_throughput_per_sec: 854.9974519090298
num_env_steps_trained: 261500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9974668101465
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 46.12857142857143
  ram_util_percent: 79.28571428571426
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11073989244569446
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04058189851962934
  mean_inference_ms: 2.1070372031619344
  mean_raw_obs_processing_ms: 0.466136878033503
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.029564619064331055
    StateBufferConnector_ms: 0.005339384078979492
    ViewRequirementAgentConnector_ms: 0.17953896522521973
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.13
  episode_reward_min: 0.0
  episodes_this_iter: 67
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 3.0, 1.0, 3.0, 1.0,
      2.0, 4.0, 3.0, 6.0, 2.0, 3.0, 3.0, 0.0, 2.0, 1.0, 4.0, 2.0, 0.0, 0.0, 3.0, 2.0,
      2.0, 3.0, 2.0, 4.0, 1.0, 4.0, 5.0, 1.0, 0.0, 1.0, 0.0, 2.0, 2.0, 3.0, 0.0, 2.0,
      3.0, 4.0, 3.0, 5.0, 3.0, 1.0, 2.0, 0.0, 3.0, 2.0, 1.0, 3.0, 0.0, 2.0, 4.0, 1.0,
      2.0, 0.0, 4.0, 1.0, 1.0, 3.0, 1.0, 3.0, 2.0, 1.0, 3.0, 0.0, 1.0, 2.0, 1.0, 4.0,
      5.0, 2.0, 2.0, 1.0, 5.0, 4.0, 2.0, 0.0, 3.0, 1.0, 3.0, 2.0, 0.0, 1.0, 1.0, 1.0,
      3.0, 6.0, 3.0, 5.0, 3.0, 1.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11073989244569446
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04058189851962934
    mean_inference_ms: 2.1070372031619344
    mean_raw_obs_processing_ms: 0.466136878033503
time_since_restore: 391.89652156829834
time_this_iter_s: 10.198622941970825
time_total_s: 391.89652156829834
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344489
timesteps_total: 278100
training_iteration: 38
trial_id: default
train step: 39
agent_timesteps_total: 286750
connector_metrics:
  ObsPreprocessorConnector_ms: 0.029985904693603516
  StateBufferConnector_ms: 0.0053615570068359375
  ViewRequirementAgentConnector_ms: 0.17962336540222168
counters:
  num_agent_steps_sampled: 286750
  num_agent_steps_trained: 270000
  num_env_steps_sampled: 286750
  num_env_steps_trained: 270000
  num_samples_added_to_queue: 286500
  num_training_step_calls_since_last_synch_worker_weights: 786
  num_weight_broadcasts: 5622
custom_metrics: {}
date: 2023-08-18_16-41-39
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.24
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 2241
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.30000000000001
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4260014295578003
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -14.599020004272461
        total_loss: -6.911477565765381
        var_gnorm: 63.31570053100586
        vf_explained_var: 0.12652915716171265
        vf_loss: 16.80108642578125
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 540.0
  learner_queue:
    size_count: 545
    size_mean: 15.04
    size_quantiles: [11.0, 12.9, 16.0, 16.0, 16.0]
    size_std: 1.4961283367412035
  num_agent_steps_sampled: 286750
  num_agent_steps_trained: 270000
  num_env_steps_sampled: 286750
  num_env_steps_trained: 270000
  num_samples_added_to_queue: 286500
  num_training_step_calls_since_last_synch_worker_weights: 786
  num_weight_broadcasts: 5622
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 339.113
    learner_load_time_ms: 3.103
    learner_load_wait_time_ms: 2.497
iterations_since_restore: 39
node_ip: 127.0.0.1
num_agent_steps_sampled: 286750
num_agent_steps_trained: 270000
num_env_steps_sampled: 286750
num_env_steps_sampled_this_iter: 8650
num_env_steps_sampled_throughput_per_sec: 864.9946586223464
num_env_steps_trained: 270000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9947512473923
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 42.50666666666666
  ram_util_percent: 79.27999999999999
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.11021659024641492
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.040355728934496495
  mean_inference_ms: 2.0963977668506004
  mean_raw_obs_processing_ms: 0.46405842147499127
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.029985904693603516
    StateBufferConnector_ms: 0.0053615570068359375
    ViewRequirementAgentConnector_ms: 0.17962336540222168
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.24
  episode_reward_min: 0.0
  episodes_this_iter: 68
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 1.0, 3.0, 0.0, 1.0, 2.0, 1.0, 4.0, 5.0, 2.0, 2.0, 1.0,
      5.0, 4.0, 2.0, 0.0, 3.0, 1.0, 3.0, 2.0, 0.0, 1.0, 1.0, 1.0, 3.0, 6.0, 3.0, 5.0,
      3.0, 1.0, 1.0, 2.0, 2.0, 3.0, 1.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 4.0,
      3.0, 2.0, 1.0, 0.0, 4.0, 1.0, 3.0, 2.0, 0.0, 3.0, 0.0, 3.0, 0.0, 5.0, 5.0, 1.0,
      3.0, 3.0, 2.0, 1.0, 3.0, 3.0, 1.0, 2.0, 7.0, 2.0, 2.0, 5.0, 1.0, 0.0, 4.0, 2.0,
      2.0, 1.0, 4.0, 4.0, 3.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0,
      1.0, 0.0, 1.0, 4.0, 2.0, 5.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.11021659024641492
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.040355728934496495
    mean_inference_ms: 2.0963977668506004
    mean_raw_obs_processing_ms: 0.46405842147499127
time_since_restore: 402.07380270957947
time_this_iter_s: 10.177281141281128
time_total_s: 402.07380270957947
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.074
timestamp: 1692344499
timesteps_total: 286750
training_iteration: 39
trial_id: default
train step: 40
agent_timesteps_total: 295400
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0296022891998291
  StateBufferConnector_ms: 0.0052433013916015625
  ViewRequirementAgentConnector_ms: 0.17906594276428223
counters:
  num_agent_steps_sampled: 295400
  num_agent_steps_trained: 278500
  num_env_steps_sampled: 295400
  num_env_steps_trained: 278500
  num_samples_added_to_queue: 295000
  num_training_step_calls_since_last_synch_worker_weights: 417
  num_weight_broadcasts: 5792
custom_metrics: {}
date: 2023-08-18_16-41-50
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.37
episode_reward_min: 0.0
episodes_this_iter: 68
episodes_total: 2309
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4105786085128784
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 0.48949307203292847
        total_loss: 9.375703811645508
        var_gnorm: 63.31582260131836
        vf_explained_var: 0.13104534149169922
        vf_loss: 19.183000564575195
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 557.0
  learner_queue:
    size_count: 563
    size_mean: 15.06
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.405844941663198
  num_agent_steps_sampled: 295400
  num_agent_steps_trained: 278500
  num_env_steps_sampled: 295400
  num_env_steps_trained: 278500
  num_samples_added_to_queue: 295000
  num_training_step_calls_since_last_synch_worker_weights: 417
  num_weight_broadcasts: 5792
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 299.779
    learner_load_time_ms: 3.099
    learner_load_wait_time_ms: 2.501
iterations_since_restore: 40
node_ip: 127.0.0.1
num_agent_steps_sampled: 295400
num_agent_steps_trained: 278500
num_env_steps_sampled: 295400
num_env_steps_sampled_this_iter: 8650
num_env_steps_sampled_throughput_per_sec: 864.9983501465795
num_env_steps_trained: 278500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9983787567544
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 45.66428571428572
  ram_util_percent: 79.23571428571428
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10970996227077028
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04013784123703584
  mean_inference_ms: 2.086331896309346
  mean_raw_obs_processing_ms: 0.4620935715514019
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0296022891998291
    StateBufferConnector_ms: 0.0052433013916015625
    ViewRequirementAgentConnector_ms: 0.17906594276428223
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.37
  episode_reward_min: 0.0
  episodes_this_iter: 68
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 7.0, 2.0, 2.0, 5.0, 1.0, 0.0, 4.0, 2.0, 2.0, 1.0, 4.0, 4.0,
      3.0, 1.0, 2.0, 2.0, 3.0, 3.0, 2.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0, 0.0, 1.0, 4.0,
      2.0, 5.0, 3.0, 3.0, 1.0, 3.0, 0.0, 4.0, 3.0, 2.0, 4.0, 2.0, 4.0, 2.0, 0.0, 3.0,
      1.0, 3.0, 2.0, 3.0, 2.0, 3.0, 2.0, 2.0, 2.0, 5.0, 4.0, 3.0, 3.0, 2.0, 1.0, 3.0,
      5.0, 6.0, 2.0, 2.0, 3.0, 4.0, 1.0, 5.0, 5.0, 2.0, 2.0, 0.0, 1.0, 6.0, 0.0, 1.0,
      2.0, 0.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 4.0, 0.0, 4.0, 2.0, 0.0, 2.0, 2.0, 1.0,
      3.0, 0.0, 2.0, 2.0, 2.0, 4.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10970996227077028
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04013784123703584
    mean_inference_ms: 2.086331896309346
    mean_raw_obs_processing_ms: 0.4620935715514019
time_since_restore: 412.28926968574524
time_this_iter_s: 10.215466976165771
time_total_s: 412.28926968574524
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.075
timestamp: 1692344510
timesteps_total: 295400
training_iteration: 40
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000040'.
train step: 41
agent_timesteps_total: 303350
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03139328956604004
  StateBufferConnector_ms: 0.005570888519287109
  ViewRequirementAgentConnector_ms: 0.1905994415283203
counters:
  num_agent_steps_sampled: 303350
  num_agent_steps_trained: 286500
  num_env_steps_sampled: 303350
  num_env_steps_trained: 286500
  num_samples_added_to_queue: 303000
  num_training_step_calls_since_last_synch_worker_weights: 62
  num_weight_broadcasts: 5948
custom_metrics: {}
date: 2023-08-18_16-42-00
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.28
episode_reward_min: 0.0
episodes_this_iter: 61
episodes_total: 2370
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3692231178283691
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 22.452701568603516
        total_loss: 33.39446258544922
        var_gnorm: 63.31599426269531
        vf_explained_var: 0.13952851295471191
        vf_loss: 23.252744674682617
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 573.0
  learner_queue:
    size_count: 579
    size_mean: 14.96
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.535708305636197
  num_agent_steps_sampled: 303350
  num_agent_steps_trained: 286500
  num_env_steps_sampled: 303350
  num_env_steps_trained: 286500
  num_samples_added_to_queue: 303000
  num_training_step_calls_since_last_synch_worker_weights: 62
  num_weight_broadcasts: 5948
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 320.062
    learner_load_time_ms: 3.089
    learner_load_wait_time_ms: 2.587
iterations_since_restore: 41
node_ip: 127.0.0.1
num_agent_steps_sampled: 303350
num_agent_steps_trained: 286500
num_env_steps_sampled: 303350
num_env_steps_sampled_this_iter: 7950
num_env_steps_sampled_throughput_per_sec: 794.9979339891719
num_env_steps_trained: 286500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9979209953931
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 45.25999999999999
  ram_util_percent: 79.49333333333333
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10943767885232504
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.04000821421152559
  mean_inference_ms: 2.08001111600592
  mean_raw_obs_processing_ms: 0.4610146340787922
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03139328956604004
    StateBufferConnector_ms: 0.005570888519287109
    ViewRequirementAgentConnector_ms: 0.1905994415283203
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.28
  episode_reward_min: 0.0
  episodes_this_iter: 61
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [5.0, 6.0, 2.0, 2.0, 3.0, 4.0, 1.0, 5.0, 5.0, 2.0, 2.0, 0.0, 1.0,
      6.0, 0.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 4.0, 0.0, 4.0, 2.0, 0.0,
      2.0, 2.0, 1.0, 3.0, 0.0, 2.0, 2.0, 2.0, 4.0, 2.0, 5.0, 2.0, 2.0, 2.0, 3.0, 1.0,
      2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 4.0, 2.0, 3.0, 2.0,
      4.0, 3.0, 3.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0, 2.0, 1.0, 3.0, 6.0, 2.0, 4.0, 3.0,
      2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 1.0, 1.0, 4.0, 1.0, 2.0, 3.0, 2.0, 1.0, 2.0, 1.0,
      1.0, 4.0, 1.0, 5.0, 2.0, 5.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10943767885232504
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.04000821421152559
    mean_inference_ms: 2.08001111600592
    mean_raw_obs_processing_ms: 0.4610146340787922
time_since_restore: 422.52835273742676
time_this_iter_s: 10.239083051681519
time_total_s: 422.52835273742676
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.073
timestamp: 1692344520
timesteps_total: 303350
training_iteration: 41
trial_id: default
train step: 42
agent_timesteps_total: 311000
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03349804878234863
  StateBufferConnector_ms: 0.005940437316894531
  ViewRequirementAgentConnector_ms: 0.19932818412780762
counters:
  num_agent_steps_sampled: 311000
  num_agent_steps_trained: 294500
  num_env_steps_sampled: 311000
  num_env_steps_trained: 294500
  num_samples_added_to_queue: 311000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 6098
custom_metrics: {}
date: 2023-08-18_16-42-10
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.29
episode_reward_min: 0.0
episodes_this_iter: 60
episodes_total: 2430
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.372123122215271
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 7.6248884201049805
        total_loss: 15.368473052978516
        var_gnorm: 63.316192626953125
        vf_explained_var: 0.1691582202911377
        vf_loss: 16.859291076660156
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 589.0
  learner_queue:
    size_count: 594
    size_mean: 14.72
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6375591592366976
  num_agent_steps_sampled: 311000
  num_agent_steps_trained: 294500
  num_env_steps_sampled: 311000
  num_env_steps_trained: 294500
  num_samples_added_to_queue: 311000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 6098
  timing_breakdown:
    learner_dequeue_time_ms: 0.006
    learner_grad_time_ms: 318.942
    learner_load_time_ms: 2.7
    learner_load_wait_time_ms: 2.6
iterations_since_restore: 42
node_ip: 127.0.0.1
num_agent_steps_sampled: 311000
num_agent_steps_trained: 294500
num_env_steps_sampled: 311000
num_env_steps_sampled_this_iter: 7650
num_env_steps_sampled_throughput_per_sec: 764.0480694778751
num_env_steps_trained: 294500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.0045171010458
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 54.56428571428571
  ram_util_percent: 79.79999999999998
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10933292725794892
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03993544111255074
  mean_inference_ms: 2.0759523201654666
  mean_raw_obs_processing_ms: 0.46051359580764784
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03349804878234863
    StateBufferConnector_ms: 0.005940437316894531
    ViewRequirementAgentConnector_ms: 0.19932818412780762
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.29
  episode_reward_min: 0.0
  episodes_this_iter: 60
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 4.0, 3.0, 3.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0, 2.0, 1.0, 3.0,
      6.0, 2.0, 4.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 1.0, 1.0, 4.0, 1.0, 2.0, 3.0,
      2.0, 1.0, 2.0, 1.0, 1.0, 4.0, 1.0, 5.0, 2.0, 5.0, 2.0, 4.0, 2.0, 3.0, 6.0, 0.0,
      2.0, 2.0, 3.0, 4.0, 3.0, 3.0, 1.0, 0.0, 3.0, 1.0, 1.0, 3.0, 4.0, 1.0, 3.0, 1.0,
      0.0, 1.0, 3.0, 3.0, 4.0, 1.0, 7.0, 2.0, 2.0, 0.0, 4.0, 3.0, 3.0, 1.0, 3.0, 1.0,
      3.0, 1.0, 3.0, 2.0, 5.0, 2.0, 1.0, 0.0, 4.0, 3.0, 2.0, 3.0, 2.0, 1.0, 0.0, 0.0,
      1.0, 2.0, 1.0, 3.0, 3.0, 0.0, 7.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10933292725794892
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03993544111255074
    mean_inference_ms: 2.0759523201654666
    mean_raw_obs_processing_ms: 0.46051359580764784
time_since_restore: 432.7371506690979
time_this_iter_s: 10.208797931671143
time_total_s: 432.7371506690979
timers:
  sample_time_ms: 0.057
  synch_weights_time_ms: 1.089
  training_iteration_time_ms: 3.737
timestamp: 1692344530
timesteps_total: 311000
training_iteration: 42
trial_id: default
train step: 43
agent_timesteps_total: 319400
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03095269203186035
  StateBufferConnector_ms: 0.005541324615478516
  ViewRequirementAgentConnector_ms: 0.1857445240020752
counters:
  num_agent_steps_sampled: 319400
  num_agent_steps_trained: 302500
  num_env_steps_sampled: 319400
  num_env_steps_trained: 302500
  num_samples_added_to_queue: 319000
  num_training_step_calls_since_last_synch_worker_weights: 955
  num_weight_broadcasts: 6263
custom_metrics: {}
date: 2023-08-18_16-42-20
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 8.0
episode_reward_mean: 2.32
episode_reward_min: 0.0
episodes_this_iter: 66
episodes_total: 2496
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.39141845703125
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -31.291610717773438
        total_loss: -22.943115234375
        var_gnorm: 63.316368103027344
        vf_explained_var: 0.034473419189453125
        vf_loss: 18.088409423828125
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 605.0
  learner_queue:
    size_count: 609
    size_mean: 14.68
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.6054905792311582
  num_agent_steps_sampled: 319400
  num_agent_steps_trained: 302500
  num_env_steps_sampled: 319400
  num_env_steps_trained: 302500
  num_samples_added_to_queue: 319000
  num_training_step_calls_since_last_synch_worker_weights: 955
  num_weight_broadcasts: 6263
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 415.873
    learner_load_time_ms: 2.697
    learner_load_wait_time_ms: 2.655
iterations_since_restore: 43
node_ip: 127.0.0.1
num_agent_steps_sampled: 319400
num_agent_steps_trained: 302500
num_env_steps_sampled: 319400
num_env_steps_sampled_this_iter: 8400
num_env_steps_sampled_throughput_per_sec: 839.9956341016714
num_env_steps_trained: 302500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9958420015918
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 48.92857142857143
  ram_util_percent: 79.69285714285714
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10906528130450573
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03983440770988768
  mean_inference_ms: 2.0709805442044416
  mean_raw_obs_processing_ms: 0.459532960740368
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03095269203186035
    StateBufferConnector_ms: 0.005541324615478516
    ViewRequirementAgentConnector_ms: 0.1857445240020752
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 8.0
  episode_reward_mean: 2.32
  episode_reward_min: 0.0
  episodes_this_iter: 66
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 7.0, 2.0, 2.0, 0.0, 4.0, 3.0, 3.0, 1.0, 3.0, 1.0, 3.0, 1.0,
      3.0, 2.0, 5.0, 2.0, 1.0, 0.0, 4.0, 3.0, 2.0, 3.0, 2.0, 1.0, 0.0, 0.0, 1.0, 2.0,
      1.0, 3.0, 3.0, 0.0, 7.0, 1.0, 1.0, 2.0, 4.0, 4.0, 4.0, 4.0, 2.0, 1.0, 1.0, 2.0,
      1.0, 2.0, 3.0, 5.0, 0.0, 5.0, 1.0, 1.0, 3.0, 0.0, 1.0, 3.0, 6.0, 3.0, 1.0, 0.0,
      2.0, 4.0, 3.0, 3.0, 1.0, 1.0, 0.0, 3.0, 2.0, 3.0, 3.0, 3.0, 5.0, 3.0, 3.0, 1.0,
      0.0, 1.0, 2.0, 3.0, 2.0, 3.0, 3.0, 4.0, 3.0, 3.0, 2.0, 3.0, 0.0, 8.0, 2.0, 1.0,
      4.0, 1.0, 1.0, 2.0, 1.0, 2.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10906528130450573
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03983440770988768
    mean_inference_ms: 2.0709805442044416
    mean_raw_obs_processing_ms: 0.459532960740368
time_since_restore: 442.9138617515564
time_this_iter_s: 10.176711082458496
time_total_s: 442.9138617515564
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.075
timestamp: 1692344540
timesteps_total: 319400
training_iteration: 43
trial_id: default
train step: 44
agent_timesteps_total: 327500
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03054523468017578
  StateBufferConnector_ms: 0.005354642868041992
  ViewRequirementAgentConnector_ms: 0.18392634391784668
counters:
  num_agent_steps_sampled: 327500
  num_agent_steps_trained: 311000
  num_env_steps_sampled: 327500
  num_env_steps_trained: 311000
  num_samples_added_to_queue: 327500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 6422
custom_metrics: {}
date: 2023-08-18_16-42-30
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 8.0
episode_reward_mean: 2.3
episode_reward_min: 0.0
episodes_this_iter: 63
episodes_total: 2559
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.34507417678833
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 9.86733627319336
        total_loss: 22.460168838500977
        var_gnorm: 63.31642150878906
        vf_explained_var: 0.08524256944656372
        vf_loss: 26.530738830566406
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 622.0
  learner_queue:
    size_count: 625
    size_mean: 14.98
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.4351306560728192
  num_agent_steps_sampled: 327500
  num_agent_steps_trained: 311000
  num_env_steps_sampled: 327500
  num_env_steps_trained: 311000
  num_samples_added_to_queue: 327500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 6422
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 435.086
    learner_load_time_ms: 2.701
    learner_load_wait_time_ms: 2.75
iterations_since_restore: 44
node_ip: 127.0.0.1
num_agent_steps_sampled: 327500
num_agent_steps_trained: 311000
num_env_steps_sampled: 327500
num_env_steps_sampled_this_iter: 8100
num_env_steps_sampled_throughput_per_sec: 809.1836210701655
num_env_steps_trained: 311000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.1433060612848
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 53.34666666666667
  ram_util_percent: 79.89333333333333
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1087997109625558
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03972130580054892
  mean_inference_ms: 2.065673971186951
  mean_raw_obs_processing_ms: 0.4585368788170095
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03054523468017578
    StateBufferConnector_ms: 0.005354642868041992
    ViewRequirementAgentConnector_ms: 0.18392634391784668
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 8.0
  episode_reward_mean: 2.3
  episode_reward_min: 0.0
  episodes_this_iter: 63
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 3.0, 1.0, 1.0, 0.0, 3.0, 2.0, 3.0, 3.0, 3.0, 5.0, 3.0, 3.0,
      1.0, 0.0, 1.0, 2.0, 3.0, 2.0, 3.0, 3.0, 4.0, 3.0, 3.0, 2.0, 3.0, 0.0, 8.0, 2.0,
      1.0, 4.0, 1.0, 1.0, 2.0, 1.0, 2.0, 4.0, 0.0, 3.0, 1.0, 1.0, 6.0, 2.0, 2.0, 3.0,
      2.0, 2.0, 5.0, 3.0, 2.0, 3.0, 5.0, 1.0, 3.0, 2.0, 2.0, 1.0, 2.0, 1.0, 0.0, 0.0,
      4.0, 1.0, 3.0, 1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 2.0, 6.0, 2.0, 6.0, 3.0, 1.0, 2.0,
      1.0, 1.0, 3.0, 0.0, 3.0, 3.0, 3.0, 2.0, 2.0, 4.0, 1.0, 1.0, 2.0, 5.0, 5.0, 1.0,
      2.0, 1.0, 2.0, 1.0, 1.0, 3.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1087997109625558
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03972130580054892
    mean_inference_ms: 2.065673971186951
    mean_raw_obs_processing_ms: 0.4585368788170095
time_since_restore: 453.05452251434326
time_this_iter_s: 10.140660762786865
time_total_s: 453.05452251434326
timers:
  sample_time_ms: 0.098
  synch_weights_time_ms: 0.698
  training_iteration_time_ms: 3.134
timestamp: 1692344550
timesteps_total: 327500
training_iteration: 44
trial_id: default
train step: 45
agent_timesteps_total: 335650
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0316004753112793
  StateBufferConnector_ms: 0.0057070255279541016
  ViewRequirementAgentConnector_ms: 0.19303369522094727
counters:
  num_agent_steps_sampled: 335650
  num_agent_steps_trained: 319000
  num_env_steps_sampled: 335650
  num_env_steps_trained: 319000
  num_samples_added_to_queue: 335500
  num_training_step_calls_since_last_synch_worker_weights: 304
  num_weight_broadcasts: 6583
custom_metrics: {}
date: 2023-08-18_16-42-41
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.39
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 2623
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.36703622341156
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -30.930227279663086
        total_loss: -23.611764907836914
        var_gnorm: 63.31655502319336
        vf_explained_var: 0.09551513195037842
        vf_loss: 16.00395965576172
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 638.0
  learner_queue:
    size_count: 644
    size_mean: 15.28
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.2335315156087419
  num_agent_steps_sampled: 335650
  num_agent_steps_trained: 319000
  num_env_steps_sampled: 335650
  num_env_steps_trained: 319000
  num_samples_added_to_queue: 335500
  num_training_step_calls_since_last_synch_worker_weights: 304
  num_weight_broadcasts: 6583
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 313.496
    learner_load_time_ms: 2.705
    learner_load_wait_time_ms: 2.363
iterations_since_restore: 45
node_ip: 127.0.0.1
num_agent_steps_sampled: 335650
num_agent_steps_trained: 319000
num_env_steps_sampled: 335650
num_env_steps_sampled_this_iter: 8150
num_env_steps_sampled_throughput_per_sec: 814.9929077050373
num_env_steps_trained: 319000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9930382380734
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 49.45714285714285
  ram_util_percent: 80.14285714285715
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10855889517608418
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03961315182905341
  mean_inference_ms: 2.0602131374826835
  mean_raw_obs_processing_ms: 0.45759342830466904
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0316004753112793
    StateBufferConnector_ms: 0.0057070255279541016
    ViewRequirementAgentConnector_ms: 0.19303369522094727
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.39
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 2.0, 6.0, 2.0, 6.0, 3.0, 1.0, 2.0,
      1.0, 1.0, 3.0, 0.0, 3.0, 3.0, 3.0, 2.0, 2.0, 4.0, 1.0, 1.0, 2.0, 5.0, 5.0, 1.0,
      2.0, 1.0, 2.0, 1.0, 1.0, 3.0, 1.0, 4.0, 4.0, 2.0, 1.0, 1.0, 2.0, 5.0, 3.0, 4.0,
      0.0, 1.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 1.0, 3.0, 2.0, 6.0, 5.0, 2.0, 3.0,
      2.0, 4.0, 3.0, 4.0, 3.0, 3.0, 4.0, 2.0, 2.0, 4.0, 3.0, 2.0, 2.0, 2.0, 3.0, 2.0,
      4.0, 2.0, 1.0, 3.0, 3.0, 0.0, 3.0, 1.0, 2.0, 3.0, 2.0, 5.0, 1.0, 2.0, 1.0, 2.0,
      2.0, 2.0, 4.0, 5.0, 2.0, 2.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10855889517608418
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03961315182905341
    mean_inference_ms: 2.0602131374826835
    mean_raw_obs_processing_ms: 0.45759342830466904
time_since_restore: 463.2725305557251
time_this_iter_s: 10.218008041381836
time_total_s: 463.2725305557251
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.085
timestamp: 1692344561
timesteps_total: 335650
training_iteration: 45
trial_id: default
train step: 46
agent_timesteps_total: 343300
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03379678726196289
  StateBufferConnector_ms: 0.006090402603149414
  ViewRequirementAgentConnector_ms: 0.20719480514526367
counters:
  num_agent_steps_sampled: 343300
  num_agent_steps_trained: 326500
  num_env_steps_sampled: 343300
  num_env_steps_trained: 326500
  num_samples_added_to_queue: 343000
  num_training_step_calls_since_last_synch_worker_weights: 537
  num_weight_broadcasts: 6733
custom_metrics: {}
date: 2023-08-18_16-42-51
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.39
episode_reward_min: 0.0
episodes_this_iter: 60
episodes_total: 2683
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3600804805755615
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -21.712549209594727
        total_loss: -15.790148735046387
        var_gnorm: 63.31672668457031
        vf_explained_var: 0.13861316442489624
        vf_loss: 13.20488166809082
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 653.0
  learner_queue:
    size_count: 658
    size_mean: 15.1
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.374772708486752
  num_agent_steps_sampled: 343300
  num_agent_steps_trained: 326500
  num_env_steps_sampled: 343300
  num_env_steps_trained: 326500
  num_samples_added_to_queue: 343000
  num_training_step_calls_since_last_synch_worker_weights: 537
  num_weight_broadcasts: 6733
  timing_breakdown:
    learner_dequeue_time_ms: 0.004
    learner_grad_time_ms: 383.644
    learner_load_time_ms: 2.677
    learner_load_wait_time_ms: 2.649
iterations_since_restore: 46
node_ip: 127.0.0.1
num_agent_steps_sampled: 343300
num_agent_steps_trained: 326500
num_env_steps_sampled: 343300
num_env_steps_sampled_this_iter: 7650
num_env_steps_sampled_throughput_per_sec: 764.9950207795937
num_env_steps_trained: 326500
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.9951184113663
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 50.14285714285713
  ram_util_percent: 80.29285714285713
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10843355561566423
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03955860158192812
  mean_inference_ms: 2.05711336918304
  mean_raw_obs_processing_ms: 0.4570970569572156
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03379678726196289
    StateBufferConnector_ms: 0.006090402603149414
    ViewRequirementAgentConnector_ms: 0.20719480514526367
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.39
  episode_reward_min: 0.0
  episodes_this_iter: 60
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 3.0, 4.0, 2.0, 2.0, 4.0, 3.0, 2.0,
      2.0, 2.0, 3.0, 2.0, 4.0, 2.0, 1.0, 3.0, 3.0, 0.0, 3.0, 1.0, 2.0, 3.0, 2.0, 5.0,
      1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 4.0, 5.0, 2.0, 2.0, 1.0, 4.0, 2.0, 2.0, 3.0, 4.0,
      2.0, 6.0, 1.0, 2.0, 4.0, 4.0, 0.0, 2.0, 2.0, 1.0, 1.0, 3.0, 1.0, 4.0, 3.0, 6.0,
      6.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 3.0, 2.0, 5.0, 3.0, 1.0, 5.0, 2.0, 0.0, 2.0,
      2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 3.0, 4.0, 2.0, 5.0, 3.0, 1.0,
      3.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10843355561566423
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03955860158192812
    mean_inference_ms: 2.05711336918304
    mean_raw_obs_processing_ms: 0.4570970569572156
time_since_restore: 473.48886156082153
time_this_iter_s: 10.216331005096436
time_total_s: 473.48886156082153
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344571
timesteps_total: 343300
training_iteration: 46
trial_id: default
train step: 47
agent_timesteps_total: 351650
connector_metrics:
  ObsPreprocessorConnector_ms: 0.031653404235839844
  StateBufferConnector_ms: 0.005685091018676758
  ViewRequirementAgentConnector_ms: 0.19002389907836914
counters:
  num_agent_steps_sampled: 351650
  num_agent_steps_trained: 335000
  num_env_steps_sampled: 351650
  num_env_steps_trained: 335000
  num_samples_added_to_queue: 351500
  num_training_step_calls_since_last_synch_worker_weights: 906
  num_weight_broadcasts: 6897
custom_metrics: {}
date: 2023-08-18_16-43-01
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.3
episode_reward_min: 0.0
episodes_this_iter: 65
episodes_total: 2748
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3592123985290527
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 80.30770874023438
        total_loss: 98.64582061767578
        var_gnorm: 63.31686782836914
        vf_explained_var: 0.12029838562011719
        vf_loss: 38.03542709350586
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 670.0
  learner_queue:
    size_count: 674
    size_mean: 15.02
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.3782597723216043
  num_agent_steps_sampled: 351650
  num_agent_steps_trained: 335000
  num_env_steps_sampled: 351650
  num_env_steps_trained: 335000
  num_samples_added_to_queue: 351500
  num_training_step_calls_since_last_synch_worker_weights: 906
  num_weight_broadcasts: 6897
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 387.66
    learner_load_time_ms: 2.7
    learner_load_wait_time_ms: 2.769
iterations_since_restore: 47
node_ip: 127.0.0.1
num_agent_steps_sampled: 351650
num_agent_steps_trained: 335000
num_env_steps_sampled: 351650
num_env_steps_sampled_this_iter: 8350
num_env_steps_sampled_throughput_per_sec: 834.9983277354353
num_env_steps_trained: 335000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9982976947545
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 48.5
  ram_util_percent: 79.86666666666666
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10822911709872178
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03947419602000359
  mean_inference_ms: 2.0527962869130905
  mean_raw_obs_processing_ms: 0.4563321607955127
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.031653404235839844
    StateBufferConnector_ms: 0.005685091018676758
    ViewRequirementAgentConnector_ms: 0.19002389907836914
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.3
  episode_reward_min: 0.0
  episodes_this_iter: 65
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 0.0, 2.0, 3.0, 2.0, 5.0, 3.0, 1.0, 5.0, 2.0, 0.0, 2.0, 2.0,
      2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 1.0, 3.0, 4.0, 2.0, 5.0, 3.0, 1.0, 3.0,
      2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 2.0, 3.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 6.0, 2.0,
      0.0, 4.0, 4.0, 1.0, 6.0, 4.0, 3.0, 2.0, 2.0, 3.0, 2.0, 4.0, 0.0, 0.0, 2.0, 1.0,
      3.0, 1.0, 2.0, 1.0, 3.0, 6.0, 5.0, 5.0, 1.0, 0.0, 5.0, 1.0, 1.0, 3.0, 1.0, 4.0,
      2.0, 3.0, 3.0, 3.0, 3.0, 0.0, 0.0, 1.0, 5.0, 2.0, 2.0, 4.0, 1.0, 3.0, 3.0, 3.0,
      0.0, 0.0, 5.0, 4.0, 0.0, 3.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10822911709872178
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03947419602000359
    mean_inference_ms: 2.0527962869130905
    mean_raw_obs_processing_ms: 0.4563321607955127
time_since_restore: 483.6813635826111
time_this_iter_s: 10.19250202178955
time_total_s: 483.6813635826111
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.075
timestamp: 1692344581
timesteps_total: 351650
training_iteration: 47
trial_id: default
train step: 48
agent_timesteps_total: 360200
connector_metrics:
  ObsPreprocessorConnector_ms: 0.030149221420288086
  StateBufferConnector_ms: 0.005398750305175781
  ViewRequirementAgentConnector_ms: 0.1804194450378418
counters:
  num_agent_steps_sampled: 360200
  num_agent_steps_trained: 343500
  num_env_steps_sampled: 360200
  num_env_steps_trained: 343500
  num_samples_added_to_queue: 360000
  num_training_step_calls_since_last_synch_worker_weights: 955
  num_weight_broadcasts: 7065
custom_metrics: {}
date: 2023-08-18_16-43-11
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.5
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 2815
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.4023865461349487
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -29.9212589263916
        total_loss: -22.56633949279785
        var_gnorm: 63.31709289550781
        vf_explained_var: 0.13881105184555054
        vf_loss: 16.112226486206055
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 687.0
  learner_queue:
    size_count: 691
    size_mean: 14.94
    size_quantiles: [11.0, 12.9, 16.0, 16.0, 16.0]
    size_std: 1.4616429112474771
  num_agent_steps_sampled: 360200
  num_agent_steps_trained: 343500
  num_env_steps_sampled: 360200
  num_env_steps_trained: 343500
  num_samples_added_to_queue: 360000
  num_training_step_calls_since_last_synch_worker_weights: 955
  num_weight_broadcasts: 7065
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 392.498
    learner_load_time_ms: 2.275
    learner_load_wait_time_ms: 2.621
iterations_since_restore: 48
node_ip: 127.0.0.1
num_agent_steps_sampled: 360200
num_agent_steps_trained: 343500
num_env_steps_sampled: 360200
num_env_steps_sampled_this_iter: 8550
num_env_steps_sampled_throughput_per_sec: 854.9977780638263
num_env_steps_trained: 343500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9977910576051
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 46.74285714285714
  ram_util_percent: 79.58571428571429
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10790396218228221
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03936205548963416
  mean_inference_ms: 2.0473960688171817
  mean_raw_obs_processing_ms: 0.4551359691988942
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.030149221420288086
    StateBufferConnector_ms: 0.005398750305175781
    ViewRequirementAgentConnector_ms: 0.1804194450378418
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.5
  episode_reward_min: 0.0
  episodes_this_iter: 67
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [5.0, 5.0, 1.0, 0.0, 5.0, 1.0, 1.0, 3.0, 1.0, 4.0, 2.0, 3.0, 3.0,
      3.0, 3.0, 0.0, 0.0, 1.0, 5.0, 2.0, 2.0, 4.0, 1.0, 3.0, 3.0, 3.0, 0.0, 0.0, 5.0,
      4.0, 0.0, 3.0, 0.0, 0.0, 1.0, 3.0, 5.0, 5.0, 3.0, 3.0, 2.0, 2.0, 6.0, 2.0, 3.0,
      1.0, 2.0, 0.0, 1.0, 3.0, 5.0, 1.0, 3.0, 3.0, 1.0, 1.0, 4.0, 2.0, 1.0, 4.0, 3.0,
      5.0, 1.0, 3.0, 3.0, 2.0, 2.0, 5.0, 0.0, 0.0, 4.0, 0.0, 3.0, 0.0, 3.0, 5.0, 3.0,
      5.0, 1.0, 1.0, 1.0, 6.0, 3.0, 6.0, 2.0, 2.0, 5.0, 3.0, 1.0, 2.0, 5.0, 2.0, 5.0,
      0.0, 3.0, 4.0, 4.0, 0.0, 1.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10790396218228221
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03936205548963416
    mean_inference_ms: 2.0473960688171817
    mean_raw_obs_processing_ms: 0.4551359691988942
time_since_restore: 493.8402063846588
time_this_iter_s: 10.15884280204773
time_total_s: 493.8402063846588
timers:
  sample_time_ms: 0.024
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.073
timestamp: 1692344591
timesteps_total: 360200
training_iteration: 48
trial_id: default
train step: 49
agent_timesteps_total: 368550
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03047943115234375
  StateBufferConnector_ms: 0.005561113357543945
  ViewRequirementAgentConnector_ms: 0.18318653106689453
counters:
  num_agent_steps_sampled: 368550
  num_agent_steps_trained: 352000
  num_env_steps_sampled: 368550
  num_env_steps_trained: 352000
  num_samples_added_to_queue: 368500
  num_training_step_calls_since_last_synch_worker_weights: 46
  num_weight_broadcasts: 7229
custom_metrics: {}
date: 2023-08-18_16-43-21
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.49
episode_reward_min: 0.0
episodes_this_iter: 65
episodes_total: 2880
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.10000000000002
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3318814039230347
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 9.624378204345703
        total_loss: 23.466691970825195
        var_gnorm: 63.31739044189453
        vf_explained_var: 0.09099245071411133
        vf_loss: 29.016510009765625
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 704.0
  learner_queue:
    size_count: 710
    size_mean: 15.24
    size_quantiles: [11.0, 13.0, 16.0, 16.0, 16.0]
    size_std: 1.3047605144240073
  num_agent_steps_sampled: 368550
  num_agent_steps_trained: 352000
  num_env_steps_sampled: 368550
  num_env_steps_trained: 352000
  num_samples_added_to_queue: 368500
  num_training_step_calls_since_last_synch_worker_weights: 46
  num_weight_broadcasts: 7229
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 263.024
    learner_load_time_ms: 2.281
    learner_load_wait_time_ms: 2.476
iterations_since_restore: 49
node_ip: 127.0.0.1
num_agent_steps_sampled: 368550
num_agent_steps_trained: 352000
num_env_steps_sampled: 368550
num_env_steps_sampled_this_iter: 8350
num_env_steps_sampled_throughput_per_sec: 834.997252711752
num_env_steps_trained: 352000
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9972033592686
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 46.63999999999999
  ram_util_percent: 79.35333333333334
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10763711630079496
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039245638964320616
  mean_inference_ms: 2.041866378120572
  mean_raw_obs_processing_ms: 0.45410424613296174
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03047943115234375
    StateBufferConnector_ms: 0.005561113357543945
    ViewRequirementAgentConnector_ms: 0.18318653106689453
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.49
  episode_reward_min: 0.0
  episodes_this_iter: 65
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 5.0, 0.0, 0.0, 4.0, 0.0, 3.0, 0.0, 3.0, 5.0, 3.0, 5.0,
      1.0, 1.0, 1.0, 6.0, 3.0, 6.0, 2.0, 2.0, 5.0, 3.0, 1.0, 2.0, 5.0, 2.0, 5.0, 0.0,
      3.0, 4.0, 4.0, 0.0, 1.0, 3.0, 2.0, 3.0, 2.0, 3.0, 1.0, 4.0, 2.0, 0.0, 1.0, 3.0,
      0.0, 2.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.0, 5.0, 3.0, 2.0, 2.0, 3.0, 2.0, 4.0,
      2.0, 2.0, 1.0, 3.0, 2.0, 1.0, 0.0, 5.0, 1.0, 2.0, 0.0, 1.0, 1.0, 3.0, 2.0, 5.0,
      3.0, 1.0, 5.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 5.0, 4.0, 3.0, 1.0, 4.0, 3.0, 1.0,
      3.0, 4.0, 2.0, 1.0, 2.0, 1.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10763711630079496
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039245638964320616
    mean_inference_ms: 2.041866378120572
    mean_raw_obs_processing_ms: 0.45410424613296174
time_since_restore: 504.06486678123474
time_this_iter_s: 10.224660396575928
time_total_s: 504.06486678123474
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.076
timestamp: 1692344601
timesteps_total: 368550
training_iteration: 49
trial_id: default
train step: 50
agent_timesteps_total: 377100
connector_metrics:
  ObsPreprocessorConnector_ms: 0.029907703399658203
  StateBufferConnector_ms: 0.005454540252685547
  ViewRequirementAgentConnector_ms: 0.18233013153076172
counters:
  num_agent_steps_sampled: 377100
  num_agent_steps_trained: 360500
  num_env_steps_sampled: 377100
  num_env_steps_trained: 360500
  num_samples_added_to_queue: 377000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 7397
custom_metrics: {}
date: 2023-08-18_16-43-32
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.62
episode_reward_min: 0.0
episodes_this_iter: 67
episodes_total: 2947
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 32.5
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3414181470870972
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 4.701654434204102
        total_loss: 12.767354965209961
        var_gnorm: 63.31767272949219
        vf_explained_var: 0.16048628091812134
        vf_loss: 17.47281837463379
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 721.0
  learner_queue:
    size_count: 727
    size_mean: 15.04
    size_quantiles: [11.0, 12.9, 16.0, 16.0, 16.0]
    size_std: 1.5094369811290564
  num_agent_steps_sampled: 377100
  num_agent_steps_trained: 360500
  num_env_steps_sampled: 377100
  num_env_steps_trained: 360500
  num_samples_added_to_queue: 377000
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 7397
  timing_breakdown:
    learner_dequeue_time_ms: 0.005
    learner_grad_time_ms: 265.123
    learner_load_time_ms: 2.316
    learner_load_wait_time_ms: 2.409
iterations_since_restore: 50
node_ip: 127.0.0.1
num_agent_steps_sampled: 377100
num_agent_steps_trained: 360500
num_env_steps_sampled: 377100
num_env_steps_sampled_this_iter: 8550
num_env_steps_sampled_throughput_per_sec: 854.9246848413827
num_env_steps_trained: 360500
num_env_steps_trained_this_iter: 8500
num_env_steps_trained_throughput_per_sec: 849.9251252809069
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8500
perf:
  cpu_util_percent: 44.41428571428571
  ram_util_percent: 79.22142857142858
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10736453464363162
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039121531038613606
  mean_inference_ms: 2.0359407480053306
  mean_raw_obs_processing_ms: 0.4530263070281033
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.029907703399658203
    StateBufferConnector_ms: 0.005454540252685547
    ViewRequirementAgentConnector_ms: 0.18233013153076172
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.62
  episode_reward_min: 0.0
  episodes_this_iter: 67
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 5.0, 1.0, 2.0, 0.0, 1.0, 1.0, 3.0, 2.0, 5.0, 3.0, 1.0, 5.0,
      2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 5.0, 4.0, 3.0, 1.0, 4.0, 3.0, 1.0, 3.0, 4.0, 2.0,
      1.0, 2.0, 1.0, 3.0, 0.0, 4.0, 3.0, 2.0, 2.0, 4.0, 1.0, 4.0, 2.0, 2.0, 3.0, 4.0,
      5.0, 2.0, 3.0, 4.0, 4.0, 4.0, 0.0, 3.0, 2.0, 3.0, 1.0, 4.0, 3.0, 4.0, 2.0, 6.0,
      2.0, 3.0, 1.0, 2.0, 1.0, 6.0, 3.0, 4.0, 1.0, 4.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0,
      1.0, 2.0, 3.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 3.0, 3.0, 5.0, 2.0, 5.0, 3.0, 0.0,
      3.0, 1.0, 2.0, 1.0, 3.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10736453464363162
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039121531038613606
    mean_inference_ms: 2.0359407480053306
    mean_raw_obs_processing_ms: 0.4530263070281033
time_since_restore: 514.2979278564453
time_this_iter_s: 10.233061075210571
time_total_s: 514.2979278564453
timers:
  sample_time_ms: 0.399
  synch_weights_time_ms: 0.736
  training_iteration_time_ms: 1.304
timestamp: 1692344612
timesteps_total: 377100
training_iteration: 50
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000050'.
train step: 51
agent_timesteps_total: 385100
connector_metrics:
  ObsPreprocessorConnector_ms: 0.031358957290649414
  StateBufferConnector_ms: 0.005627870559692383
  ViewRequirementAgentConnector_ms: 0.19025969505310059
counters:
  num_agent_steps_sampled: 385100
  num_agent_steps_trained: 368500
  num_env_steps_sampled: 385100
  num_env_steps_trained: 368500
  num_samples_added_to_queue: 385000
  num_training_step_calls_since_last_synch_worker_weights: 271
  num_weight_broadcasts: 7555
custom_metrics: {}
date: 2023-08-18_16-43-42
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.32
episode_reward_min: 0.0
episodes_this_iter: 62
episodes_total: 3009
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3584359884262085
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -27.47565460205078
        total_loss: -20.171009063720703
        var_gnorm: 63.31775665283203
        vf_explained_var: 0.12001949548721313
        vf_loss: 15.967729568481445
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 737.0
  learner_queue:
    size_count: 744
    size_mean: 14.46
    size_quantiles: [10.0, 11.0, 16.0, 16.0, 16.0]
    size_std: 1.9515122341404882
  num_agent_steps_sampled: 385100
  num_agent_steps_trained: 368500
  num_env_steps_sampled: 385100
  num_env_steps_trained: 368500
  num_samples_added_to_queue: 385000
  num_training_step_calls_since_last_synch_worker_weights: 271
  num_weight_broadcasts: 7555
  timing_breakdown:
    learner_dequeue_time_ms: 0.01
    learner_grad_time_ms: 261.664
    learner_load_time_ms: 2.326
    learner_load_wait_time_ms: 3.597
iterations_since_restore: 51
node_ip: 127.0.0.1
num_agent_steps_sampled: 385100
num_agent_steps_trained: 368500
num_env_steps_sampled: 385100
num_env_steps_sampled_this_iter: 8000
num_env_steps_sampled_throughput_per_sec: 799.9985694910833
num_env_steps_trained: 368500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9985694910833
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 47.98
  ram_util_percent: 80.06666666666665
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10722443257508715
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03905762647916748
  mean_inference_ms: 2.0324788321874907
  mean_raw_obs_processing_ms: 0.45244328210919965
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.031358957290649414
    StateBufferConnector_ms: 0.005627870559692383
    ViewRequirementAgentConnector_ms: 0.19025969505310059
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.32
  episode_reward_min: 0.0
  episodes_this_iter: 62
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 1.0, 2.0, 1.0, 6.0, 3.0, 4.0, 1.0, 4.0, 1.0, 1.0, 2.0, 2.0,
      2.0, 2.0, 1.0, 2.0, 3.0, 3.0, 4.0, 5.0, 4.0, 4.0, 3.0, 3.0, 3.0, 5.0, 2.0, 5.0,
      3.0, 0.0, 3.0, 1.0, 2.0, 1.0, 3.0, 1.0, 2.0, 3.0, 3.0, 2.0, 1.0, 1.0, 3.0, 6.0,
      1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 3.0, 2.0, 4.0, 0.0, 1.0, 4.0, 1.0, 3.0, 2.0, 0.0,
      7.0, 3.0, 0.0, 2.0, 2.0, 3.0, 2.0, 5.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 0.0, 0.0,
      4.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 4.0, 4.0, 1.0, 0.0, 0.0, 4.0, 2.0,
      3.0, 3.0, 0.0, 4.0, 0.0, 2.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10722443257508715
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03905762647916748
    mean_inference_ms: 2.0324788321874907
    mean_raw_obs_processing_ms: 0.45244328210919965
time_since_restore: 524.7575509548187
time_this_iter_s: 10.459623098373413
time_total_s: 524.7575509548187
timers:
  sample_time_ms: 0.037
  synch_weights_time_ms: 0.011
  training_iteration_time_ms: 0.11
timestamp: 1692344622
timesteps_total: 385100
training_iteration: 51
trial_id: default
train step: 52
agent_timesteps_total: 393300
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0323643684387207
  StateBufferConnector_ms: 0.005784511566162109
  ViewRequirementAgentConnector_ms: 0.19418740272521973
counters:
  num_agent_steps_sampled: 393300
  num_agent_steps_trained: 376500
  num_env_steps_sampled: 393300
  num_env_steps_trained: 376500
  num_samples_added_to_queue: 393000
  num_training_step_calls_since_last_synch_worker_weights: 713
  num_weight_broadcasts: 7716
custom_metrics: {}
date: 2023-08-18_16-43-52
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.37
episode_reward_min: 0.0
episodes_this_iter: 64
episodes_total: 3073
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3350900411605835
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -25.999679565429688
        total_loss: -17.60523223876953
        var_gnorm: 63.31784439086914
        vf_explained_var: 0.12143737077713013
        vf_loss: 18.123985290527344
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 753.0
  learner_queue:
    size_count: 758
    size_mean: 14.18
    size_quantiles: [10.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 1.9868568141665368
  num_agent_steps_sampled: 393300
  num_agent_steps_trained: 376500
  num_env_steps_sampled: 393300
  num_env_steps_trained: 376500
  num_samples_added_to_queue: 393000
  num_training_step_calls_since_last_synch_worker_weights: 713
  num_weight_broadcasts: 7716
  timing_breakdown:
    learner_dequeue_time_ms: 0.01
    learner_grad_time_ms: 407.488
    learner_load_time_ms: 2.492
    learner_load_wait_time_ms: 2.935
iterations_since_restore: 52
node_ip: 127.0.0.1
num_agent_steps_sampled: 393300
num_agent_steps_trained: 376500
num_env_steps_sampled: 393300
num_env_steps_sampled_this_iter: 8200
num_env_steps_sampled_throughput_per_sec: 819.998944283891
num_env_steps_trained: 376500
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9989700330643
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 47.67857142857143
  ram_util_percent: 80.67857142857143
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10707963768434897
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0389918708942581
  mean_inference_ms: 2.0290882454671872
  mean_raw_obs_processing_ms: 0.4518204078465925
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0323643684387207
    StateBufferConnector_ms: 0.005784511566162109
    ViewRequirementAgentConnector_ms: 0.19418740272521973
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.37
  episode_reward_min: 0.0
  episodes_this_iter: 64
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 2.0, 3.0, 2.0, 5.0, 3.0, 2.0, 3.0, 2.0, 2.0, 3.0, 0.0, 0.0,
      4.0, 2.0, 3.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0, 4.0, 4.0, 1.0, 0.0, 0.0, 4.0, 2.0,
      3.0, 3.0, 0.0, 4.0, 0.0, 2.0, 3.0, 5.0, 4.0, 1.0, 2.0, 3.0, 5.0, 0.0, 4.0, 2.0,
      2.0, 0.0, 4.0, 3.0, 0.0, 4.0, 3.0, 3.0, 2.0, 3.0, 3.0, 5.0, 4.0, 1.0, 3.0, 5.0,
      7.0, 5.0, 2.0, 1.0, 5.0, 4.0, 0.0, 0.0, 0.0, 2.0, 1.0, 2.0, 3.0, 1.0, 3.0, 2.0,
      3.0, 1.0, 4.0, 0.0, 2.0, 4.0, 1.0, 2.0, 3.0, 2.0, 3.0, 1.0, 5.0, 2.0, 2.0, 2.0,
      1.0, 0.0, 1.0, 3.0, 2.0, 1.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10707963768434897
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0389918708942581
    mean_inference_ms: 2.0290882454671872
    mean_raw_obs_processing_ms: 0.4518204078465925
time_since_restore: 534.9617969989777
time_this_iter_s: 10.204246044158936
time_total_s: 534.9617969989777
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.075
timestamp: 1692344632
timesteps_total: 393300
training_iteration: 52
trial_id: default
train step: 53
agent_timesteps_total: 400700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03401803970336914
  StateBufferConnector_ms: 0.0062029361724853516
  ViewRequirementAgentConnector_ms: 0.20458459854125977
counters:
  num_agent_steps_sampled: 400700
  num_agent_steps_trained: 384000
  num_env_steps_sampled: 400700
  num_env_steps_trained: 384000
  num_samples_added_to_queue: 400500
  num_training_step_calls_since_last_synch_worker_weights: 262
  num_weight_broadcasts: 7861
custom_metrics: {}
date: 2023-08-18_16-44-03
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.55
episode_reward_min: 0.0
episodes_this_iter: 59
episodes_total: 3132
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3235743045806885
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 0.2795398235321045
        total_loss: 7.894427299499512
        var_gnorm: 63.31800842285156
        vf_explained_var: 0.152002215385437
        vf_loss: 16.553348541259766
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 768.0
  learner_queue:
    size_count: 775
    size_mean: 14.14
    size_quantiles: [10.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 2.0494877408757537
  num_agent_steps_sampled: 400700
  num_agent_steps_trained: 384000
  num_env_steps_sampled: 400700
  num_env_steps_trained: 384000
  num_samples_added_to_queue: 400500
  num_training_step_calls_since_last_synch_worker_weights: 262
  num_weight_broadcasts: 7861
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 267.381
    learner_load_time_ms: 2.597
    learner_load_wait_time_ms: 2.796
iterations_since_restore: 53
node_ip: 127.0.0.1
num_agent_steps_sampled: 400700
num_agent_steps_trained: 384000
num_env_steps_sampled: 400700
num_env_steps_sampled_this_iter: 7400
num_env_steps_sampled_throughput_per_sec: 739.9932075170303
num_env_steps_trained: 384000
num_env_steps_trained_this_iter: 7500
num_env_steps_trained_throughput_per_sec: 749.99311572672
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7500
perf:
  cpu_util_percent: 53.80666666666668
  ram_util_percent: 80.14000000000001
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10704202621149887
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03898384607922788
  mean_inference_ms: 2.0276122673898525
  mean_raw_obs_processing_ms: 0.4515893219316306
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03401803970336914
    StateBufferConnector_ms: 0.0062029361724853516
    ViewRequirementAgentConnector_ms: 0.20458459854125977
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.55
  episode_reward_min: 0.0
  episodes_this_iter: 59
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 5.0, 7.0, 5.0, 2.0, 1.0, 5.0, 4.0, 0.0, 0.0, 0.0, 2.0, 1.0,
      2.0, 3.0, 1.0, 3.0, 2.0, 3.0, 1.0, 4.0, 0.0, 2.0, 4.0, 1.0, 2.0, 3.0, 2.0, 3.0,
      1.0, 5.0, 2.0, 2.0, 2.0, 1.0, 0.0, 1.0, 3.0, 2.0, 1.0, 4.0, 1.0, 1.0, 4.0, 4.0,
      2.0, 2.0, 2.0, 4.0, 5.0, 3.0, 0.0, 1.0, 2.0, 4.0, 4.0, 4.0, 2.0, 1.0, 2.0, 1.0,
      3.0, 1.0, 4.0, 1.0, 3.0, 5.0, 5.0, 4.0, 1.0, 1.0, 1.0, 4.0, 3.0, 3.0, 3.0, 3.0,
      3.0, 4.0, 5.0, 1.0, 0.0, 2.0, 1.0, 3.0, 1.0, 2.0, 3.0, 3.0, 2.0, 3.0, 5.0, 4.0,
      3.0, 4.0, 2.0, 5.0, 3.0, 2.0, 5.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10704202621149887
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03898384607922788
    mean_inference_ms: 2.0276122673898525
    mean_raw_obs_processing_ms: 0.4515893219316306
time_since_restore: 545.2375500202179
time_this_iter_s: 10.275753021240234
time_total_s: 545.2375500202179
timers:
  sample_time_ms: 0.032
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.091
timestamp: 1692344643
timesteps_total: 400700
training_iteration: 53
trial_id: default
train step: 54
agent_timesteps_total: 407900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03670001029968262
  StateBufferConnector_ms: 0.006609439849853516
  ViewRequirementAgentConnector_ms: 0.2167215347290039
counters:
  num_agent_steps_sampled: 407900
  num_agent_steps_trained: 391000
  num_env_steps_sampled: 407900
  num_env_steps_trained: 391000
  num_samples_added_to_queue: 407500
  num_training_step_calls_since_last_synch_worker_weights: 537
  num_weight_broadcasts: 8002
custom_metrics: {}
date: 2023-08-18_16-44-13
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.78
episode_reward_min: 0.0
episodes_this_iter: 56
episodes_total: 3188
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3311119079589844
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 39.9814453125
        total_loss: 54.54384231567383
        var_gnorm: 63.318180084228516
        vf_explained_var: 0.15001839399337769
        vf_loss: 30.45591163635254
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 782.0
  learner_queue:
    size_count: 788
    size_mean: 14.22
    size_quantiles: [10.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 1.9109160107131866
  num_agent_steps_sampled: 407900
  num_agent_steps_trained: 391000
  num_env_steps_sampled: 407900
  num_env_steps_trained: 391000
  num_samples_added_to_queue: 407500
  num_training_step_calls_since_last_synch_worker_weights: 537
  num_weight_broadcasts: 8002
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 468.432
    learner_load_time_ms: 2.679
    learner_load_wait_time_ms: 2.954
iterations_since_restore: 54
node_ip: 127.0.0.1
num_agent_steps_sampled: 407900
num_agent_steps_trained: 391000
num_env_steps_sampled: 407900
num_env_steps_sampled_this_iter: 7200
num_env_steps_sampled_throughput_per_sec: 719.9987640402076
num_env_steps_trained: 391000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9987983724241
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 50.385714285714286
  ram_util_percent: 80.41428571428573
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10712954008479736
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039018028669518506
  mean_inference_ms: 2.0277901800098137
  mean_raw_obs_processing_ms: 0.4518272250834087
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03670001029968262
    StateBufferConnector_ms: 0.006609439849853516
    ViewRequirementAgentConnector_ms: 0.2167215347290039
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.78
  episode_reward_min: 0.0
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 4.0, 1.0, 3.0, 5.0, 5.0, 4.0,
      1.0, 1.0, 1.0, 4.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 5.0, 1.0, 0.0, 2.0, 1.0, 3.0,
      1.0, 2.0, 3.0, 3.0, 2.0, 3.0, 5.0, 4.0, 3.0, 4.0, 2.0, 5.0, 3.0, 2.0, 5.0, 4.0,
      4.0, 2.0, 3.0, 1.0, 1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 3.0, 4.0, 2.0,
      4.0, 2.0, 4.0, 2.0, 3.0, 0.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 2.0, 2.0, 3.0,
      3.0, 2.0, 3.0, 2.0, 4.0, 4.0, 2.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 1.0,
      2.0, 5.0, 1.0, 2.0, 2.0, 6.0, 4.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10712954008479736
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039018028669518506
    mean_inference_ms: 2.0277901800098137
    mean_raw_obs_processing_ms: 0.4518272250834087
time_since_restore: 555.5212938785553
time_this_iter_s: 10.283743858337402
time_total_s: 555.5212938785553
timers:
  sample_time_ms: 0.028
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.086
timestamp: 1692344653
timesteps_total: 407900
training_iteration: 54
trial_id: default
train step: 55
agent_timesteps_total: 414550
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03966474533081055
  StateBufferConnector_ms: 0.006896257400512695
  ViewRequirementAgentConnector_ms: 0.2264852523803711
counters:
  num_agent_steps_sampled: 414550
  num_agent_steps_trained: 398000
  num_env_steps_sampled: 414550
  num_env_steps_trained: 398000
  num_samples_added_to_queue: 414500
  num_training_step_calls_since_last_synch_worker_weights: 1065
  num_weight_broadcasts: 8132
custom_metrics: {}
date: 2023-08-18_16-44-23
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.58
episode_reward_min: 0.0
episodes_this_iter: 51
episodes_total: 3239
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3499953746795654
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -32.38603591918945
        total_loss: -24.474031448364258
        var_gnorm: 63.31832504272461
        vf_explained_var: 0.14157849550247192
        vf_loss: 17.17400550842285
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 796.0
  learner_queue:
    size_count: 801
    size_mean: 14.56
    size_quantiles: [10.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.722324011328879
  num_agent_steps_sampled: 414550
  num_agent_steps_trained: 398000
  num_env_steps_sampled: 414550
  num_env_steps_trained: 398000
  num_samples_added_to_queue: 414500
  num_training_step_calls_since_last_synch_worker_weights: 1065
  num_weight_broadcasts: 8132
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 454.559
    learner_load_time_ms: 2.715
    learner_load_wait_time_ms: 3.053
iterations_since_restore: 55
node_ip: 127.0.0.1
num_agent_steps_sampled: 414550
num_agent_steps_trained: 398000
num_env_steps_sampled: 414550
num_env_steps_sampled_this_iter: 6650
num_env_steps_sampled_throughput_per_sec: 664.9947520908607
num_env_steps_trained: 398000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9944758851166
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 57.199999999999996
  ram_util_percent: 81.49333333333334
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10731192657692867
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03909044376506171
  mean_inference_ms: 2.029580106830343
  mean_raw_obs_processing_ms: 0.45249314484523895
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03966474533081055
    StateBufferConnector_ms: 0.006896257400512695
    ViewRequirementAgentConnector_ms: 0.2264852523803711
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.58
  episode_reward_min: 0.0
  episodes_this_iter: 51
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 2.0, 1.0, 3.0, 4.0, 5.0, 2.0, 3.0, 4.0, 2.0, 4.0, 2.0, 4.0,
      2.0, 3.0, 0.0, 1.0, 3.0, 4.0, 5.0, 2.0, 1.0, 3.0, 2.0, 2.0, 3.0, 3.0, 2.0, 3.0,
      2.0, 4.0, 4.0, 2.0, 2.0, 2.0, 3.0, 5.0, 3.0, 4.0, 3.0, 5.0, 1.0, 2.0, 5.0, 1.0,
      2.0, 2.0, 6.0, 4.0, 4.0, 0.0, 4.0, 2.0, 2.0, 1.0, 6.0, 3.0, 1.0, 2.0, 2.0, 4.0,
      2.0, 2.0, 1.0, 1.0, 3.0, 4.0, 3.0, 1.0, 2.0, 4.0, 1.0, 3.0, 2.0, 0.0, 3.0, 1.0,
      2.0, 5.0, 1.0, 1.0, 2.0, 0.0, 2.0, 4.0, 4.0, 2.0, 2.0, 3.0, 1.0, 2.0, 4.0, 4.0,
      3.0, 2.0, 4.0, 1.0, 2.0, 2.0, 1.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10731192657692867
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03909044376506171
    mean_inference_ms: 2.029580106830343
    mean_raw_obs_processing_ms: 0.45249314484523895
time_since_restore: 565.7876808643341
time_this_iter_s: 10.266386985778809
time_total_s: 565.7876808643341
timers:
  sample_time_ms: 0.065
  synch_weights_time_ms: 0.017
  training_iteration_time_ms: 0.147
timestamp: 1692344663
timesteps_total: 414550
training_iteration: 55
trial_id: default
train step: 56
agent_timesteps_total: 420950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.040297508239746094
  StateBufferConnector_ms: 0.007058382034301758
  ViewRequirementAgentConnector_ms: 0.2507803440093994
counters:
  num_agent_steps_sampled: 420950
  num_agent_steps_trained: 404000
  num_env_steps_sampled: 420950
  num_env_steps_trained: 404000
  num_samples_added_to_queue: 420500
  num_training_step_calls_since_last_synch_worker_weights: 142
  num_weight_broadcasts: 8257
custom_metrics: {}
date: 2023-08-18_16-44-34
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 6.0
episode_reward_mean: 2.33
episode_reward_min: 0.0
episodes_this_iter: 50
episodes_total: 3289
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3387386798858643
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 27.285810470581055
        total_loss: 41.52961730957031
        var_gnorm: 63.3184814453125
        vf_explained_var: 0.19818055629730225
        vf_loss: 29.826351165771484
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 808.0
  learner_queue:
    size_count: 815
    size_mean: 14.32
    size_quantiles: [10.0, 11.9, 15.0, 16.0, 16.0]
    size_std: 1.8701871564097534
  num_agent_steps_sampled: 420950
  num_agent_steps_trained: 404000
  num_env_steps_sampled: 420950
  num_env_steps_trained: 404000
  num_samples_added_to_queue: 420500
  num_training_step_calls_since_last_synch_worker_weights: 142
  num_weight_broadcasts: 8257
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 408.095
    learner_load_time_ms: 2.715
    learner_load_wait_time_ms: 3.062
iterations_since_restore: 56
node_ip: 127.0.0.1
num_agent_steps_sampled: 420950
num_agent_steps_trained: 404000
num_env_steps_sampled: 420950
num_env_steps_sampled_this_iter: 6400
num_env_steps_sampled_throughput_per_sec: 639.9985351596027
num_env_steps_trained: 404000
num_env_steps_trained_this_iter: 6000
num_env_steps_trained_throughput_per_sec: 599.9986267121276
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6000
perf:
  cpu_util_percent: 60.57142857142856
  ram_util_percent: 80.7
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10760390555637481
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03919744475263066
  mean_inference_ms: 2.0336543408145933
  mean_raw_obs_processing_ms: 0.4536257855584769
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.040297508239746094
    StateBufferConnector_ms: 0.007058382034301758
    ViewRequirementAgentConnector_ms: 0.2507803440093994
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 6.0
  episode_reward_mean: 2.33
  episode_reward_min: 0.0
  episodes_this_iter: 50
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [0.0, 4.0, 2.0, 2.0, 1.0, 6.0, 3.0, 1.0, 2.0, 2.0, 4.0, 2.0, 2.0,
      1.0, 1.0, 3.0, 4.0, 3.0, 1.0, 2.0, 4.0, 1.0, 3.0, 2.0, 0.0, 3.0, 1.0, 2.0, 5.0,
      1.0, 1.0, 2.0, 0.0, 2.0, 4.0, 4.0, 2.0, 2.0, 3.0, 1.0, 2.0, 4.0, 4.0, 3.0, 2.0,
      4.0, 1.0, 2.0, 2.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 3.0,
      4.0, 3.0, 3.0, 4.0, 1.0, 3.0, 4.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 2.0,
      3.0, 3.0, 3.0, 2.0, 4.0, 3.0, 1.0, 2.0, 3.0, 5.0, 1.0, 2.0, 3.0, 1.0, 5.0, 3.0,
      3.0, 2.0, 3.0, 2.0, 1.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10760390555637481
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03919744475263066
    mean_inference_ms: 2.0336543408145933
    mean_raw_obs_processing_ms: 0.4536257855584769
time_since_restore: 576.1549611091614
time_this_iter_s: 10.36728024482727
time_total_s: 576.1549611091614
timers:
  sample_time_ms: 0.031
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.09
timestamp: 1692344674
timesteps_total: 420950
training_iteration: 56
trial_id: default
train step: 57
agent_timesteps_total: 427150
connector_metrics:
  ObsPreprocessorConnector_ms: 0.039290666580200195
  StateBufferConnector_ms: 0.007377147674560547
  ViewRequirementAgentConnector_ms: 0.2532007694244385
counters:
  num_agent_steps_sampled: 427150
  num_agent_steps_trained: 410500
  num_env_steps_sampled: 427150
  num_env_steps_trained: 410500
  num_samples_added_to_queue: 427000
  num_training_step_calls_since_last_synch_worker_weights: 431
  num_weight_broadcasts: 8378
custom_metrics: {}
date: 2023-08-18_16-44-44
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 8.0
episode_reward_mean: 2.36
episode_reward_min: 0.0
episodes_this_iter: 49
episodes_total: 3338
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3316525220870972
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -23.356220245361328
        total_loss: -12.262103080749512
        var_gnorm: 63.318660736083984
        vf_explained_var: 0.06760179996490479
        vf_loss: 23.519886016845703
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 821.0
  learner_queue:
    size_count: 827
    size_mean: 14.36
    size_quantiles: [10.0, 11.9, 15.0, 16.0, 16.0]
    size_std: 1.8194504664870657
  num_agent_steps_sampled: 427150
  num_agent_steps_trained: 410500
  num_env_steps_sampled: 427150
  num_env_steps_trained: 410500
  num_samples_added_to_queue: 427000
  num_training_step_calls_since_last_synch_worker_weights: 431
  num_weight_broadcasts: 8378
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 627.375
    learner_load_time_ms: 3.454
    learner_load_wait_time_ms: 19.14
iterations_since_restore: 57
node_ip: 127.0.0.1
num_agent_steps_sampled: 427150
num_agent_steps_trained: 410500
num_env_steps_sampled: 427150
num_env_steps_sampled_this_iter: 6200
num_env_steps_sampled_throughput_per_sec: 619.9962158434091
num_env_steps_trained: 410500
num_env_steps_trained_this_iter: 6500
num_env_steps_trained_throughput_per_sec: 649.996032739058
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6500
perf:
  cpu_util_percent: 59.553333333333335
  ram_util_percent: 80.52
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10793473224944503
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039346214141004816
  mean_inference_ms: 2.0396461644437527
  mean_raw_obs_processing_ms: 0.4549614116599966
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.039290666580200195
    StateBufferConnector_ms: 0.007377147674560547
    ViewRequirementAgentConnector_ms: 0.2532007694244385
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 8.0
  episode_reward_mean: 2.36
  episode_reward_min: 0.0
  episodes_this_iter: 49
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 4.0, 4.0, 2.0, 2.0, 3.0, 4.0,
      3.0, 3.0, 4.0, 1.0, 3.0, 4.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 1.0, 2.0, 2.0, 3.0,
      3.0, 3.0, 2.0, 4.0, 3.0, 1.0, 2.0, 3.0, 5.0, 1.0, 2.0, 3.0, 1.0, 5.0, 3.0, 3.0,
      2.0, 3.0, 2.0, 1.0, 2.0, 2.0, 1.0, 2.0, 1.0, 3.0, 2.0, 2.0, 3.0, 0.0, 3.0, 4.0,
      7.0, 1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 3.0,
      5.0, 1.0, 8.0, 3.0, 1.0, 1.0, 6.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 1.0, 1.0, 7.0,
      3.0, 2.0, 4.0, 4.0, 2.0, 4.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10793473224944503
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039346214141004816
    mean_inference_ms: 2.0396461644437527
    mean_raw_obs_processing_ms: 0.4549614116599966
time_since_restore: 586.4133958816528
time_this_iter_s: 10.258434772491455
time_total_s: 586.4133958816528
timers:
  sample_time_ms: 0.029
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.086
timestamp: 1692344684
timesteps_total: 427150
training_iteration: 57
trial_id: default
train step: 58
agent_timesteps_total: 433900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0393826961517334
  StateBufferConnector_ms: 0.00732731819152832
  ViewRequirementAgentConnector_ms: 0.23353290557861328
counters:
  num_agent_steps_sampled: 433900
  num_agent_steps_trained: 417000
  num_env_steps_sampled: 433900
  num_env_steps_trained: 417000
  num_samples_added_to_queue: 433500
  num_training_step_calls_since_last_synch_worker_weights: 608
  num_weight_broadcasts: 8510
custom_metrics: {}
date: 2023-08-18_16-44-54
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 8.0
episode_reward_mean: 2.42
episode_reward_min: 0.0
episodes_this_iter: 53
episodes_total: 3391
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3311971426010132
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -24.478538513183594
        total_loss: -18.750076293945312
        var_gnorm: 63.3188362121582
        vf_explained_var: 0.11103183031082153
        vf_loss: 12.788119316101074
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 834.0
  learner_queue:
    size_count: 839
    size_mean: 14.42
    size_quantiles: [10.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.7898603297464304
  num_agent_steps_sampled: 433900
  num_agent_steps_trained: 417000
  num_env_steps_sampled: 433900
  num_env_steps_trained: 417000
  num_samples_added_to_queue: 433500
  num_training_step_calls_since_last_synch_worker_weights: 608
  num_weight_broadcasts: 8510
  timing_breakdown:
    learner_dequeue_time_ms: 0.016
    learner_grad_time_ms: 648.468
    learner_load_time_ms: 3.457
    learner_load_wait_time_ms: 3.085
iterations_since_restore: 58
node_ip: 127.0.0.1
num_agent_steps_sampled: 433900
num_agent_steps_trained: 417000
num_env_steps_sampled: 433900
num_env_steps_sampled_this_iter: 6750
num_env_steps_sampled_throughput_per_sec: 674.9946570819342
num_env_steps_trained: 417000
num_env_steps_trained_this_iter: 6500
num_env_steps_trained_throughput_per_sec: 649.9948549677885
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6500
perf:
  cpu_util_percent: 54.52857142857143
  ram_util_percent: 80.39999999999999
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10819264080475001
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039477470409940624
  mean_inference_ms: 2.0445732256506073
  mean_raw_obs_processing_ms: 0.4559643016189236
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0393826961517334
    StateBufferConnector_ms: 0.00732731819152832
    ViewRequirementAgentConnector_ms: 0.23353290557861328
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 8.0
  episode_reward_mean: 2.42
  episode_reward_min: 0.0
  episodes_this_iter: 53
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 3.0, 2.0, 2.0, 3.0, 0.0, 3.0, 4.0, 7.0, 1.0, 3.0, 2.0, 3.0,
      3.0, 1.0, 1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 3.0, 5.0, 1.0, 8.0, 3.0, 1.0,
      1.0, 6.0, 2.0, 1.0, 2.0, 2.0, 3.0, 2.0, 1.0, 1.0, 7.0, 3.0, 2.0, 4.0, 4.0, 2.0,
      4.0, 0.0, 3.0, 1.0, 1.0, 6.0, 4.0, 0.0, 4.0, 3.0, 1.0, 2.0, 3.0, 6.0, 0.0, 2.0,
      3.0, 3.0, 2.0, 1.0, 3.0, 4.0, 1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 0.0, 1.0, 3.0, 2.0,
      2.0, 2.0, 1.0, 5.0, 1.0, 3.0, 5.0, 3.0, 2.0, 4.0, 7.0, 5.0, 2.0, 2.0, 3.0, 2.0,
      3.0, 3.0, 0.0, 0.0, 0.0, 2.0, 3.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10819264080475001
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039477470409940624
    mean_inference_ms: 2.0445732256506073
    mean_raw_obs_processing_ms: 0.4559643016189236
time_since_restore: 596.6160387992859
time_this_iter_s: 10.202642917633057
time_total_s: 596.6160387992859
timers:
  sample_time_ms: 0.025
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.074
timestamp: 1692344694
timesteps_total: 433900
training_iteration: 58
trial_id: default
train step: 59
agent_timesteps_total: 440900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03940773010253906
  StateBufferConnector_ms: 0.007068157196044922
  ViewRequirementAgentConnector_ms: 0.2304701805114746
counters:
  num_agent_steps_sampled: 440900
  num_agent_steps_trained: 424000
  num_env_steps_sampled: 440900
  num_env_steps_trained: 424000
  num_samples_added_to_queue: 440500
  num_training_step_calls_since_last_synch_worker_weights: 1820
  num_weight_broadcasts: 8647
custom_metrics: {}
date: 2023-08-18_16-45-05
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 10.0
episode_reward_mean: 2.65
episode_reward_min: 0.0
episodes_this_iter: 54
episodes_total: 3445
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 30.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.2981950044631958
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 17.405330657958984
        total_loss: 26.86479949951172
        var_gnorm: 63.319026947021484
        vf_explained_var: 0.18243670463562012
        vf_loss: 20.217134475708008
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 848.0
  learner_queue:
    size_count: 853
    size_mean: 14.38
    size_quantiles: [10.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.8208789086592223
  num_agent_steps_sampled: 440900
  num_agent_steps_trained: 424000
  num_env_steps_sampled: 440900
  num_env_steps_trained: 424000
  num_samples_added_to_queue: 440500
  num_training_step_calls_since_last_synch_worker_weights: 1820
  num_weight_broadcasts: 8647
  timing_breakdown:
    learner_dequeue_time_ms: 0.02
    learner_grad_time_ms: 490.181
    learner_load_time_ms: 3.525
    learner_load_wait_time_ms: 3.692
iterations_since_restore: 59
node_ip: 127.0.0.1
num_agent_steps_sampled: 440900
num_agent_steps_trained: 424000
num_env_steps_sampled: 440900
num_env_steps_sampled_this_iter: 7000
num_env_steps_sampled_throughput_per_sec: 699.9947429098125
num_env_steps_trained: 424000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9947429098125
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 54.32
  ram_util_percent: 79.80666666666666
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.1083266844096658
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03956586117782159
  mean_inference_ms: 2.0478661865177497
  mean_raw_obs_processing_ms: 0.4565736489412814
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03940773010253906
    StateBufferConnector_ms: 0.007068157196044922
    ViewRequirementAgentConnector_ms: 0.2304701805114746
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 10.0
  episode_reward_mean: 2.65
  episode_reward_min: 0.0
  episodes_this_iter: 54
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 1.0, 2.0, 3.0, 6.0, 0.0, 2.0, 3.0, 3.0, 2.0, 1.0, 3.0, 4.0,
      1.0, 1.0, 2.0, 3.0, 2.0, 2.0, 0.0, 1.0, 3.0, 2.0, 2.0, 2.0, 1.0, 5.0, 1.0, 3.0,
      5.0, 3.0, 2.0, 4.0, 7.0, 5.0, 2.0, 2.0, 3.0, 2.0, 3.0, 3.0, 0.0, 0.0, 0.0, 2.0,
      3.0, 1.0, 3.0, 0.0, 2.0, 3.0, 2.0, 0.0, 7.0, 1.0, 6.0, 1.0, 1.0, 10.0, 3.0,
      3.0, 2.0, 2.0, 3.0, 0.0, 3.0, 4.0, 4.0, 0.0, 6.0, 4.0, 3.0, 4.0, 4.0, 3.0, 8.0,
      1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 4.0, 3.0, 1.0, 0.0, 6.0, 2.0, 2.0,
      4.0, 4.0, 3.0, 5.0, 3.0, 3.0, 0.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.1083266844096658
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03956586117782159
    mean_inference_ms: 2.0478661865177497
    mean_raw_obs_processing_ms: 0.4565736489412814
time_since_restore: 606.9183070659637
time_this_iter_s: 10.302268266677856
time_total_s: 606.9183070659637
timers:
  sample_time_ms: 0.03
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.089
timestamp: 1692344705
timesteps_total: 440900
training_iteration: 59
trial_id: default
train step: 60
agent_timesteps_total: 448900
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03543567657470703
  StateBufferConnector_ms: 0.006400346755981445
  ViewRequirementAgentConnector_ms: 0.21298742294311523
counters:
  num_agent_steps_sampled: 448900
  num_agent_steps_trained: 432000
  num_env_steps_sampled: 448900
  num_env_steps_trained: 432000
  num_samples_added_to_queue: 448500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 8804
custom_metrics: {}
date: 2023-08-18_16-45-15
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 8.0
episode_reward_mean: 2.73
episode_reward_min: 0.0
episodes_this_iter: 62
episodes_total: 3507
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.10000000000002
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3213434219360352
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 2.43496036529541
        total_loss: 11.029754638671875
        var_gnorm: 63.31928634643555
        vf_explained_var: 0.10846900939941406
        vf_loss: 18.51093292236328
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 864.0
  learner_queue:
    size_count: 869
    size_mean: 14.72
    size_quantiles: [11.0, 12.0, 16.0, 16.0, 16.0]
    size_std: 1.5625619987699688
  num_agent_steps_sampled: 448900
  num_agent_steps_trained: 432000
  num_env_steps_sampled: 448900
  num_env_steps_trained: 432000
  num_samples_added_to_queue: 448500
  num_training_step_calls_since_last_synch_worker_weights: 0
  num_weight_broadcasts: 8804
  timing_breakdown:
    learner_dequeue_time_ms: 0.024
    learner_grad_time_ms: 369.266
    learner_load_time_ms: 3.551
    learner_load_wait_time_ms: 2.813
iterations_since_restore: 60
node_ip: 127.0.0.1
num_agent_steps_sampled: 448900
num_agent_steps_trained: 432000
num_env_steps_sampled: 448900
num_env_steps_sampled_this_iter: 8000
num_env_steps_sampled_throughput_per_sec: 799.8044492095316
num_env_steps_trained: 432000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.8044492095316
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 3
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 50.47142857142857
  ram_util_percent: 79.75714285714287
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10830505258096818
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03959729847431737
  mean_inference_ms: 2.0485519795186127
  mean_raw_obs_processing_ms: 0.45666623432652775
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03543567657470703
    StateBufferConnector_ms: 0.006400346755981445
    ViewRequirementAgentConnector_ms: 0.21298742294311523
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 8.0
  episode_reward_mean: 2.73
  episode_reward_min: 0.0
  episodes_this_iter: 62
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 3.0, 0.0, 3.0, 4.0, 4.0, 0.0, 6.0, 4.0, 3.0, 4.0, 4.0, 3.0,
      8.0, 1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 3.0, 2.0, 3.0, 4.0, 3.0, 1.0, 0.0, 6.0, 2.0,
      2.0, 4.0, 4.0, 3.0, 5.0, 3.0, 3.0, 0.0, 2.0, 4.0, 2.0, 2.0, 4.0, 4.0, 2.0, 2.0,
      1.0, 4.0, 1.0, 3.0, 3.0, 4.0, 6.0, 2.0, 5.0, 1.0, 1.0, 3.0, 5.0, 2.0, 3.0, 1.0,
      3.0, 3.0, 3.0, 4.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 3.0, 0.0, 2.0, 2.0, 0.0, 0.0,
      0.0, 2.0, 4.0, 2.0, 3.0, 2.0, 3.0, 3.0, 5.0, 3.0, 3.0, 4.0, 1.0, 3.0, 3.0, 7.0,
      2.0, 0.0, 5.0, 4.0, 0.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10830505258096818
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03959729847431737
    mean_inference_ms: 2.0485519795186127
    mean_raw_obs_processing_ms: 0.45666623432652775
time_since_restore: 617.1151580810547
time_this_iter_s: 10.196851015090942
time_total_s: 617.1151580810547
timers:
  sample_time_ms: 0.103
  synch_weights_time_ms: 0.395
  training_iteration_time_ms: 0.622
timestamp: 1692344715
timesteps_total: 448900
training_iteration: 60
trial_id: default
An Algorithm checkpoint has been created inside directory: '/Users/sangbin/ray_results/Impala_MemoryPlanningGame_2023-08-18_16-34-54mhw74o1i/checkpoint_000060'.
train step: 61
agent_timesteps_total: 455450
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03690838813781738
  StateBufferConnector_ms: 0.0067560672760009766
  ViewRequirementAgentConnector_ms: 0.21779561042785645
counters:
  num_agent_steps_sampled: 455450
  num_agent_steps_trained: 438500
  num_env_steps_sampled: 455450
  num_env_steps_trained: 438500
  num_samples_added_to_queue: 455000
  num_training_step_calls_since_last_synch_worker_weights: 157
  num_weight_broadcasts: 8933
custom_metrics: {}
date: 2023-08-18_16-45-25
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.63
episode_reward_min: 0.0
episodes_this_iter: 52
episodes_total: 3559
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.299999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3016327619552612
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 1.5190470218658447
        total_loss: 12.106571197509766
        var_gnorm: 63.31947708129883
        vf_explained_var: 0.12104779481887817
        vf_loss: 22.476680755615234
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 877.0
  learner_queue:
    size_count: 884
    size_mean: 14.56
    size_quantiles: [10.0, 12.0, 15.0, 16.0, 16.0]
    size_std: 1.6871277367170514
  num_agent_steps_sampled: 455450
  num_agent_steps_trained: 438500
  num_env_steps_sampled: 455450
  num_env_steps_trained: 438500
  num_samples_added_to_queue: 455000
  num_training_step_calls_since_last_synch_worker_weights: 157
  num_weight_broadcasts: 8933
  timing_breakdown:
    learner_dequeue_time_ms: 0.024
    learner_grad_time_ms: 327.775
    learner_load_time_ms: 3.556
    learner_load_wait_time_ms: 2.909
iterations_since_restore: 61
node_ip: 127.0.0.1
num_agent_steps_sampled: 455450
num_agent_steps_trained: 438500
num_env_steps_sampled: 455450
num_env_steps_sampled_this_iter: 6550
num_env_steps_sampled_throughput_per_sec: 654.9970016616745
num_env_steps_trained: 438500
num_env_steps_trained_this_iter: 6500
num_env_steps_trained_throughput_per_sec: 649.9970245497533
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6500
perf:
  cpu_util_percent: 51.08
  ram_util_percent: 80.61333333333333
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10837745263678031
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0396345643038042
  mean_inference_ms: 2.0492501410981276
  mean_raw_obs_processing_ms: 0.45696107211800757
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03690838813781738
    StateBufferConnector_ms: 0.0067560672760009766
    ViewRequirementAgentConnector_ms: 0.21779561042785645
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.63
  episode_reward_min: 0.0
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 5.0, 1.0, 1.0, 3.0, 5.0, 2.0, 3.0, 1.0, 3.0, 3.0, 3.0, 4.0,
      2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 3.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 4.0, 2.0,
      3.0, 2.0, 3.0, 3.0, 5.0, 3.0, 3.0, 4.0, 1.0, 3.0, 3.0, 7.0, 2.0, 0.0, 5.0, 4.0,
      0.0, 1.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, 0.0, 4.0, 1.0, 4.0, 0.0, 2.0, 2.0, 2.0,
      2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 4.0, 4.0, 3.0, 4.0, 0.0, 7.0, 7.0, 3.0, 5.0, 7.0,
      2.0, 1.0, 3.0, 6.0, 3.0, 3.0, 1.0, 2.0, 0.0, 5.0, 3.0, 4.0, 1.0, 6.0, 3.0, 1.0,
      2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10837745263678031
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0396345643038042
    mean_inference_ms: 2.0492501410981276
    mean_raw_obs_processing_ms: 0.45696107211800757
time_since_restore: 627.4225158691406
time_this_iter_s: 10.307357788085938
time_total_s: 627.4225158691406
timers:
  sample_time_ms: 0.031
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.088
timestamp: 1692344725
timesteps_total: 455450
training_iteration: 61
trial_id: default
train step: 62
agent_timesteps_total: 459150
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0438542366027832
  StateBufferConnector_ms: 0.008139371871948242
  ViewRequirementAgentConnector_ms: 0.2618427276611328
counters:
  num_agent_steps_sampled: 459150
  num_agent_steps_trained: 443500
  num_env_steps_sampled: 459150
  num_env_steps_trained: 443500
  num_samples_added_to_queue: 459000
  num_training_step_calls_since_last_synch_worker_weights: 444
  num_weight_broadcasts: 9004
custom_metrics: {}
date: 2023-08-18_16-45-35
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.58
episode_reward_min: 0.0
episodes_this_iter: 28
episodes_total: 3587
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.200000000000045
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.293703556060791
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -43.706825256347656
        total_loss: -36.87771987915039
        var_gnorm: 63.31964111328125
        vf_explained_var: 0.10060864686965942
        vf_loss: 14.951911926269531
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 887.0
  learner_queue:
    size_count: 893
    size_mean: 14.14
    size_quantiles: [9.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 2.0298768435548005
  num_agent_steps_sampled: 459150
  num_agent_steps_trained: 443500
  num_env_steps_sampled: 459150
  num_env_steps_trained: 443500
  num_samples_added_to_queue: 459000
  num_training_step_calls_since_last_synch_worker_weights: 444
  num_weight_broadcasts: 9004
  timing_breakdown:
    learner_dequeue_time_ms: 0.024
    learner_grad_time_ms: 1004.147
    learner_load_time_ms: 3.556
    learner_load_wait_time_ms: 26.061
iterations_since_restore: 62
node_ip: 127.0.0.1
num_agent_steps_sampled: 459150
num_agent_steps_trained: 443500
num_env_steps_sampled: 459150
num_env_steps_sampled_this_iter: 3700
num_env_steps_sampled_throughput_per_sec: 369.9991090319154
num_env_steps_trained: 443500
num_env_steps_trained_this_iter: 5000
num_env_steps_trained_throughput_per_sec: 499.99879598907484
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 5000
perf:
  cpu_util_percent: 75.1846153846154
  ram_util_percent: 82.56923076923077
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10864463070149694
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.0397698486333442
  mean_inference_ms: 2.0559226240634514
  mean_raw_obs_processing_ms: 0.4582961224218788
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0438542366027832
    StateBufferConnector_ms: 0.008139371871948242
    ViewRequirementAgentConnector_ms: 0.2618427276611328
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.58
  episode_reward_min: 0.0
  episodes_this_iter: 28
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [2.0, 3.0, 2.0, 3.0, 3.0, 5.0, 3.0, 3.0, 4.0, 1.0, 3.0, 3.0, 7.0,
      2.0, 0.0, 5.0, 4.0, 0.0, 1.0, 2.0, 3.0, 3.0, 3.0, 2.0, 3.0, 0.0, 4.0, 1.0, 4.0,
      0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 1.0, 2.0, 3.0, 2.0, 4.0, 4.0, 3.0, 4.0, 0.0, 7.0,
      7.0, 3.0, 5.0, 7.0, 2.0, 1.0, 3.0, 6.0, 3.0, 3.0, 1.0, 2.0, 0.0, 5.0, 3.0, 4.0,
      1.0, 6.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0, 2.0, 2.0, 2.0, 0.0,
      1.0, 3.0, 4.0, 2.0, 1.0, 2.0, 4.0, 5.0, 1.0, 1.0, 4.0, 1.0, 3.0, 3.0, 2.0, 2.0,
      2.0, 1.0, 3.0, 0.0, 5.0, 1.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10864463070149694
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.0397698486333442
    mean_inference_ms: 2.0559226240634514
    mean_raw_obs_processing_ms: 0.4582961224218788
time_since_restore: 637.6510620117188
time_this_iter_s: 10.228546142578125
time_total_s: 637.6510620117188
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.076
timestamp: 1692344735
timesteps_total: 459150
training_iteration: 62
trial_id: default
train step: 63
agent_timesteps_total: 466700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.042821407318115234
  StateBufferConnector_ms: 0.007851123809814453
  ViewRequirementAgentConnector_ms: 0.25628042221069336
counters:
  num_agent_steps_sampled: 466700
  num_agent_steps_trained: 450000
  num_env_steps_sampled: 466700
  num_env_steps_trained: 450000
  num_samples_added_to_queue: 466500
  num_training_step_calls_since_last_synch_worker_weights: 62
  num_weight_broadcasts: 9152
custom_metrics: {}
date: 2023-08-18_16-45-46
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.36
episode_reward_min: 0.0
episodes_this_iter: 60
episodes_total: 3647
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.2855836153030396
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 27.005069732666016
        total_loss: 39.815040588378906
        var_gnorm: 63.31989288330078
        vf_explained_var: 0.19499677419662476
        vf_loss: 26.905521392822266
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 900.0
  learner_queue:
    size_count: 906
    size_mean: 13.84
    size_quantiles: [9.0, 11.0, 14.0, 16.0, 16.0]
    size_std: 2.062619693496598
  num_agent_steps_sampled: 466700
  num_agent_steps_trained: 450000
  num_env_steps_sampled: 466700
  num_env_steps_trained: 450000
  num_samples_added_to_queue: 466500
  num_training_step_calls_since_last_synch_worker_weights: 62
  num_weight_broadcasts: 9152
  timing_breakdown:
    learner_dequeue_time_ms: 0.019
    learner_grad_time_ms: 543.291
    learner_load_time_ms: 3.711
    learner_load_wait_time_ms: 13.041
iterations_since_restore: 63
node_ip: 127.0.0.1
num_agent_steps_sampled: 466700
num_agent_steps_trained: 450000
num_env_steps_sampled: 466700
num_env_steps_sampled_this_iter: 7550
num_env_steps_sampled_throughput_per_sec: 754.9970479126964
num_env_steps_trained: 450000
num_env_steps_trained_this_iter: 6500
num_env_steps_trained_throughput_per_sec: 649.9974584678844
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 6500
perf:
  cpu_util_percent: 54.48
  ram_util_percent: 81.58666666666667
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10912751119183049
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03995237903190064
  mean_inference_ms: 2.0654616953937177
  mean_raw_obs_processing_ms: 0.46052427276780905
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.042821407318115234
    StateBufferConnector_ms: 0.007851123809814453
    ViewRequirementAgentConnector_ms: 0.25628042221069336
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.36
  episode_reward_min: 0.0
  episodes_this_iter: 60
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 1.0, 6.0, 3.0, 1.0, 2.0, 2.0, 1.0, 2.0, 2.0, 2.0, 2.0, 3.0,
      2.0, 2.0, 2.0, 0.0, 1.0, 3.0, 4.0, 2.0, 1.0, 2.0, 4.0, 5.0, 1.0, 1.0, 4.0, 1.0,
      3.0, 3.0, 2.0, 2.0, 2.0, 1.0, 3.0, 0.0, 5.0, 1.0, 0.0, 2.0, 0.0, 2.0, 2.0, 3.0,
      2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 4.0, 3.0, 2.0, 1.0, 3.0, 5.0, 3.0, 2.0, 3.0, 3.0,
      2.0, 1.0, 3.0, 1.0, 0.0, 3.0, 2.0, 3.0, 1.0, 1.0, 3.0, 4.0, 2.0, 2.0, 1.0, 5.0,
      1.0, 1.0, 2.0, 5.0, 7.0, 2.0, 5.0, 2.0, 4.0, 6.0, 5.0, 2.0, 1.0, 0.0, 2.0, 3.0,
      1.0, 5.0, 1.0, 5.0, 5.0, 0.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10912751119183049
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03995237903190064
    mean_inference_ms: 2.0654616953937177
    mean_raw_obs_processing_ms: 0.46052427276780905
time_since_restore: 647.8977928161621
time_this_iter_s: 10.24673080444336
time_total_s: 647.8977928161621
timers:
  sample_time_ms: 0.027
  synch_weights_time_ms: 0.008
  training_iteration_time_ms: 0.08
timestamp: 1692344746
timesteps_total: 466700
training_iteration: 63
trial_id: default
train step: 64
agent_timesteps_total: 474750
connector_metrics:
  ObsPreprocessorConnector_ms: 0.0335843563079834
  StateBufferConnector_ms: 0.00586247444152832
  ViewRequirementAgentConnector_ms: 0.19919538497924805
counters:
  num_agent_steps_sampled: 474750
  num_agent_steps_trained: 458000
  num_env_steps_sampled: 474750
  num_env_steps_trained: 458000
  num_samples_added_to_queue: 474500
  num_training_step_calls_since_last_synch_worker_weights: 660
  num_weight_broadcasts: 9310
custom_metrics: {}
date: 2023-08-18_16-45-56
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.54
episode_reward_min: 0.0
episodes_this_iter: 63
episodes_total: 3710
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 29.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.2761578559875488
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: -32.7970085144043
        total_loss: -25.84065055847168
        var_gnorm: 63.320064544677734
        vf_explained_var: 0.09078651666641235
        vf_loss: 15.188874244689941
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 916.0
  learner_queue:
    size_count: 922
    size_mean: 13.7
    size_quantiles: [9.0, 11.0, 14.0, 16.0, 16.0]
    size_std: 2.090454496036687
  num_agent_steps_sampled: 474750
  num_agent_steps_trained: 458000
  num_env_steps_sampled: 474750
  num_env_steps_trained: 458000
  num_samples_added_to_queue: 474500
  num_training_step_calls_since_last_synch_worker_weights: 660
  num_weight_broadcasts: 9310
  timing_breakdown:
    learner_dequeue_time_ms: 0.019
    learner_grad_time_ms: 339.035
    learner_load_time_ms: 3.503
    learner_load_wait_time_ms: 2.811
iterations_since_restore: 64
node_ip: 127.0.0.1
num_agent_steps_sampled: 474750
num_agent_steps_trained: 458000
num_env_steps_sampled: 474750
num_env_steps_sampled_this_iter: 8050
num_env_steps_sampled_throughput_per_sec: 804.9958927840935
num_env_steps_trained: 458000
num_env_steps_trained_this_iter: 8000
num_env_steps_trained_throughput_per_sec: 799.9959182947513
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 8000
perf:
  cpu_util_percent: 51.42857142857143
  ram_util_percent: 81.42142857142856
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10910822278173306
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.039975302717593066
  mean_inference_ms: 2.066582718142033
  mean_raw_obs_processing_ms: 0.46080428031793474
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.0335843563079834
    StateBufferConnector_ms: 0.00586247444152832
    ViewRequirementAgentConnector_ms: 0.19919538497924805
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.54
  episode_reward_min: 0.0
  episodes_this_iter: 63
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [3.0, 1.0, 0.0, 3.0, 2.0, 3.0, 1.0, 1.0, 3.0, 4.0, 2.0, 2.0, 1.0,
      5.0, 1.0, 1.0, 2.0, 5.0, 7.0, 2.0, 5.0, 2.0, 4.0, 6.0, 5.0, 2.0, 1.0, 0.0, 2.0,
      3.0, 1.0, 5.0, 1.0, 5.0, 5.0, 0.0, 2.0, 2.0, 1.0, 1.0, 2.0, 3.0, 1.0, 7.0, 0.0,
      1.0, 3.0, 3.0, 2.0, 2.0, 1.0, 3.0, 2.0, 3.0, 4.0, 2.0, 4.0, 3.0, 2.0, 3.0, 0.0,
      2.0, 1.0, 3.0, 0.0, 4.0, 1.0, 5.0, 3.0, 4.0, 2.0, 1.0, 6.0, 1.0, 3.0, 2.0, 2.0,
      2.0, 2.0, 1.0, 5.0, 2.0, 1.0, 4.0, 3.0, 1.0, 4.0, 1.0, 3.0, 2.0, 3.0, 5.0, 3.0,
      1.0, 2.0, 4.0, 4.0, 5.0, 1.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10910822278173306
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.039975302717593066
    mean_inference_ms: 2.066582718142033
    mean_raw_obs_processing_ms: 0.46080428031793474
time_since_restore: 658.1878588199615
time_this_iter_s: 10.290066003799438
time_total_s: 658.1878588199615
timers:
  sample_time_ms: 0.032
  synch_weights_time_ms: 0.009
  training_iteration_time_ms: 0.09
timestamp: 1692344756
timesteps_total: 474750
training_iteration: 64
trial_id: default
train step: 65
agent_timesteps_total: 481950
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03470039367675781
  StateBufferConnector_ms: 0.0061109066009521484
  ViewRequirementAgentConnector_ms: 0.20577311515808105
counters:
  num_agent_steps_sampled: 481950
  num_agent_steps_trained: 465000
  num_env_steps_sampled: 481950
  num_env_steps_trained: 465000
  num_samples_added_to_queue: 481500
  num_training_step_calls_since_last_synch_worker_weights: 573
  num_weight_broadcasts: 9450
custom_metrics: {}
date: 2023-08-18_16-46-06
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.44
episode_reward_min: 0.0
episodes_this_iter: 56
episodes_total: 3766
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 31.799999999999955
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.2741267681121826
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 13.626659393310547
        total_loss: 24.523677825927734
        var_gnorm: 63.320255279541016
        vf_explained_var: 0.1017349362373352
        vf_loss: 23.068164825439453
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 930.0
  learner_queue:
    size_count: 936
    size_mean: 13.84
    size_quantiles: [9.0, 11.0, 14.0, 16.0, 16.0]
    size_std: 1.9935897271003382
  num_agent_steps_sampled: 481950
  num_agent_steps_trained: 465000
  num_env_steps_sampled: 481950
  num_env_steps_trained: 465000
  num_samples_added_to_queue: 481500
  num_training_step_calls_since_last_synch_worker_weights: 573
  num_weight_broadcasts: 9450
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 367.985
    learner_load_time_ms: 3.452
    learner_load_wait_time_ms: 2.84
iterations_since_restore: 65
node_ip: 127.0.0.1
num_agent_steps_sampled: 481950
num_agent_steps_trained: 465000
num_env_steps_sampled: 481950
num_env_steps_sampled_this_iter: 7200
num_env_steps_sampled_throughput_per_sec: 719.9944553802231
num_env_steps_trained: 465000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9946093974391
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 54.75333333333333
  ram_util_percent: 81.62666666666668
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10909237476160027
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03995758578881426
  mean_inference_ms: 2.0654001926023944
  mean_raw_obs_processing_ms: 0.46067030319203695
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03470039367675781
    StateBufferConnector_ms: 0.0061109066009521484
    ViewRequirementAgentConnector_ms: 0.20577311515808105
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.44
  episode_reward_min: 0.0
  episodes_this_iter: 56
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [4.0, 3.0, 2.0, 3.0, 0.0, 2.0, 1.0, 3.0, 0.0, 4.0, 1.0, 5.0, 3.0,
      4.0, 2.0, 1.0, 6.0, 1.0, 3.0, 2.0, 2.0, 2.0, 2.0, 1.0, 5.0, 2.0, 1.0, 4.0, 3.0,
      1.0, 4.0, 1.0, 3.0, 2.0, 3.0, 5.0, 3.0, 1.0, 2.0, 4.0, 4.0, 5.0, 1.0, 2.0, 4.0,
      4.0, 3.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 3.0, 3.0, 1.0, 4.0, 0.0, 1.0,
      1.0, 2.0, 2.0, 0.0, 2.0, 5.0, 2.0, 3.0, 3.0, 3.0, 2.0, 1.0, 3.0, 7.0, 3.0, 2.0,
      2.0, 3.0, 2.0, 4.0, 4.0, 6.0, 2.0, 2.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 1.0, 1.0,
      3.0, 2.0, 2.0, 2.0, 1.0, 2.0, 2.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10909237476160027
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03995758578881426
    mean_inference_ms: 2.0654001926023944
    mean_raw_obs_processing_ms: 0.46067030319203695
time_since_restore: 668.4203848838806
time_this_iter_s: 10.232526063919067
time_total_s: 668.4203848838806
timers:
  sample_time_ms: 0.026
  synch_weights_time_ms: 0.007
  training_iteration_time_ms: 0.076
timestamp: 1692344766
timesteps_total: 481950
training_iteration: 65
trial_id: default
train step: 66
agent_timesteps_total: 488700
connector_metrics:
  ObsPreprocessorConnector_ms: 0.03633427619934082
  StateBufferConnector_ms: 0.006445407867431641
  ViewRequirementAgentConnector_ms: 0.2186286449432373
counters:
  num_agent_steps_sampled: 488700
  num_agent_steps_trained: 472000
  num_env_steps_sampled: 488700
  num_env_steps_trained: 472000
  num_samples_added_to_queue: 488500
  num_training_step_calls_since_last_synch_worker_weights: 7
  num_weight_broadcasts: 9582
custom_metrics: {}
date: 2023-08-18_16-46-16
done: false
episode_len_mean: 128.0
episode_media: {}
episode_reward_max: 7.0
episode_reward_mean: 2.45
episode_reward_min: 0.0
episodes_this_iter: 52
episodes_total: 3818
hostname: bagsangbins-MacBook-Air.local
info:
  learner:
    default_policy:
      custom_metrics: {}
      diff_num_grad_updates_vs_sampler_policy: 33.0
      learner_stats:
        allreduce_latency: 0.0
        cur_lr: 1.0e-05
        entropy: 1.3167707920074463
        entropy_coeff: 0.001
        grad_gnorm: 20.0
        policy_loss: 14.789739608764648
        total_loss: 28.307443618774414
        var_gnorm: 63.320457458496094
        vf_explained_var: 0.09634441137313843
        vf_loss: 28.35218048095703
      model: {}
      num_agent_steps_trained: 500.0
      num_grad_updates_lifetime: 944.0
  learner_queue:
    size_count: 951
    size_mean: 14.2
    size_quantiles: [10.0, 11.0, 15.0, 16.0, 16.0]
    size_std: 1.8867962264113207
  num_agent_steps_sampled: 488700
  num_agent_steps_trained: 472000
  num_env_steps_sampled: 488700
  num_env_steps_trained: 472000
  num_samples_added_to_queue: 488500
  num_training_step_calls_since_last_synch_worker_weights: 7
  num_weight_broadcasts: 9582
  timing_breakdown:
    learner_dequeue_time_ms: 0.015
    learner_grad_time_ms: 323.653
    learner_load_time_ms: 3.505
    learner_load_wait_time_ms: 2.851
iterations_since_restore: 66
node_ip: 127.0.0.1
num_agent_steps_sampled: 488700
num_agent_steps_trained: 472000
num_env_steps_sampled: 488700
num_env_steps_sampled_this_iter: 6750
num_env_steps_sampled_throughput_per_sec: 674.9979722560762
num_env_steps_trained: 472000
num_env_steps_trained_this_iter: 7000
num_env_steps_trained_throughput_per_sec: 699.9978971544494
num_faulty_episodes: 0
num_healthy_workers: 2
num_in_flight_async_reqs: 4
num_remote_worker_restarts: 0
num_steps_trained_this_iter: 7000
perf:
  cpu_util_percent: 57.864285714285714
  ram_util_percent: 80.91428571428571
pid: 47173
policy_reward_max: {}
policy_reward_mean: {}
policy_reward_min: {}
sampler_perf:
  mean_action_processing_ms: 0.10920343148783086
  mean_env_render_ms: 0.0
  mean_env_wait_ms: 0.03997896415174945
  mean_inference_ms: 2.0658240391247835
  mean_raw_obs_processing_ms: 0.4609458769745272
sampler_results:
  connector_metrics:
    ObsPreprocessorConnector_ms: 0.03633427619934082
    StateBufferConnector_ms: 0.006445407867431641
    ViewRequirementAgentConnector_ms: 0.2186286449432373
  custom_metrics: {}
  episode_len_mean: 128.0
  episode_media: {}
  episode_reward_max: 7.0
  episode_reward_mean: 2.45
  episode_reward_min: 0.0
  episodes_this_iter: 52
  hist_stats:
    episode_lengths: [128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128, 128,
      128, 128, 128, 128, 128, 128, 128, 128]
    episode_reward: [1.0, 1.0, 1.0, 3.0, 3.0, 1.0, 4.0, 0.0, 1.0, 1.0, 2.0, 2.0, 0.0,
      2.0, 5.0, 2.0, 3.0, 3.0, 3.0, 2.0, 1.0, 3.0, 7.0, 3.0, 2.0, 2.0, 3.0, 2.0, 4.0,
      4.0, 6.0, 2.0, 2.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 1.0, 1.0, 3.0, 2.0, 2.0, 2.0,
      1.0, 2.0, 2.0, 5.0, 2.0, 3.0, 4.0, 2.0, 6.0, 5.0, 2.0, 2.0, 3.0, 3.0, 2.0, 4.0,
      2.0, 1.0, 1.0, 0.0, 1.0, 1.0, 3.0, 1.0, 5.0, 4.0, 3.0, 1.0, 4.0, 4.0, 1.0, 1.0,
      4.0, 2.0, 3.0, 3.0, 5.0, 2.0, 1.0, 2.0, 1.0, 4.0, 2.0, 5.0, 2.0, 4.0, 0.0, 0.0,
      1.0, 2.0, 6.0, 3.0, 4.0, 1.0, 0.0]
  num_faulty_episodes: 0
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf:
    mean_action_processing_ms: 0.10920343148783086
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 0.03997896415174945
    mean_inference_ms: 2.0658240391247835
    mean_raw_obs_processing_ms: 0.4609458769745272
time_since_restore: 678.6917979717255
time_this_iter_s: 10.271413087844849
time_total_s: 678.6917979717255
timers:
  sample_time_ms: 0.134
  synch_weights_time_ms: 0.457
  training_iteration_time_ms: 0.734
timestamp: 1692344776
timesteps_total: 488700
training_iteration: 66
trial_id: default
train step: 67
Traceback (most recent call last):
  File "/Users/sangbin/Impala/launch.py", line 304, in <module>
    result = algo.train()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 372, in train
    result = self.step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 2837, in _run_one_training_iteration
    results = self.training_step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 747, in training_step
    train_results = self.process_trained_results()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 1026, in process_trained_results
    final_learner_info = copy.deepcopy(self._learner_thread.learner_info)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 144, in deepcopy
    copier = _deepcopy_dispatch.get(cls)
KeyboardInterrupt
Traceback (most recent call last):
  File "/Users/sangbin/Impala/launch.py", line 304, in <module>
    result = algo.train()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/tune/trainable/trainable.py", line 372, in train
    result = self.step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 853, in step
    results, train_iter_ctx = self._run_one_training_iteration()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py", line 2837, in _run_one_training_iteration
    results = self.training_step()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 747, in training_step
    train_results = self.process_trained_results()
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ray/rllib/algorithms/impala/impala.py", line 1026, in process_trained_results
    final_learner_info = copy.deepcopy(self._learner_thread.learner_info)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 146, in deepcopy
    y = copier(x, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 231, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/copy.py", line 144, in deepcopy
    copier = _deepcopy_dispatch.get(cls)
KeyboardInterrupt